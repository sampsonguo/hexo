{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon.png","path":"images/apple-touch-icon.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32.png","path":"images/favicon-32x32.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16.png","path":"images/favicon-16x16.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1507435369127},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1507435369134},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1507435369130},{"_id":"themes/next/.gitignore","hash":"b935cc0e5b099ebd343ca1766e02f65138c13dd0","modified":1507435369154},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1507435369158},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1507435369164},{"_id":"themes/next/.javascript_ignore","hash":"cd250ad74ca22bd2c054476456a73d9687f05f87","modified":1507435369161},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1507435369175},{"_id":"themes/next/README.cn.md","hash":"59e323ce21535d561507c9ecc984b7c4dcb61514","modified":1507435369179},{"_id":"themes/next/.travis.yml","hash":"6674fbdfe0d0c03b8a04527ffb8ab66a94253acd","modified":1507435369172},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1507435369195},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1507435369168},{"_id":"themes/next/README.md","hash":"225962d533233395f5c57606de1b8585821354d9","modified":1507435369182},{"_id":"themes/next/bower.json","hash":"47471a8f13528dc4052b746db5b4be2375682173","modified":1507435369191},{"_id":"themes/next/package.json","hash":"92a106da76a9b436593fe468d076972b550c8ca2","modified":1507435369546},{"_id":"source/_posts/.DS_Store","hash":"ff11f73ef665561d7cb9c5b1b01e160e1e3bf2a8","modified":1521427969297},{"_id":"themes/next/_config.yml","hash":"3985756d36c1b1b1e513688bdae370441d633e66","modified":1507435369186},{"_id":"source/_posts/MLR.md","hash":"b53b3cfd1de8f9fb0de5bb2bc506703ec3e29841","modified":1522061707388},{"_id":"source/_posts/ctr-recalibration.md","hash":"6677c805ccdd21175569ffe387795f418d4c5d89","modified":1542166681663},{"_id":"source/_posts/auc-n-logloss.md","hash":"1ccccc9241f9ea366494abc5fef4030aef12bedb","modified":1507559188361},{"_id":"source/_posts/bayes.md","hash":"da5a01b1061d0a113c52a9a770efe39a4635652f","modified":1520752027168},{"_id":"source/_posts/MachineLearning-ZhouZhihua.md","hash":"f5cd25ab0d5604e733fabd54bf9bf446c62f4b3c","modified":1512542818376},{"_id":"source/_posts/dict-vs-bag-of-words.md","hash":"0a2485f55921cfa85814d36adaec66f24e3357bf","modified":1538013128740},{"_id":"source/_posts/dt.md","hash":"2476ee0ab18ab3c62dc7e15d2b1313b86dcc9eb8","modified":1513235451627},{"_id":"source/_posts/distributed-tf.md","hash":"eb6acc7fa256955562047dedeb4cde1ba9b36e89","modified":1521106459157},{"_id":"source/_posts/ctr-smooth.md","hash":"5144129afe1f2305ebbf3fc3fb268a0469662a01","modified":1512737645404},{"_id":"source/_posts/dnn-embedding.md","hash":"8245caa5c008fd3bb012cc259127d929073af506","modified":1522140598974},{"_id":"source/_posts/edge-rank.md","hash":"d9aae783490f4a28d46c85db00f169abe1c3a961","modified":1516019944132},{"_id":"source/_posts/data-warehouse.md","hash":"d4503cb11b8b4e58f04ab9a6daef84e7a81b5ca4","modified":1538013092129},{"_id":"source/_posts/exploding-n-vanishing.md","hash":"334feb9c1c7d8d54de7ac666bad3915a701527c0","modified":1511420539179},{"_id":"source/_posts/elastic-search-md.md","hash":"d31925d05a1476b3ec1f2bb825aba6d7df69ac41","modified":1522823093537},{"_id":"source/_posts/ee-n-dqn.md","hash":"e65f86a30955598d25e69d7c6ee728b5830d7d36","modified":1508920497845},{"_id":"source/_posts/feature-engineer.md","hash":"9275144a40e6aa914245067714c91bc02cc97e5f","modified":1533796857848},{"_id":"source/_posts/feature-yy-list.md","hash":"40423292c4084ab72f002758ddf52be98906cfa3","modified":1538300834587},{"_id":"source/_posts/cnn.md","hash":"7a724c1807c10173509337da527180f492d9e588","modified":1521427969297},{"_id":"source/_posts/hive-param-tune.md","hash":"84c70193a4104cc76807a0cd6cf378ef2bc41197","modified":1529567342105},{"_id":"source/_posts/hivemall.md","hash":"a7f4cdb7e56c4c8f35030928616b22d7fb54faee","modified":1521704178054},{"_id":"source/_posts/kafka.md","hash":"2eb2e7ba8f784e99d73a1482d6086c8f7ce33b17","modified":1513306367976},{"_id":"source/_posts/interview-questions.md","hash":"6c3fcd5425dfe92732a223a13b5b6a3747591c9a","modified":1533780538979},{"_id":"source/_posts/logit-n-probit.md","hash":"f0f7b68a197f40a407f6796254ba969caa6fcec9","modified":1508400018204},{"_id":"source/_posts/ocpc.md","hash":"bf71da188c803a18414958e38e29da994d99e334","modified":1539747028657},{"_id":"source/_posts/pr.md","hash":"47c436f8a975db4b032d289b3a39a77f7368bd48","modified":1508379158013},{"_id":"source/_posts/lda.md","hash":"9045c6a320c93dde73b1086b1f0578dfc51f1485","modified":1538207057167},{"_id":"source/_posts/spark-streaming.md","hash":"69866965a8ace44c42725039ac5d1f1df81959c1","modified":1523521087005},{"_id":"source/_posts/query-parse.md","hash":"3eb4c75c045e22bb223b80eb430eccbade85bd77","modified":1521599295581},{"_id":"source/_posts/word2vec.md","hash":"0ff3e6804ef49f9bfe3006f9c99237d09654ab73","modified":1541384825614},{"_id":"source/_posts/search.md","hash":"4533900f80529c6da50cabfe6c58493a2acd9123","modified":1515412661621},{"_id":"source/_posts/tf-serving.md","hash":"9529398da6d3c084d5690712874c1e1f5cff2222","modified":1538299680245},{"_id":"source/_posts/subway.md","hash":"e4203e9934ba7fa5c1e3eb8bde625f22799d5345","modified":1508983630211},{"_id":"source/_posts/xgboost-n-spark.md","hash":"0e6f3e5c62acb7174e4553e8fa784eeca7c86d65","modified":1509172478485},{"_id":"source/categories/index.md","hash":"9b79594a918e77ea90f28dcd589c9dcf922a36bd","modified":1507435369120},{"_id":"source/about/index.md","hash":"7f70c9a03b8d9823b8c0faf1faaa6fcd841a5745","modified":1507435369115},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5adfad3ef1b870063e621bc0838268eb2c7c697a","modified":1507435369139},{"_id":"source/_posts/tf-wnd.md","hash":"8d02a0ac02b5f9e86fd898059dd42b77dee710af","modified":1515420473756},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"b1ec000babd42bb7ffd26f5ad8aac9b5bec79ae5","modified":1507435369143},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"1228506a940114288d61812bfe60c045a0abeac1","modified":1507435369147},{"_id":"themes/next/languages/de.yml","hash":"fd02d9c2035798d5dc7c1a96b4c3e24b05b31a47","modified":1507435369200},{"_id":"themes/next/languages/default.yml","hash":"b3bcd8934327448a43d9bfada5dd11b1b8c1402e","modified":1507435369203},{"_id":"themes/next/languages/en.yml","hash":"b3bcd8934327448a43d9bfada5dd11b1b8c1402e","modified":1507435369207},{"_id":"themes/next/languages/fr-FR.yml","hash":"efeeb55d5c4add54ad59a612fc0630ee1300388c","modified":1507435369210},{"_id":"themes/next/languages/id.yml","hash":"dccae33e2a5b3c9f11c0e05ec4a7201af1b25745","modified":1507435369212},{"_id":"themes/next/languages/it.yml","hash":"a215d016146b1bd92cef046042081cbe0c7f976f","modified":1507435369216},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1507435369151},{"_id":"themes/next/languages/ja.yml","hash":"37f954e47a3bc669620ca559e3edb3b0072a4be5","modified":1507435369219},{"_id":"source/about/tmp.md","hash":"c5f947754ed8b9d17e4f21e757b023815fc6464e","modified":1508379102819},{"_id":"themes/next/languages/pt.yml","hash":"2efcd240c66ab1a122f061505ca0fb1e8819877b","modified":1507435369229},{"_id":"themes/next/languages/ko.yml","hash":"dc8f3e8c64eb7c4bb2385025b3006b8efec8b31d","modified":1507435369223},{"_id":"themes/next/languages/pt-BR.yml","hash":"568d494a1f37726a5375b11452a45c71c3e2852d","modified":1507435369226},{"_id":"themes/next/languages/ru.yml","hash":"e33ee44e80f82e329900fc41eb0bb6823397a4d6","modified":1507435369233},{"_id":"themes/next/languages/zh-Hans.yml","hash":"23817934c6bf7a59a494743777526b8c8ae3350d","modified":1507435369235},{"_id":"themes/next/layout/_layout.swig","hash":"7bf52e714d445d253d13fc36fc7463096885e81b","modified":1507435369255},{"_id":"themes/next/languages/zh-hk.yml","hash":"19fb3c159fa6f4d58237e5a1a3857048a6add9a6","modified":1507435369238},{"_id":"themes/next/languages/zh-tw.yml","hash":"64a16181fcc3779ea335792c22fda3b5202e3e9e","modified":1507435369241},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1507435369523},{"_id":"themes/next/layout/index.swig","hash":"555a357ecf17128db4e29346c92bb6298e66547a","modified":1507435369527},{"_id":"themes/next/layout/post.swig","hash":"7a6ce102ca82c3a80f776e555dddae1a9981e1ed","modified":1507435369534},{"_id":"themes/next/scripts/merge-configs.js","hash":"5758f8f3f12d17bc80da65bb808a20b3a8aae186","modified":1507435369552},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1507435369556},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1507435369542},{"_id":"themes/next/layout/page.swig","hash":"e8fcaa641d46930237675d2ad4b56964d9e262e9","modified":1507435369531},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1507435369538},{"_id":"themes/next/layout/archive.swig","hash":"9a2c14874a75c7085d2bada5e39201d3fc4fd2b4","modified":1507435369519},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1507435370542},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1507435370549},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1507435370545},{"_id":"source/_posts/auc-n-logloss/2.png","hash":"4f007082dc559888d479918e5419eff48391a830","modified":1507435368992},{"_id":"source/_posts/auc-n-logloss/6.png","hash":"bf400d2df199beb6cd11f0724934b37ec6a9e5e4","modified":1507435369005},{"_id":"source/_posts/auc-n-logloss/8.png","hash":"2b160d7859c0433086fb14404165ee6db04ea68c","modified":1507529257043},{"_id":"source/_posts/auc-n-logloss/9.png","hash":"8bdd4379bb480099a83fa956b65c4bd30ce223f5","modified":1507530067228},{"_id":"source/_posts/auc-n-logloss/7.png","hash":"80956992425cd24224413be93dd711b23b33d5ff","modified":1507435369009},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A54.4.png","hash":"816d988a281cf7d1dac9a856919abc4e7ea8ea16","modified":1521427969528},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1507435370032},{"_id":"source/_posts/ctr-recalibration/图4.4.png","hash":"816d988a281cf7d1dac9a856919abc4e7ea8ea16","modified":1511761070152},{"_id":"source/_posts/feature-engineer/զ%A6ڥ%E4%FE%F6+%B5%AB%C1.png","hash":"2141bdfa44e5cf86009924a72dca0946f5ca9601","modified":1521427969538},{"_id":"source/_posts/cnn/logistic.png","hash":"d14da93bd72cd7c5880caf24323fb0af3b863162","modified":1507637148892},{"_id":"source/_posts/lda/1.png","hash":"18b316e12d4591fa543bcb5db5ea8dd8fb089ed8","modified":1521427969538},{"_id":"source/_posts/lda/100.png","hash":"9b36fbe342f6fe0c7ebd224c6bc4024c77509e2f","modified":1521427969548},{"_id":"source/_posts/ee-n-dqn/2.png","hash":"6515750b1ed88b7f6bf60910052c0b64820f7d82","modified":1508397371470},{"_id":"source/_posts/ee-n-dqn/1.png","hash":"d0ba7aa13143ebf0d5987f1cb2487a7944ea36cf","modified":1508379102813},{"_id":"source/_posts/lda/103.png","hash":"51596b033c3ef8457f11590c7eb35c9be0f08c30","modified":1521427969558},{"_id":"source/_posts/lda/2.png","hash":"4efddf70286df1d8b3ae4f69597eeb9f159ce682","modified":1521427969570},{"_id":"source/_posts/lda/104.png","hash":"dd5fe476184597c8871fd0a581ae15fd5528f862","modified":1521427969558},{"_id":"source/_posts/lda/106.png","hash":"e24199f39e09a9034f4c37b9dd6d56060dd71518","modified":1521427969568},{"_id":"source/_posts/lda/105.png","hash":"a46d624d3e25bfdfe15dc68dc2d7e17ab2bda16f","modified":1521427969558},{"_id":"source/_posts/lda/3.png","hash":"63dc46e8d0254975359f8c02d560a417bcc7686c","modified":1521427969570},{"_id":"source/_posts/logit-n-probit/logistic.png","hash":"d14da93bd72cd7c5880caf24323fb0af3b863162","modified":1507637148892},{"_id":"source/_posts/lda/101.png","hash":"e8a736596eaab0a2863ebaf7cfc2806c05cf2b48","modified":1521427969548},{"_id":"source/_posts/logit-n-probit/logit.png","hash":"318241cd887a667d301d8bd068d6791da4dba7ab","modified":1507637171574},{"_id":"source/_posts/search/IDF.png","hash":"a9d19b122aaad81b2ebdf7ac47382bda3a9a6d30","modified":1515409644123},{"_id":"source/_posts/logit-n-probit/logit-logistic-relation.jpg","hash":"4bc9396bf4c63a5e6e03efad1f0d08a3226c5127","modified":1507638828909},{"_id":"source/_posts/search/TF.png","hash":"a1e40176a572fccb513815595fc6288543bd0a1c","modified":1515409631316},{"_id":"source/_posts/word2vec/cbow-hs-2.png","hash":"617e210a750b28fcf44480cb41778f9a7b7ce88b","modified":1540889690916},{"_id":"source/_posts/word2vec/cbow-hs-hand.jpg","hash":"a8f650036accc0f25ff56d01bc8f0a3cf5889d21","modified":1540889731123},{"_id":"source/_posts/word2vec/cbow-hs-code.png","hash":"fcf93e9550a06c0de3435a11247efee95f26344d","modified":1540889706509},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1507435369260},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"f83befdc740beb8dc88805efd7fbb0fef9ed19be","modified":1507435369264},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"b9f9959225876fb56fb3fba96306d19396e704d4","modified":1507435369277},{"_id":"themes/next/layout/_macro/reward.swig","hash":"357d86ec9586705bfbb2c40a8c7d247a407db21a","modified":1507435369273},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1507435369281},{"_id":"themes/next/layout/_macro/post.swig","hash":"9896b34a7edc112c03b393a1602a616710a66ae1","modified":1507435369268},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"1634fb887842698e01ff6e632597fe03c75d2d01","modified":1507435369315},{"_id":"themes/next/layout/_partials/head.swig","hash":"f14a39dad1ddd98e6d3ceb25dda092ba80d391b5","modified":1507435369295},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1507435369319},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1507435369353},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1507435369357},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1507435369312},{"_id":"themes/next/layout/_partials/comments.swig","hash":"8005c3a585209a788e8a17f848faa482dd1a3be5","modified":1507435369287},{"_id":"themes/next/layout/_partials/footer.swig","hash":"26e93336dc57a39590ba8dc80564a1d2ad5ff93b","modified":1507435369291},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1507435369467},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1507435369471},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"53c894e6f3573c662dc4e4f7b5a6f1a32f1a8c94","modified":1507435369379},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1507435369474},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1507435369478},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1507435369486},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1507435369482},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1507435369247},{"_id":"themes/next/scripts/tags/button.js","hash":"aaf71be6b483fca7a65cd6296c2cf1c2271c26a6","modified":1507435369567},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1507435369571},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1507435369578},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1507435369574},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1507435369251},{"_id":"themes/next/scripts/tags/label.js","hash":"6f00952d70aadece844ce7fd27adc52816cc7374","modified":1507435369588},{"_id":"themes/next/scripts/tags/note.js","hash":"f7eae135f35cdab23728e9d0d88b76e00715faa0","modified":1507435369595},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1507435369599},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1507435369583},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1507435369592},{"_id":"themes/next/layout/_partials/header.swig","hash":"c54b32263bc8d75918688fb21f795103b3f57f03","modified":1507435369308},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1507435370048},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1507435370044},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1507435370037},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1507435370060},{"_id":"themes/next/source/images/apple-touch-icon.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1507435370041},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1507435370056},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1507435370029},{"_id":"themes/next/source/images/favicon-32x32.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1507435370078},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1507435370052},{"_id":"themes/next/source/images/favicon-16x16.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1507435370074},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1507435370071},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1507435370084},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1507435370081},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1507435370067},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1507435370088},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1507435370094},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1507435370064},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1507435370091},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1507435370097},{"_id":"source/_posts/auc-n-logloss/4.png","hash":"768177ead93284112e04805f9738479f3ab2fbf3","modified":1507435369001},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A52.2.png","hash":"ab179418ff1eeb992aaec1db8a7299c180cbd8b9","modified":1521427969357},{"_id":"source/_posts/ctr-recalibration/图2.2.png","hash":"ab179418ff1eeb992aaec1db8a7299c180cbd8b9","modified":1511441508302},{"_id":"source/_posts/cnn/cnn002.png","hash":"085da484b2f03ebab31484861b2a47a4c660bec9","modified":1521106276291},{"_id":"source/_posts/cnn/cnn004.png","hash":"2a9096e5228c8521c34579d85a6e0e2147ef254f","modified":1521166798694},{"_id":"source/_posts/lda/301.png","hash":"493c179e66444d0e3f92ffc70fa12a2914c36fcd","modified":1507435369028},{"_id":"source/_posts/lda/401.png","hash":"d9b18be14991ff22d5beb80e85bbc56fc5e599b3","modified":1507435369037},{"_id":"source/_posts/lda/5.png","hash":"f59c3d0cea4445366028261080d4e1945b8b1a78","modified":1521427969570},{"_id":"source/_posts/lda/4.png","hash":"ec1cc3151a7a8db835a6150dac1b3109898f1134","modified":1521427969570},{"_id":"source/_posts/lda/6.png","hash":"d2f2c684ef148b4aab12d54f9e1b14829bc0bee9","modified":1521427969580},{"_id":"source/_posts/lda/501.png","hash":"d671c983e48fc27e61c2690f8d2de05cb39635b1","modified":1507435369045},{"_id":"source/_posts/lda/7.png","hash":"48fc762aa53ae47d05caf6937ded51e9fcf9dded","modified":1521427969590},{"_id":"source/_posts/lda/102.png","hash":"10e75aed2f2f3cdee5972f3c4779a2fe34388505","modified":1521427969548},{"_id":"source/_posts/lda/201.png","hash":"1b600e59fec7e58cccbf38dced4a4d8953e3a5b8","modified":1521427969570},{"_id":"source/_posts/lda/601.jpg","hash":"1011ca8d03ebc9849b5ab510f6a10ee0366fdb47","modified":1507435369050},{"_id":"source/_posts/word2vec/cbow-1.png","hash":"40b9dfae96c6dc4a4f0ba65ff7198339ff604308","modified":1540889644419},{"_id":"source/_posts/word2vec/cbow-hs.png","hash":"40af8fde14726aa5d4e7bf24f1146b3da9495d60","modified":1540889679002},{"_id":"source/_posts/lda/8.png","hash":"be892636e7f7a78f8d86b2898f87ca31295007db","modified":1521427969590},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1507435369369},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1507435369371},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1507435369897},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1507435369899},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1507435369909},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1507435370014},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1507435370026},{"_id":"source/_posts/auc-n-logloss/3.png","hash":"e60e44b56a987bef83090ab1ac4a1d50b788e7ac","modified":1507435368997},{"_id":"source/_posts/cnn/lenet.png","hash":"1a63ac6a9fce5a59b243adc71965e8b55286d654","modified":1521427969337},{"_id":"source/_posts/hivemall/hivemall_1.PNG","hash":"2e1c089f2e96339ca09f7cc843c19ae7b9365ef0","modified":1521429535309},{"_id":"source/_posts/lda/402.png","hash":"fb0d940ff0c339a33ce2056c6089c1d1e2544aeb","modified":1507435369041},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1507435369328},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1507435369304},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1507435369300},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1507435369331},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1507435369344},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1507435369336},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1507435369340},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1507435369367},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1507435369324},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1507435369361},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1507435369375},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1507435369387},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"7b11eac3a0685fa1ab2ab6ecff60afc4f15f0d16","modified":1507435369398},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1507435369402},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1507435369391},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1507435369406},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1507435369395},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1507435369422},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b1e13df83fb2b1d5d513b30b7aa6158b0837daab","modified":1507435369410},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"45f3f629c2aacc381095750e1c8649041a71a84b","modified":1507435369414},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1507435369425},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1507435369435},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1507435369417},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1507435369429},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1507435369443},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1507435369447},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"92ea45b877b1fec2010c7b409f121c986ee5075b","modified":1507435369452},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b83a51bbe0f1e2ded9819070840b0ea145f003a6","modified":1507435369439},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"9246162d4bc7e949ce1d12d135cbbaf5dc3024ec","modified":1507435369455},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1507435369463},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"0a9cdd6958395fcdffc80ab60f0c6301b63664a5","modified":1507435369347},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1507435369501},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"602104d7ac47f7888d97e810419e58593a79e8ba","modified":1507435369459},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"93479642fd076a1257fecc25fcf5d20ccdefe509","modified":1507435369505},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1507435369509},{"_id":"source/_posts/lda/9.png","hash":"17570e2afd53b60ed2b6558c1ed88fa3b6742a4f","modified":1521427969590},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1507435369515},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1507435369895},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"7896c3ee107e1a8b9108b6019f1c070600a1e8cc","modified":1507435369904},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1507435369907},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"58e7dd5947817d9fc30770712fc39b2f52230d1e","modified":1507435370008},{"_id":"themes/next/source/css/_variables/base.styl","hash":"4fcfd48c7fce88ae3a0efa027bf739b8fad5437f","modified":1507435370023},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"cefa0189bd928b0b35b25fd5264b127828a469ca","modified":1507435370019},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"e55265c8a8a6ae0c3c08e3509de92ee62c3cb5f6","modified":1507435370013},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1507435369890},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"0289031200c3d4c2bdd801ee10fff13bb2c353e4","modified":1507435370112},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1507435370103},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1507435370124},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1507435370108},{"_id":"themes/next/source/js/src/post-details.js","hash":"93a18271b4123dd8f94f09d1439b47c3c19a8712","modified":1507435370133},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1507435370144},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1507435370116},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1507435370120},{"_id":"themes/next/source/js/src/utils.js","hash":"d6ce6939c9bd9a7e2ef1d8b15c836cbed02d715f","modified":1507435370153},{"_id":"themes/next/source/js/src/motion.js","hash":"885176ed51d468f662fbf0fc09611f45c7e5a3b1","modified":1507435370128},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1507435370199},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"672d3b5767e0eacd83bb41b188c913f2cf754793","modified":1507435370226},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1507435370212},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1507435370296},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"82fee688910efc644d3d1c3305c6ae28ba3f38f9","modified":1507435370217},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1507435370299},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1507435370303},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1507435370328},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1507435370331},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1507435370307},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1507435370324},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1507435370338},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1507435370334},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1507435370148},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1507435370396},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1507435370400},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1507435370408},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1507435370404},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1507435370416},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1507435370412},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1507435370422},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1507435370426},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1507435370443},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1507435370434},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1507435370430},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1507435370455},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1507435370451},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1507435370459},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1507435370447},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1507435370467},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1507435370470},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1507435370463},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1507435370481},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"9be892a4e14e0da18ff9cb962c9ef71f163b1b22","modified":1507435370223},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1507435370439},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1507435370485},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1507435370474},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1507435370489},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1507435370515},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1507435370535},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1507435370531},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1507435370387},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1507435370527},{"_id":"source/_posts/auc-n-logloss/1.gif","hash":"6aa137ccf577a9689570505ad4fa5eac52a784c0","modified":1507435368989},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1507435370519},{"_id":"source/_posts/cnn/alexNet.png","hash":"2abd32ba6a560379605ac5d554cfa0b10bb3253b","modified":1521427969307},{"_id":"source/_posts/lda/302.png","hash":"119cb4b9ba91acfb35ec5338011d1a196b296a72","modified":1507435369033},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1507435370391},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"8f86f694c0749a18ab3ad6f6df75466ca137a4bc","modified":1507435369610},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1507435369497},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"22828f5141c0cecb9ef25a110e194cdfa3a36423","modified":1507435369618},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1507435369625},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1507435369621},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1507435369614},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1507435369686},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1507435369786},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1507435369868},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"24ee4b356ff55fc6e58f26a929fa07750002cf29","modified":1507435369864},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1507435369493},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1507435369928},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1507435369881},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1507435369877},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1507435369932},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1507435369885},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"91ca75492cd51f2553f4d294ed2f48239fcd55eb","modified":1507435369872},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1507435369925},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1507435369938},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1507435369922},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1507435369982},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"416988dca389e6e2fdfa51fa7f4ee07eb53f82fb","modified":1507435369986},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1507435369942},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1507435369994},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1507435369961},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"643eb1ad5bef63e1f5eff13ed33fc7b21111189e","modified":1507435369934},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1507435369859},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1507435369958},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"60fa84aa7731760f05f52dd7d8f79b5f74ac478d","modified":1507435369916},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1507435369972},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1507435369969},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"4642e30010af8b2b037f5b43146b10a934941958","modified":1507435369990},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"798efe02a4bf52d8820f99a5a458cd3d8ad3c3cc","modified":1507435369998},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"86197902dfd3bededba10ba62b8f9f22e0420bde","modified":1507435370003},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1507435370191},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"f1d0b5d7af32c423eaa8bb93ab6a0b45655645dc","modified":1507435370140},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1507435370194},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1507435370235},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"f23ac53ab901c48859dd29eee6e386b60ff956ba","modified":1507435369965},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1507435370239},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1507435370183},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1507435370243},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1507435370249},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1507435370312},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1507435370288},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"d71602cbca33b9ecdb7ab291b7f86a49530f3601","modified":1507435370284},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1507435370347},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1507435370343},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1507435370351},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1507435370246},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1507435370231},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1507435370280},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1507435370318},{"_id":"source/_posts/word2vec/cbow-hs-hand.png","hash":"16d34f43bc475e90040a20880b99176d97e64bac","modified":1540891105751},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1507435370509},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1507435370503},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1507435370187},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1507435370379},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1507435370382},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1507435370524},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A51.1.png","hash":"8fc18abcde73fb0194e8eab4e1da7070fe0e0d5b","modified":1521427969347},{"_id":"source/_posts/ctr-recalibration/图1.1.png","hash":"8fc18abcde73fb0194e8eab4e1da7070fe0e0d5b","modified":1511440983413},{"_id":"source/_posts/cnn/cnn003.png","hash":"5f925b3cabb4d20d3b212c546aaeeab664d54555","modified":1521427969337},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1507435369646},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1507435369639},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1507435369650},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1507435369655},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1507435369643},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1507435369635},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"50450d9fdc8a2b2be8cfca51e3e1a01ffd636c0b","modified":1507435369658},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1507435369668},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1507435369663},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1507435369671},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f9760ecf186954cee3ba4a149be334e9ba296b89","modified":1507435369675},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1507435369683},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1507435369679},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1507435369703},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1507435369695},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"27b7326fa3bb09e9473f349984bfe69aa17277d2","modified":1507435369692},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1507435369699},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"a6c6eb8adba0a090ad1f4b9124e866887f20d10d","modified":1507435369706},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1507435369717},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d0d7a5c90d62b685520d2b47fea8ba6019ff5402","modified":1507435369713},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ca88ea6999a61fb905eb6e72eba5f92d4ee31e6e","modified":1507435369720},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1507435369729},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1507435369725},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1507435369710},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1507435369737},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"ccb34c52be8adba5996c6b94f9e723bd07d34c16","modified":1507435369732},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"2cb09973d29a8e34e2a3425ac6e0938296970d8e","modified":1507435369740},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1507435369753},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"681f8b41599964a6c60bd341f46cb15efc20423b","modified":1507435369744},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1507435369757},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1507435369760},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"39f04c4c7237a4e10acd3002331992b79945d241","modified":1507435369750},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1507435369775},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"28a8737c090fbffd188d73a00b42e90b9ee57df2","modified":1507435369771},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1507435369767},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1507435369783},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1507435369764},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1507435369791},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1507435369795},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"3159b55f35c40bd08e55b00148c523760a708c51","modified":1507435369798},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1507435369806},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1507435369802},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"962b654f8f7cbd18a298126a403d236ed4540516","modified":1507435369810},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1507435369823},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1507435369819},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1507435369835},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1507435369814},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1507435369843},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1507435369846},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1507435369839},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1507435369850},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1507435369828},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"37e406ec42b7a53c72395bdbaa434270019e7179","modified":1507435369854},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"9f73c4696f0907aa451a855444f88fc0698fa472","modified":1507435369630},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1507435369947},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"5ef6343835f484a2c0770bd1eb9cc443609e4c39","modified":1507435369779},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1507435369953},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1507435369977},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1507435369832},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1507435370166},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1507435370169},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1507435370176},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1507435370172},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1507435370258},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1507435370180},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1507435370254},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"ee948b4489aedeb548a77c9e45d8c7c5732fd62d","modified":1507435370262},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1507435370271},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"51139a4c79573d372a347ef01a493222a1eaf10a","modified":1507435370267},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1507435370358},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1507435370275},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1507435370364},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1507435370376},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1507435370207},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1507435370496},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1507435370370},{"_id":"source/_posts/word2vec/tfboard.png","hash":"98be0b89cc3f97e205e26dbe17cd336e101d07a2","modified":1540889533354},{"_id":"source/_posts/ctr-recalibration/图3.3.jpg","hash":"0d0d141f6967b62ec615766d43911993eea99b9a","modified":1511613949645},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A53.3.jpg","hash":"0d0d141f6967b62ec615766d43911993eea99b9a","modified":1521427969377},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A53.3.png","hash":"21b3b0eb1a89a2c3723673e632e463b17a0428b3","modified":1521427969528},{"_id":"source/_posts/ctr-recalibration/图3.3.png","hash":"21b3b0eb1a89a2c3723673e632e463b17a0428b3","modified":1511614082824},{"_id":"public/categories/index.html","hash":"42881f3c5597c13e12c4efde484594962c151178","modified":1542166703948},{"_id":"public/about/index.html","hash":"534c418a23eab3a468009bd546d143c8c6d1088b","modified":1542166703948},{"_id":"public/about/tmp.html","hash":"9ce24e359303fdd18bb4539b881c6a1f11adfbc3","modified":1542166703948},{"_id":"public/2018/10/17/ocpc/index.html","hash":"c69710c0d2668968d8e1186869f730b7b70d0d79","modified":1542166703949},{"_id":"public/2018/09/30/feature-yy-list/index.html","hash":"96517ed58a3626a37d06279b85d74da7b1d51152","modified":1542166703949},{"_id":"public/2018/09/30/tf-serving/index.html","hash":"1b64c854239340a3b9a1966819d04cce065f4f2b","modified":1542166703949},{"_id":"public/2018/09/27/dict-vs-bag-of-words/index.html","hash":"6949fa24c2ffc563e4e226477e6556520e3e5a79","modified":1542166703949},{"_id":"public/2018/09/10/data-warehouse/index.html","hash":"0ec85bddaa6075b55a330851114ccf2b428c2d88","modified":1542166703949},{"_id":"public/2018/04/02/elastic-search-md/index.html","hash":"5184f5b6dc51c8a52a0e0f19f7632846b2e50105","modified":1542166703949},{"_id":"public/2018/03/24/dnn-embedding/index.html","hash":"c2251f5622744ed1c823ef080338209ca019fd70","modified":1542166703949},{"_id":"public/2018/03/24/MLR/index.html","hash":"4b22e249e5d0fc282128d18e8b0debac4c52a6b1","modified":1542166703949},{"_id":"public/2018/03/20/query-parse/index.html","hash":"eade415da944e58b98e221be129368d326a1c48a","modified":1542166703949},{"_id":"public/2018/01/16/bayes/index.html","hash":"95eb45bb05acf572850c1c80f0245a8116b40bcf","modified":1542166703949},{"_id":"public/2018/01/15/edge-rank/index.html","hash":"29d07566600aa13c969e5578e6f829bc450e9107","modified":1542166703949},{"_id":"public/2017/12/15/kafka/index.html","hash":"a281ebeb260f7c7ddecc0f1b0189260dddd1f027","modified":1542166703949},{"_id":"public/2017/12/13/dt/index.html","hash":"65c8b73f29abcae7b3258f2719ed41d0c69554af","modified":1542166703949},{"_id":"public/2017/12/14/distributed-tf/index.html","hash":"c0df3c37cb5a71d02ab9bacd2fbab5b0ce792124","modified":1542166703949},{"_id":"public/2017/11/23/exploding-n-vanishing/index.html","hash":"1ad6e6334b8c68746cfe9607de9d45ee0c82134f","modified":1542166703949},{"_id":"public/2017/10/28/xgboost-n-spark/index.html","hash":"1bb8a986dc85d4baae117bd1268694fce987d127","modified":1542166703949},{"_id":"public/2017/11/17/spark-streaming/index.html","hash":"79d1ec387771d35ad537b9e912ab7f54e16d863f","modified":1542166703949},{"_id":"public/2017/10/26/subway/index.html","hash":"5993bef4ab579d6eaf24252299c1ab29b1332138","modified":1542166703949},{"_id":"public/2017/10/10/logit-n-probit/index.html","hash":"22c1d66ac22ffa6f8fe1aede7065faf8e8d5186b","modified":1542166703949},{"_id":"public/2017/10/15/pr/index.html","hash":"333b0c59fd33280e1b09b7dc70d1cde63805da5c","modified":1542166703950},{"_id":"public/archives/page/4/index.html","hash":"6f03be7140839bf7209b8e003268af16874981cc","modified":1542166703950},{"_id":"public/archives/2017/page/2/index.html","hash":"7c2c8d8b351d6d88c5e876824b3c7d4169addf98","modified":1542166703950},{"_id":"public/archives/2017/10/index.html","hash":"ab0d1d3771ee84ae78e5e143307d82b182ede6a6","modified":1542166703950},{"_id":"public/archives/2017/11/index.html","hash":"47e45cb144e536f2b35f83865e43aa4799498095","modified":1542166703950},{"_id":"public/archives/2017/12/index.html","hash":"16636af22801d4c8ea21d1b8749f6c8a62509628","modified":1542166703950},{"_id":"public/archives/2018/page/2/index.html","hash":"f5484e0196ecb7b80182a96f69f7e2bbb3d46f24","modified":1542166703950},{"_id":"public/archives/2018/01/index.html","hash":"843c53ef8ef0f3c326e060e98e32879ba953a66f","modified":1542166703950},{"_id":"public/archives/2018/03/index.html","hash":"725d3d4a20c6861c9c7e2c3acede0657d6aeac65","modified":1542166703950},{"_id":"public/archives/2018/04/index.html","hash":"b479b7479944a56b939c2d9212e2010c0d494c32","modified":1542166703950},{"_id":"public/archives/2018/06/index.html","hash":"71c3948e5d67e67e177292546e0432f624dde2c2","modified":1542166703950},{"_id":"public/archives/2018/08/index.html","hash":"63c670e6bb10dc863f2e569605a4b214a853b6e2","modified":1542166703950},{"_id":"public/archives/2018/09/index.html","hash":"088cb0bbbcd08362133a184388ef62eb6f5e4c3f","modified":1542166703950},{"_id":"public/archives/2018/10/index.html","hash":"a6cd4c088bb670aa72ed9688451ee721a58f4d6a","modified":1542166703950},{"_id":"public/categories/机器学习/index.html","hash":"db248b2e2da10f2c55078c8a6fc9cff349374be7","modified":1542166703950},{"_id":"public/categories/数据挖掘/index.html","hash":"61611ad9ed06c719f264d5f54bf2d12489c1e09c","modified":1542166703950},{"_id":"public/tags/机器学习/index.html","hash":"d882b4a3a470ca126459705590289da90ed36869","modified":1542166703950},{"_id":"public/tags/ML/index.html","hash":"e98d7fd8de9c1614cfc4de15d95939d6612ff1a4","modified":1542166703950},{"_id":"public/tags/数据挖掘/index.html","hash":"43e8433f25ced9540dc854db19d6794677b47d11","modified":1542166703950},{"_id":"public/tags/DNN/index.html","hash":"0be2e70ad63c8615be3cecedf633bbfbb14edb7a","modified":1542166703950},{"_id":"public/tags/Search/index.html","hash":"e501d0b56b8e64a76a30ec8c26c514d1322bb9d0","modified":1542166703950},{"_id":"public/2018/08/27/ctr-recalibration/index.html","hash":"297844227425f42e0533850cb508b5b9aaeee7ab","modified":1542166703950},{"_id":"public/2018/08/05/interview-questions/index.html","hash":"bdcd8733bbae4b7263d3f3c78c513d8f3ade6dec","modified":1542166703950},{"_id":"public/2018/06/21/hive-param-tune/index.html","hash":"e48cf2729021af5772fd12cf65a88abad1f34372","modified":1542166703951},{"_id":"public/2018/06/13/word2vec/index.html","hash":"54d65a2f122928b735cb6ff32730598ccce7eb27","modified":1542166703951},{"_id":"public/2018/03/19/hivemall/index.html","hash":"b3301902e7002e9caac9ccc99f5e6b43f1830870","modified":1542166703951},{"_id":"public/2018/03/14/cnn/index.html","hash":"47ceaf01b72d06bb41cbf4309fed85b9512bde95","modified":1542166703951},{"_id":"public/2018/01/08/search/index.html","hash":"e84c7efeebd8cbfdb804b7ed1348959653c32b74","modified":1542166703951},{"_id":"public/2018/01/05/tf-wnd/index.html","hash":"37db8fef528e3115fea78ae337ceba3b9cbefc89","modified":1542166703951},{"_id":"public/2017/12/07/ctr-smooth/index.html","hash":"d307de32f5b788919ded21099b5ea8cc7591abb7","modified":1542166703951},{"_id":"public/2017/11/27/MachineLearning-ZhouZhihua/index.html","hash":"a0b4012987f519c78b02985527908df6f0a6175c","modified":1542166703951},{"_id":"public/2017/10/24/ee-n-dqn/index.html","hash":"8224e805a92f011ecbf5522321dd2c61e70cf11f","modified":1542166703951},{"_id":"public/2017/10/06/auc-n-logloss/index.html","hash":"2e4cd364511f6f798bc941996609ac85e67ed41a","modified":1542166703951},{"_id":"public/2017/10/02/feature-engineer/index.html","hash":"b2d46f185194f984281f669741565576fc9b1f12","modified":1542166703951},{"_id":"public/2017/10/05/lda/index.html","hash":"340ceba66217d397206393039b1b30c1831455a8","modified":1542166703951},{"_id":"public/archives/index.html","hash":"a9f124947f68444b93623456743157e8553037df","modified":1542166703951},{"_id":"public/archives/page/2/index.html","hash":"21ccab510b788bcaf8ef74bb5e819c3516e049cd","modified":1542166703951},{"_id":"public/archives/page/3/index.html","hash":"613f7b0b87ed77ed7dbd51a87d44b87e4e4bd694","modified":1542166703951},{"_id":"public/archives/2017/index.html","hash":"2250879a2b6c3277a9de5e7143c13b5c7bd3579a","modified":1542166703951},{"_id":"public/archives/2018/index.html","hash":"2ee54d2d2ed7a39f0fa8d1bc737a6bc7b89a4bcf","modified":1542166703951},{"_id":"public/index.html","hash":"92e3a1e55e9f6baaa68bb29f3d7215cf1a751c04","modified":1542166703951},{"_id":"public/page/2/index.html","hash":"4bed697937814cc5e91c20bc541138c0e65d2f28","modified":1542166703951},{"_id":"public/page/3/index.html","hash":"6e9f7b8087d1646de0392ac314f88d60d8b82bb1","modified":1542166703951},{"_id":"public/page/4/index.html","hash":"cfa5987ddaf2d559df8af2b2b1edd085e614299e","modified":1542166703951},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1542166703980},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1542166703980},{"_id":"public/images/apple-touch-icon.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1542166703980},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1542166703980},{"_id":"public/images/favicon-32x32.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1542166703980},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1542166703981},{"_id":"public/images/favicon-16x16.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1542166703981},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1542166703981},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1542166703981},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1542166703981},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1542166703981},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1542166703981},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1542166703981},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1542166703981},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1542166703981},{"_id":"public/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1542166703981},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1542166703981},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1542166703981},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1542166703981},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1542166703982},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1542166703982},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1542166703982},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1542166703982},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1542166703982},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1542166703982},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1542166703982},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1542166703982},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1542166703982},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1542166703982},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1542166703982},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1542166703982},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1542166703982},{"_id":"public/2017/10/02/feature-engineer/զ%A6ڥ%E4%FE%F6+%B5%AB%C1.png","hash":"2141bdfa44e5cf86009924a72dca0946f5ca9601","modified":1542166703982},{"_id":"public/2017/10/24/ee-n-dqn/2.png","hash":"6515750b1ed88b7f6bf60910052c0b64820f7d82","modified":1542166703982},{"_id":"public/2017/10/10/logit-n-probit/logistic.png","hash":"d14da93bd72cd7c5880caf24323fb0af3b863162","modified":1542166703982},{"_id":"public/2017/10/24/ee-n-dqn/1.png","hash":"d0ba7aa13143ebf0d5987f1cb2487a7944ea36cf","modified":1542166703982},{"_id":"public/2017/10/06/auc-n-logloss/2.png","hash":"4f007082dc559888d479918e5419eff48391a830","modified":1542166703982},{"_id":"public/2017/10/10/logit-n-probit/logit.png","hash":"318241cd887a667d301d8bd068d6791da4dba7ab","modified":1542166703982},{"_id":"public/2017/10/10/logit-n-probit/logit-logistic-relation.jpg","hash":"4bc9396bf4c63a5e6e03efad1f0d08a3226c5127","modified":1542166703982},{"_id":"public/2017/10/06/auc-n-logloss/6.png","hash":"bf400d2df199beb6cd11f0724934b37ec6a9e5e4","modified":1542166703982},{"_id":"public/2017/10/06/auc-n-logloss/7.png","hash":"80956992425cd24224413be93dd711b23b33d5ff","modified":1542166703983},{"_id":"public/2017/10/06/auc-n-logloss/8.png","hash":"2b160d7859c0433086fb14404165ee6db04ea68c","modified":1542166703983},{"_id":"public/2017/10/06/auc-n-logloss/9.png","hash":"8bdd4379bb480099a83fa956b65c4bd30ce223f5","modified":1542166703983},{"_id":"public/2018/08/27/ctr-recalibration/%D5%F8%A54.4.png","hash":"816d988a281cf7d1dac9a856919abc4e7ea8ea16","modified":1542166703984},{"_id":"public/2017/10/05/lda/1.png","hash":"18b316e12d4591fa543bcb5db5ea8dd8fb089ed8","modified":1542166703984},{"_id":"public/2018/08/27/ctr-recalibration/图4.4.png","hash":"816d988a281cf7d1dac9a856919abc4e7ea8ea16","modified":1542166703984},{"_id":"public/2017/10/05/lda/104.png","hash":"dd5fe476184597c8871fd0a581ae15fd5528f862","modified":1542166703984},{"_id":"public/2017/10/05/lda/101.png","hash":"e8a736596eaab0a2863ebaf7cfc2806c05cf2b48","modified":1542166703984},{"_id":"public/2017/10/05/lda/105.png","hash":"a46d624d3e25bfdfe15dc68dc2d7e17ab2bda16f","modified":1542166703984},{"_id":"public/2017/10/05/lda/103.png","hash":"51596b033c3ef8457f11590c7eb35c9be0f08c30","modified":1542166703984},{"_id":"public/2017/10/05/lda/100.png","hash":"9b36fbe342f6fe0c7ebd224c6bc4024c77509e2f","modified":1542166703984},{"_id":"public/2017/10/05/lda/2.png","hash":"4efddf70286df1d8b3ae4f69597eeb9f159ce682","modified":1542166703984},{"_id":"public/2017/10/05/lda/3.png","hash":"63dc46e8d0254975359f8c02d560a417bcc7686c","modified":1542166703984},{"_id":"public/2017/10/05/lda/106.png","hash":"e24199f39e09a9034f4c37b9dd6d56060dd71518","modified":1542166703984},{"_id":"public/2018/01/08/search/IDF.png","hash":"a9d19b122aaad81b2ebdf7ac47382bda3a9a6d30","modified":1542166703984},{"_id":"public/2018/01/08/search/TF.png","hash":"a1e40176a572fccb513815595fc6288543bd0a1c","modified":1542166703984},{"_id":"public/2018/03/14/cnn/logistic.png","hash":"d14da93bd72cd7c5880caf24323fb0af3b863162","modified":1542166703984},{"_id":"public/2018/06/13/word2vec/cbow-hs-2.png","hash":"617e210a750b28fcf44480cb41778f9a7b7ce88b","modified":1542166703984},{"_id":"public/2018/06/13/word2vec/cbow-hs-code.png","hash":"fcf93e9550a06c0de3435a11247efee95f26344d","modified":1542166703984},{"_id":"public/2018/06/13/word2vec/cbow-hs-hand.jpg","hash":"a8f650036accc0f25ff56d01bc8f0a3cf5889d21","modified":1542166703984},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1542166703984},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1542166705129},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1542166705139},{"_id":"public/2017/10/06/auc-n-logloss/4.png","hash":"768177ead93284112e04805f9738479f3ab2fbf3","modified":1542166705143},{"_id":"public/2018/08/27/ctr-recalibration/%D5%F8%A52.2.png","hash":"ab179418ff1eeb992aaec1db8a7299c180cbd8b9","modified":1542166705143},{"_id":"public/2018/08/27/ctr-recalibration/图2.2.png","hash":"ab179418ff1eeb992aaec1db8a7299c180cbd8b9","modified":1542166705143},{"_id":"public/2017/10/05/lda/102.png","hash":"10e75aed2f2f3cdee5972f3c4779a2fe34388505","modified":1542166705144},{"_id":"public/2017/10/05/lda/4.png","hash":"ec1cc3151a7a8db835a6150dac1b3109898f1134","modified":1542166705144},{"_id":"public/2017/10/05/lda/201.png","hash":"1b600e59fec7e58cccbf38dced4a4d8953e3a5b8","modified":1542166705144},{"_id":"public/2017/10/05/lda/5.png","hash":"f59c3d0cea4445366028261080d4e1945b8b1a78","modified":1542166705144},{"_id":"public/2017/10/05/lda/501.png","hash":"d671c983e48fc27e61c2690f8d2de05cb39635b1","modified":1542166705144},{"_id":"public/2017/10/05/lda/301.png","hash":"493c179e66444d0e3f92ffc70fa12a2914c36fcd","modified":1542166705144},{"_id":"public/2017/10/05/lda/6.png","hash":"d2f2c684ef148b4aab12d54f9e1b14829bc0bee9","modified":1542166705145},{"_id":"public/2017/10/05/lda/401.png","hash":"d9b18be14991ff22d5beb80e85bbc56fc5e599b3","modified":1542166705145},{"_id":"public/2017/10/05/lda/601.jpg","hash":"1011ca8d03ebc9849b5ab510f6a10ee0366fdb47","modified":1542166705145},{"_id":"public/2017/10/05/lda/7.png","hash":"48fc762aa53ae47d05caf6937ded51e9fcf9dded","modified":1542166705145},{"_id":"public/2017/10/05/lda/8.png","hash":"be892636e7f7a78f8d86b2898f87ca31295007db","modified":1542166705145},{"_id":"public/2018/03/14/cnn/cnn004.png","hash":"2a9096e5228c8521c34579d85a6e0e2147ef254f","modified":1542166705145},{"_id":"public/2018/03/14/cnn/cnn002.png","hash":"085da484b2f03ebab31484861b2a47a4c660bec9","modified":1542166705145},{"_id":"public/2018/06/13/word2vec/cbow-1.png","hash":"40b9dfae96c6dc4a4f0ba65ff7198339ff604308","modified":1542166705145},{"_id":"public/2018/06/13/word2vec/cbow-hs.png","hash":"40af8fde14726aa5d4e7bf24f1146b3da9495d60","modified":1542166705145},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1542166705155},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1542166705155},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1542166705155},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1542166705155},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1542166705155},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1542166705155},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1542166705155},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1542166705155},{"_id":"public/js/src/utils.js","hash":"6b0eeeb9dda4a7c94c1c4f6fafd2c801da6e8f96","modified":1542166705155},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1542166705155},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1542166705155},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1542166705155},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1542166705155},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1542166705155},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1542166705155},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1542166705155},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1542166705155},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1542166705156},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1542166705156},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1542166705156},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1542166705156},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1542166705156},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1542166705156},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1542166705156},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1542166705156},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1542166705156},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1542166705156},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1542166705156},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1542166705156},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1542166705156},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1542166705156},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1542166705156},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1542166705156},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1542166705156},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1542166705156},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1542166705156},{"_id":"public/css/main.css","hash":"4d7a1a025d51742d5077394110b72971882ff0c0","modified":1542166705157},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1542166705157},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1542166705157},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1542166705157},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1542166705157},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1542166705157},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1542166705157},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1542166705157},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1542166705157},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1542166705157},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1542166705157},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1542166705157},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1542166705157},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1542166705157},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1542166705157},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1542166705157},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1542166705157},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1542166705157},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1542166705157},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1542166705157},{"_id":"public/2017/10/06/auc-n-logloss/3.png","hash":"e60e44b56a987bef83090ab1ac4a1d50b788e7ac","modified":1542166705158},{"_id":"public/2017/10/05/lda/9.png","hash":"17570e2afd53b60ed2b6558c1ed88fa3b6742a4f","modified":1542166705158},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1542166705158},{"_id":"public/2018/03/19/hivemall/hivemall_1.PNG","hash":"2e1c089f2e96339ca09f7cc843c19ae7b9365ef0","modified":1542166705158},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1542166705158},{"_id":"public/2017/10/05/lda/402.png","hash":"fb0d940ff0c339a33ce2056c6089c1d1e2544aeb","modified":1542166705158},{"_id":"public/2018/03/14/cnn/lenet.png","hash":"1a63ac6a9fce5a59b243adc71965e8b55286d654","modified":1542166705158},{"_id":"public/2017/10/06/auc-n-logloss/1.gif","hash":"6aa137ccf577a9689570505ad4fa5eac52a784c0","modified":1542166705172},{"_id":"public/2018/03/14/cnn/alexNet.png","hash":"2abd32ba6a560379605ac5d554cfa0b10bb3253b","modified":1542166705172},{"_id":"public/2017/10/05/lda/302.png","hash":"119cb4b9ba91acfb35ec5338011d1a196b296a72","modified":1542166705172},{"_id":"public/2018/06/13/word2vec/cbow-hs-hand.png","hash":"16d34f43bc475e90040a20880b99176d97e64bac","modified":1542166705194},{"_id":"public/2018/08/27/ctr-recalibration/%D5%F8%A51.1.png","hash":"8fc18abcde73fb0194e8eab4e1da7070fe0e0d5b","modified":1542166705212},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1542166705212},{"_id":"public/2018/08/27/ctr-recalibration/图1.1.png","hash":"8fc18abcde73fb0194e8eab4e1da7070fe0e0d5b","modified":1542166705212},{"_id":"public/2018/03/14/cnn/cnn003.png","hash":"5f925b3cabb4d20d3b212c546aaeeab664d54555","modified":1542166705212},{"_id":"public/2018/06/13/word2vec/tfboard.png","hash":"98be0b89cc3f97e205e26dbe17cd336e101d07a2","modified":1542166705537},{"_id":"public/2018/08/27/ctr-recalibration/图3.3.jpg","hash":"0d0d141f6967b62ec615766d43911993eea99b9a","modified":1542166705638},{"_id":"public/2018/08/27/ctr-recalibration/%D5%F8%A53.3.jpg","hash":"0d0d141f6967b62ec615766d43911993eea99b9a","modified":1542166705638},{"_id":"public/2018/08/27/ctr-recalibration/%D5%F8%A53.3.png","hash":"21b3b0eb1a89a2c3723673e632e463b17a0428b3","modified":1542166705731},{"_id":"public/2018/08/27/ctr-recalibration/图3.3.png","hash":"21b3b0eb1a89a2c3723673e632e463b17a0428b3","modified":1542166705742}],"Category":[{"name":"机器学习","_id":"cjogm8g9o0008tcin6khce5j5"},{"name":"数据挖掘","_id":"cjogm8gbi0016tcinjyft21cx"}],"Data":[],"Page":[{"title":"categories","date":"2017-10-02T13:07:45.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-10-02 21:07:45\ntype: categories\n---\n","updated":"2017-10-08T04:02:49.120Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjogm8g8m0001tcinha3l339y","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"","date":"2017-10-02T12:29:04.000Z","_content":"\n### 博主简介\n推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。\n\n### Contact me\nsampsonguo302@gmail.com\n","source":"about/index.md","raw":"---\ntitle:\ndate: 2017-10-02 20:29:04\n---\n\n### 博主简介\n推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。\n\n### Contact me\nsampsonguo302@gmail.com\n","updated":"2017-10-08T04:02:49.115Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjogm8g8q0003tcinkufc1fxd","content":"<h3 id=\"博主简介\"><a href=\"#博主简介\" class=\"headerlink\" title=\"博主简介\"></a>博主简介</h3><p>推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。</p>\n<h3 id=\"Contact-me\"><a href=\"#Contact-me\" class=\"headerlink\" title=\"Contact me\"></a>Contact me</h3><p>sampsonguo302@gmail.com</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"博主简介\"><a href=\"#博主简介\" class=\"headerlink\" title=\"博主简介\"></a>博主简介</h3><p>推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。</p>\n<h3 id=\"Contact-me\"><a href=\"#Contact-me\" class=\"headerlink\" title=\"Contact me\"></a>Contact me</h3><p>sampsonguo302@gmail.com</p>\n"},{"_content":"推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。","source":"about/tmp.md","raw":"推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。","date":"2017-10-19T02:11:42.819Z","updated":"2017-10-19T02:11:42.819Z","path":"about/tmp.html","title":"","comments":1,"layout":"page","_id":"cjogm8g920006tcinpeajtcri","content":"<p>推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。</p>\n"}],"Post":[{"title":"MLR","date":"2018-03-24T14:35:58.000Z","_content":"\n#### MLR\nMLR = mixed logistic regression\nLS-PLM = Large Scale Piece-wise Linear Model\n\n####","source":"_posts/MLR.md","raw":"---\ntitle: MLR\ndate: 2018-03-24 22:35:58\ntags: 机器学习\n---\n\n#### MLR\nMLR = mixed logistic regression\nLS-PLM = Large Scale Piece-wise Linear Model\n\n####","slug":"MLR","published":1,"updated":"2018-03-26T10:55:07.388Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8g8a0000tcinywk3w626","content":"<h4 id=\"MLR\"><a href=\"#MLR\" class=\"headerlink\" title=\"MLR\"></a>MLR</h4><p>MLR = mixed logistic regression<br>LS-PLM = Large Scale Piece-wise Linear Model</p>\n<p>####</p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"MLR\"><a href=\"#MLR\" class=\"headerlink\" title=\"MLR\"></a>MLR</h4><p>MLR = mixed logistic regression<br>LS-PLM = Large Scale Piece-wise Linear Model</p>\n<p>####</p>\n"},{"title":"ctr_recalibration","date":"2018-08-27T12:21:40.000Z","_content":"\n### CTR为什么会不准？\n在计算广告中，pCTR往往对比真实的CTR偏高或者偏低的现象，尤其在\n1. 热门曝光广告和冷门曝光广告之间\n2. 高CTR广告和低CTR广告之间\n\n因此，CTR需要校准。\n\n<!-- more -->\n\n### CTR为什么要校准？\nAUC体现序准；\nlogloss体现值准；\n要计算商业价值最大化，因此需要值准；\n因此需要校准；\n校准之后，logloss会降低。\n\n### CTR如何校准\nCTR校准有很多方法，本质在于“拟合校准前和校准后”，即\nf(pCTR校准前) = pCTR校准后\n如何设计函数f，是校准的关键。\n\n#### binning\nbinning就是样本等频分桶后，每个bin求平均，如下图：\n {% asset_img \"图1.1.png\" [图1.1.png] %}\n\n#### Isotonic regression(保序回归）\n保序回归，就是单调回归（保证按照自变量x和按照因变量y排序序不变，即成正比）\n为何要保序？\n为了保证不影响AUC，即默认原始CTR和校准后CTR的正相关性。\n{% asset_img \"图2.2.png\" [图2.2.png] %}\n\n### Best practice\n#### 分解动作\n* 将统计ctr加入特征中（最好做离散化处理）\n* 建立f(pCTR)=统计CTR的函数\n* 进行将f(pCTR)作为新的CTR\n#### 小demo\n假设训练数据集合为：\n物品3：pCTR_统计=0.8\n物品2：pCTR_统计=0.5\n物品1：pCTR_统计=0.3\n\n##### 原始LR\n```\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nX = [\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3]\n]\ny = [\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n0,\n0]\nLR = LogisticRegression()\nLR.fit(X, y)\ny_p = LR.predict_proba(X)\nscore = log_loss(y, y_p)\n```\n\n##### LR+保序回归\n```\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.isotonic import IsotonicRegression\nX = [\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3]\n]\ny = [\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n0,\n0]\nLR = LogisticRegression()\nLR.fit(X, y)\ny_lr = LR.predict_proba(X)\nir = IsotonicRegression()\ny_ir = ir.fit_transform(map(lambda x:x[1], y_lr), map(lambda x:x[0], X))\nscore = log_loss(y, y_ir)\n```\n\n##### itemID离散化LR\n```\nX = [\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n]\ny = [\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n0,\n0]\nLR = LogisticRegression()\nLR.fit(X, y)\ny_p = LR.predict_proba(X)\nscore = log_loss(y, y_p)\n```\n\n### 采样的校准\n由于负样本抽样后，会造成点击率偏高的假象，需要将预测值还原成真实的值。调整的公式如下：\n\n#### 为什么要采样\n1. 加快训练速度\n2. 某些模型需要正负样本比例不要太不均匀，例如nn要每个batch里有正样本\n\n#### 结论\n{% asset_img \"图4.4.png\" [图4.4.png] %}\n\n#### 推导1\n* 从结果侧推导\n{% asset_img \"图3.3.png\" [图3.3.png] %}\n\n#### 推导2\n* 证明采样之后只需要加一个bias即可校正\n* 全量集合N，正样本a，负样本b，采样比例k，扩大比例为1/k\n* p = a / (a + b) = 1 / (1+e^-wx)\n* => b/a = e^-wx\n* p' = a / (a + b/k) = 1 / (1+e^-wx-lnk) = 1 / e^-(wx+lnk)\n\n\n##### 举一个例子\n负样本采样10%\n校准回去 q = p / (10 - 9p)\np: 模型预估的pCTR\nq: 真正要输出的ctr\n\n\nREF:\nhttps://tech.meituan.com/mt_dsp.html\nhttp://blog.csdn.net/lming_08/article/details/40214921\n\n","source":"_posts/ctr-recalibration.md","raw":"---\ntitle: ctr_recalibration\ndate: 2018-08-27 20:21:40\ntags: ML\n---\n\n### CTR为什么会不准？\n在计算广告中，pCTR往往对比真实的CTR偏高或者偏低的现象，尤其在\n1. 热门曝光广告和冷门曝光广告之间\n2. 高CTR广告和低CTR广告之间\n\n因此，CTR需要校准。\n\n<!-- more -->\n\n### CTR为什么要校准？\nAUC体现序准；\nlogloss体现值准；\n要计算商业价值最大化，因此需要值准；\n因此需要校准；\n校准之后，logloss会降低。\n\n### CTR如何校准\nCTR校准有很多方法，本质在于“拟合校准前和校准后”，即\nf(pCTR校准前) = pCTR校准后\n如何设计函数f，是校准的关键。\n\n#### binning\nbinning就是样本等频分桶后，每个bin求平均，如下图：\n {% asset_img \"图1.1.png\" [图1.1.png] %}\n\n#### Isotonic regression(保序回归）\n保序回归，就是单调回归（保证按照自变量x和按照因变量y排序序不变，即成正比）\n为何要保序？\n为了保证不影响AUC，即默认原始CTR和校准后CTR的正相关性。\n{% asset_img \"图2.2.png\" [图2.2.png] %}\n\n### Best practice\n#### 分解动作\n* 将统计ctr加入特征中（最好做离散化处理）\n* 建立f(pCTR)=统计CTR的函数\n* 进行将f(pCTR)作为新的CTR\n#### 小demo\n假设训练数据集合为：\n物品3：pCTR_统计=0.8\n物品2：pCTR_统计=0.5\n物品1：pCTR_统计=0.3\n\n##### 原始LR\n```\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nX = [\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3]\n]\ny = [\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n0,\n0]\nLR = LogisticRegression()\nLR.fit(X, y)\ny_p = LR.predict_proba(X)\nscore = log_loss(y, y_p)\n```\n\n##### LR+保序回归\n```\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.isotonic import IsotonicRegression\nX = [\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3]\n]\ny = [\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n0,\n0]\nLR = LogisticRegression()\nLR.fit(X, y)\ny_lr = LR.predict_proba(X)\nir = IsotonicRegression()\ny_ir = ir.fit_transform(map(lambda x:x[1], y_lr), map(lambda x:x[0], X))\nscore = log_loss(y, y_ir)\n```\n\n##### itemID离散化LR\n```\nX = [\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n]\ny = [\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n0,\n0]\nLR = LogisticRegression()\nLR.fit(X, y)\ny_p = LR.predict_proba(X)\nscore = log_loss(y, y_p)\n```\n\n### 采样的校准\n由于负样本抽样后，会造成点击率偏高的假象，需要将预测值还原成真实的值。调整的公式如下：\n\n#### 为什么要采样\n1. 加快训练速度\n2. 某些模型需要正负样本比例不要太不均匀，例如nn要每个batch里有正样本\n\n#### 结论\n{% asset_img \"图4.4.png\" [图4.4.png] %}\n\n#### 推导1\n* 从结果侧推导\n{% asset_img \"图3.3.png\" [图3.3.png] %}\n\n#### 推导2\n* 证明采样之后只需要加一个bias即可校正\n* 全量集合N，正样本a，负样本b，采样比例k，扩大比例为1/k\n* p = a / (a + b) = 1 / (1+e^-wx)\n* => b/a = e^-wx\n* p' = a / (a + b/k) = 1 / (1+e^-wx-lnk) = 1 / e^-(wx+lnk)\n\n\n##### 举一个例子\n负样本采样10%\n校准回去 q = p / (10 - 9p)\np: 模型预估的pCTR\nq: 真正要输出的ctr\n\n\nREF:\nhttps://tech.meituan.com/mt_dsp.html\nhttp://blog.csdn.net/lming_08/article/details/40214921\n\n","slug":"ctr-recalibration","published":1,"updated":"2018-11-14T03:38:01.663Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8g8o0002tcin1u7onrsn","content":"<h3 id=\"CTR为什么会不准？\"><a href=\"#CTR为什么会不准？\" class=\"headerlink\" title=\"CTR为什么会不准？\"></a>CTR为什么会不准？</h3><p>在计算广告中，pCTR往往对比真实的CTR偏高或者偏低的现象，尤其在</p>\n<ol>\n<li>热门曝光广告和冷门曝光广告之间</li>\n<li>高CTR广告和低CTR广告之间</li>\n</ol>\n<p>因此，CTR需要校准。</p>\n<a id=\"more\"></a>\n<h3 id=\"CTR为什么要校准？\"><a href=\"#CTR为什么要校准？\" class=\"headerlink\" title=\"CTR为什么要校准？\"></a>CTR为什么要校准？</h3><p>AUC体现序准；<br>logloss体现值准；<br>要计算商业价值最大化，因此需要值准；<br>因此需要校准；<br>校准之后，logloss会降低。</p>\n<h3 id=\"CTR如何校准\"><a href=\"#CTR如何校准\" class=\"headerlink\" title=\"CTR如何校准\"></a>CTR如何校准</h3><p>CTR校准有很多方法，本质在于“拟合校准前和校准后”，即<br>f(pCTR校准前) = pCTR校准后<br>如何设计函数f，是校准的关键。</p>\n<h4 id=\"binning\"><a href=\"#binning\" class=\"headerlink\" title=\"binning\"></a>binning</h4><p>binning就是样本等频分桶后，每个bin求平均，如下图：<br> <img src=\"/2018/08/27/ctr-recalibration/图1.1.png\" alt=\"[图1.1.png]\" title=\"[图1.1.png]\"></p>\n<h4 id=\"Isotonic-regression-保序回归）\"><a href=\"#Isotonic-regression-保序回归）\" class=\"headerlink\" title=\"Isotonic regression(保序回归）\"></a>Isotonic regression(保序回归）</h4><p>保序回归，就是单调回归（保证按照自变量x和按照因变量y排序序不变，即成正比）<br>为何要保序？<br>为了保证不影响AUC，即默认原始CTR和校准后CTR的正相关性。<br><img src=\"/2018/08/27/ctr-recalibration/图2.2.png\" alt=\"[图2.2.png]\" title=\"[图2.2.png]\"></p>\n<h3 id=\"Best-practice\"><a href=\"#Best-practice\" class=\"headerlink\" title=\"Best practice\"></a>Best practice</h3><h4 id=\"分解动作\"><a href=\"#分解动作\" class=\"headerlink\" title=\"分解动作\"></a>分解动作</h4><ul>\n<li>将统计ctr加入特征中（最好做离散化处理）</li>\n<li>建立f(pCTR)=统计CTR的函数</li>\n<li>进行将f(pCTR)作为新的CTR<h4 id=\"小demo\"><a href=\"#小demo\" class=\"headerlink\" title=\"小demo\"></a>小demo</h4>假设训练数据集合为：<br>物品3：pCTR<em>统计=0.8<br>物品2：pCTR</em>统计=0.5<br>物品1：pCTR_统计=0.3</li>\n</ul>\n<h5 id=\"原始LR\"><a href=\"#原始LR\" class=\"headerlink\" title=\"原始LR\"></a>原始LR</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div></pre></td><td class=\"code\"><pre><div class=\"line\">from sklearn.metrics import log_loss</div><div class=\"line\">from sklearn.linear_model import LogisticRegression</div><div class=\"line\">X = [</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3]</div><div class=\"line\">]</div><div class=\"line\">y = [</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0]</div><div class=\"line\">LR = LogisticRegression()</div><div class=\"line\">LR.fit(X, y)</div><div class=\"line\">y_p = LR.predict_proba(X)</div><div class=\"line\">score = log_loss(y, y_p)</div></pre></td></tr></table></figure>\n<h5 id=\"LR-保序回归\"><a href=\"#LR-保序回归\" class=\"headerlink\" title=\"LR+保序回归\"></a>LR+保序回归</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div></pre></td><td class=\"code\"><pre><div class=\"line\">from sklearn.metrics import log_loss</div><div class=\"line\">from sklearn.linear_model import LogisticRegression</div><div class=\"line\">from sklearn.isotonic import IsotonicRegression</div><div class=\"line\">X = [</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3]</div><div class=\"line\">]</div><div class=\"line\">y = [</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0]</div><div class=\"line\">LR = LogisticRegression()</div><div class=\"line\">LR.fit(X, y)</div><div class=\"line\">y_lr = LR.predict_proba(X)</div><div class=\"line\">ir = IsotonicRegression()</div><div class=\"line\">y_ir = ir.fit_transform(map(lambda x:x[1], y_lr), map(lambda x:x[0], X))</div><div class=\"line\">score = log_loss(y, y_ir)</div></pre></td></tr></table></figure>\n<h5 id=\"itemID离散化LR\"><a href=\"#itemID离散化LR\" class=\"headerlink\" title=\"itemID离散化LR\"></a>itemID离散化LR</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div></pre></td><td class=\"code\"><pre><div class=\"line\">X = [</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">]</div><div class=\"line\">y = [</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0]</div><div class=\"line\">LR = LogisticRegression()</div><div class=\"line\">LR.fit(X, y)</div><div class=\"line\">y_p = LR.predict_proba(X)</div><div class=\"line\">score = log_loss(y, y_p)</div></pre></td></tr></table></figure>\n<h3 id=\"采样的校准\"><a href=\"#采样的校准\" class=\"headerlink\" title=\"采样的校准\"></a>采样的校准</h3><p>由于负样本抽样后，会造成点击率偏高的假象，需要将预测值还原成真实的值。调整的公式如下：</p>\n<h4 id=\"为什么要采样\"><a href=\"#为什么要采样\" class=\"headerlink\" title=\"为什么要采样\"></a>为什么要采样</h4><ol>\n<li>加快训练速度</li>\n<li>某些模型需要正负样本比例不要太不均匀，例如nn要每个batch里有正样本</li>\n</ol>\n<h4 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h4><img src=\"/2018/08/27/ctr-recalibration/图4.4.png\" alt=\"[图4.4.png]\" title=\"[图4.4.png]\">\n<h4 id=\"推导1\"><a href=\"#推导1\" class=\"headerlink\" title=\"推导1\"></a>推导1</h4><ul>\n<li>从结果侧推导<img src=\"/2018/08/27/ctr-recalibration/图3.3.png\" alt=\"[图3.3.png]\" title=\"[图3.3.png]\">\n</li>\n</ul>\n<h4 id=\"推导2\"><a href=\"#推导2\" class=\"headerlink\" title=\"推导2\"></a>推导2</h4><ul>\n<li>证明采样之后只需要加一个bias即可校正</li>\n<li>全量集合N，正样本a，负样本b，采样比例k，扩大比例为1/k</li>\n<li>p = a / (a + b) = 1 / (1+e^-wx)</li>\n<li>=&gt; b/a = e^-wx</li>\n<li>p’ = a / (a + b/k) = 1 / (1+e^-wx-lnk) = 1 / e^-(wx+lnk)</li>\n</ul>\n<h5 id=\"举一个例子\"><a href=\"#举一个例子\" class=\"headerlink\" title=\"举一个例子\"></a>举一个例子</h5><p>负样本采样10%<br>校准回去 q = p / (10 - 9p)<br>p: 模型预估的pCTR<br>q: 真正要输出的ctr</p>\n<p>REF:<br><a href=\"https://tech.meituan.com/mt_dsp.html\" target=\"_blank\" rel=\"external\">https://tech.meituan.com/mt_dsp.html</a><br><a href=\"http://blog.csdn.net/lming_08/article/details/40214921\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/lming_08/article/details/40214921</a></p>\n","site":{"data":{}},"excerpt":"<h3 id=\"CTR为什么会不准？\"><a href=\"#CTR为什么会不准？\" class=\"headerlink\" title=\"CTR为什么会不准？\"></a>CTR为什么会不准？</h3><p>在计算广告中，pCTR往往对比真实的CTR偏高或者偏低的现象，尤其在</p>\n<ol>\n<li>热门曝光广告和冷门曝光广告之间</li>\n<li>高CTR广告和低CTR广告之间</li>\n</ol>\n<p>因此，CTR需要校准。</p>","more":"<h3 id=\"CTR为什么要校准？\"><a href=\"#CTR为什么要校准？\" class=\"headerlink\" title=\"CTR为什么要校准？\"></a>CTR为什么要校准？</h3><p>AUC体现序准；<br>logloss体现值准；<br>要计算商业价值最大化，因此需要值准；<br>因此需要校准；<br>校准之后，logloss会降低。</p>\n<h3 id=\"CTR如何校准\"><a href=\"#CTR如何校准\" class=\"headerlink\" title=\"CTR如何校准\"></a>CTR如何校准</h3><p>CTR校准有很多方法，本质在于“拟合校准前和校准后”，即<br>f(pCTR校准前) = pCTR校准后<br>如何设计函数f，是校准的关键。</p>\n<h4 id=\"binning\"><a href=\"#binning\" class=\"headerlink\" title=\"binning\"></a>binning</h4><p>binning就是样本等频分桶后，每个bin求平均，如下图：<br> <img src=\"/2018/08/27/ctr-recalibration/图1.1.png\" alt=\"[图1.1.png]\" title=\"[图1.1.png]\"></p>\n<h4 id=\"Isotonic-regression-保序回归）\"><a href=\"#Isotonic-regression-保序回归）\" class=\"headerlink\" title=\"Isotonic regression(保序回归）\"></a>Isotonic regression(保序回归）</h4><p>保序回归，就是单调回归（保证按照自变量x和按照因变量y排序序不变，即成正比）<br>为何要保序？<br>为了保证不影响AUC，即默认原始CTR和校准后CTR的正相关性。<br><img src=\"/2018/08/27/ctr-recalibration/图2.2.png\" alt=\"[图2.2.png]\" title=\"[图2.2.png]\"></p>\n<h3 id=\"Best-practice\"><a href=\"#Best-practice\" class=\"headerlink\" title=\"Best practice\"></a>Best practice</h3><h4 id=\"分解动作\"><a href=\"#分解动作\" class=\"headerlink\" title=\"分解动作\"></a>分解动作</h4><ul>\n<li>将统计ctr加入特征中（最好做离散化处理）</li>\n<li>建立f(pCTR)=统计CTR的函数</li>\n<li>进行将f(pCTR)作为新的CTR<h4 id=\"小demo\"><a href=\"#小demo\" class=\"headerlink\" title=\"小demo\"></a>小demo</h4>假设训练数据集合为：<br>物品3：pCTR<em>统计=0.8<br>物品2：pCTR</em>统计=0.5<br>物品1：pCTR_统计=0.3</li>\n</ul>\n<h5 id=\"原始LR\"><a href=\"#原始LR\" class=\"headerlink\" title=\"原始LR\"></a>原始LR</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div></pre></td><td class=\"code\"><pre><div class=\"line\">from sklearn.metrics import log_loss</div><div class=\"line\">from sklearn.linear_model import LogisticRegression</div><div class=\"line\">X = [</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3]</div><div class=\"line\">]</div><div class=\"line\">y = [</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0]</div><div class=\"line\">LR = LogisticRegression()</div><div class=\"line\">LR.fit(X, y)</div><div class=\"line\">y_p = LR.predict_proba(X)</div><div class=\"line\">score = log_loss(y, y_p)</div></pre></td></tr></table></figure>\n<h5 id=\"LR-保序回归\"><a href=\"#LR-保序回归\" class=\"headerlink\" title=\"LR+保序回归\"></a>LR+保序回归</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div></pre></td><td class=\"code\"><pre><div class=\"line\">from sklearn.metrics import log_loss</div><div class=\"line\">from sklearn.linear_model import LogisticRegression</div><div class=\"line\">from sklearn.isotonic import IsotonicRegression</div><div class=\"line\">X = [</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3]</div><div class=\"line\">]</div><div class=\"line\">y = [</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0]</div><div class=\"line\">LR = LogisticRegression()</div><div class=\"line\">LR.fit(X, y)</div><div class=\"line\">y_lr = LR.predict_proba(X)</div><div class=\"line\">ir = IsotonicRegression()</div><div class=\"line\">y_ir = ir.fit_transform(map(lambda x:x[1], y_lr), map(lambda x:x[0], X))</div><div class=\"line\">score = log_loss(y, y_ir)</div></pre></td></tr></table></figure>\n<h5 id=\"itemID离散化LR\"><a href=\"#itemID离散化LR\" class=\"headerlink\" title=\"itemID离散化LR\"></a>itemID离散化LR</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div></pre></td><td class=\"code\"><pre><div class=\"line\">X = [</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">]</div><div class=\"line\">y = [</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0]</div><div class=\"line\">LR = LogisticRegression()</div><div class=\"line\">LR.fit(X, y)</div><div class=\"line\">y_p = LR.predict_proba(X)</div><div class=\"line\">score = log_loss(y, y_p)</div></pre></td></tr></table></figure>\n<h3 id=\"采样的校准\"><a href=\"#采样的校准\" class=\"headerlink\" title=\"采样的校准\"></a>采样的校准</h3><p>由于负样本抽样后，会造成点击率偏高的假象，需要将预测值还原成真实的值。调整的公式如下：</p>\n<h4 id=\"为什么要采样\"><a href=\"#为什么要采样\" class=\"headerlink\" title=\"为什么要采样\"></a>为什么要采样</h4><ol>\n<li>加快训练速度</li>\n<li>某些模型需要正负样本比例不要太不均匀，例如nn要每个batch里有正样本</li>\n</ol>\n<h4 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h4><img src=\"/2018/08/27/ctr-recalibration/图4.4.png\" alt=\"[图4.4.png]\" title=\"[图4.4.png]\">\n<h4 id=\"推导1\"><a href=\"#推导1\" class=\"headerlink\" title=\"推导1\"></a>推导1</h4><ul>\n<li>从结果侧推导<img src=\"/2018/08/27/ctr-recalibration/图3.3.png\" alt=\"[图3.3.png]\" title=\"[图3.3.png]\">\n</li>\n</ul>\n<h4 id=\"推导2\"><a href=\"#推导2\" class=\"headerlink\" title=\"推导2\"></a>推导2</h4><ul>\n<li>证明采样之后只需要加一个bias即可校正</li>\n<li>全量集合N，正样本a，负样本b，采样比例k，扩大比例为1/k</li>\n<li>p = a / (a + b) = 1 / (1+e^-wx)</li>\n<li>=&gt; b/a = e^-wx</li>\n<li>p’ = a / (a + b/k) = 1 / (1+e^-wx-lnk) = 1 / e^-(wx+lnk)</li>\n</ul>\n<h5 id=\"举一个例子\"><a href=\"#举一个例子\" class=\"headerlink\" title=\"举一个例子\"></a>举一个例子</h5><p>负样本采样10%<br>校准回去 q = p / (10 - 9p)<br>p: 模型预估的pCTR<br>q: 真正要输出的ctr</p>\n<p>REF:<br><a href=\"https://tech.meituan.com/mt_dsp.html\" target=\"_blank\" rel=\"external\">https://tech.meituan.com/mt_dsp.html</a><br><a href=\"http://blog.csdn.net/lming_08/article/details/40214921\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/lming_08/article/details/40214921</a></p>"},{"title":"auc_n_logloss","date":"2017-10-06T07:31:48.000Z","_content":"\n### 评估指标\n评估指标大致分为两种，值评估和序评估。\n\n<!-- more -->\n\n| 值评估 | 序评估 |\n|--- |---|\n| MSE/RMSE | AUC/AUPR |\n| R^2 | P@k/MAP/nDCG |\n| logloss | Precision/Recall |\n| MAE | TP/FP/TN/FN/F1 |\n\n### 序准 VS 值准\n* 指标和目的\n    * 序评估目的是为了序准\n    * 值评估目的是为了值准\n* 应用场景\n    * 序准适用于推荐系统，pCTR相对准确，目的是用户价值最大化\n    * 值准适用于商业化系统，pCTR绝对准确，pCTR*cpc，目的是商业价值最大化\n    * 综合公式 score=pCTR^a * cpc^b，进行调权重\n\n### AUC的由来和计算\nauc的一些基础知识，可以参考维基百科的解释:\n\nhttps://en.wikipedia.org/wiki/Receiver_operating_characteristic\n\n这里需要提到一些常见的错误：\n* 错误1：auc是一条光滑曲线\nauc是一条折线，如下图\n {% asset_img \"1.gif\" [1.gif] %}\n\n* 错误2：auc是和预估值有关系的\nauc只和序有关系，和值无关。\n\n* 错误3：求auc需要画出roc曲线\nauc计算部分，除了画出roc曲线，还可以直接计算：\n {% asset_img \"2.png\" [2.png] %}\n其中,\nM为正类样本的数目，N为负类样本的数目\nrank是用的tiedrank\n\n#### AUC的物理意义\n\n和Wilcoxon-Mann-Witney Test有关，即:\nauc=“测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score”，也即auc的物理意义。\n\n#### AUC的计算\n* spark\n```\n// Compute raw scores on the test set\nval predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n  val prediction = model.predict(features)\n  (prediction, label)\n}\n\n// Instantiate metrics object\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)\n\n// AUROC\nval auROC = metrics.areaUnderROC\nprintln(\"Area under ROC = \" + auROC)\n```\n\n* hivemall\n {% asset_img \"3.png\" [3.png] %}\n\n* C语言\nRef: https://github.com/liuzhiqiangruc/dml/blob/master/regr/auc.c\n```\ndouble auc(int n, double *x, double *y) {\n    if (!y || !x) return 0.0;\n    double *rk = (double*) malloc(sizeof(double) * n);\n    AucP *aucp = (AucP *)malloc(sizeof(AucP) * n);\n    int i, tsum;\n    double rksum, auc;\n    for (i = 0; i < n; ++i) {\n        aucp[i].x = x[i];\n        aucp[i].id = i;\n    }\n    tiedrank(n, aucp, rk);\n    for (rksum = 0., tsum = 0, i = 0; i < n; ++i) {\n        if (y[i] >= 1. - 1e-10) {\n            rksum += rk[i];\n            tsum += 1;\n        }\n    }\n    double mn, pst;\n    mn = (double) (n - tsum);\n    mn *= (double) tsum;\n    pst = (double) tsum;\n    pst *= (double) tsum + 1;\n    auc = (rksum - pst / 2.) / mn;\n    free(rk);\n    free(aucp);\n    return auc;\n}\n```\n\n#### AUC的弊端和AUPR\n {% asset_img \"4.png\" [4.png] %}\n {% asset_img \"6.png\" [6.png] %}\n {% asset_img \"7.png\" [7.png] %}\n\n### logloss的由来和计算\n\n#### logloss由来\nlogloss是根据最大似然推导得到的，可参考：\nhttp://www.csuldw.com/2016/03/26/2016-03-26-loss-function/\n\n有些概念需要区分一下\n* loss function: 样本粒度的函数，如logloss, hingeloss等。\n引用一张名图：\n{% asset_img \"9.png\" [9.png] %}\n\n>Plot of various loss functions. Blue is the 0–1 indicator function. Green is the square loss function. Purple is the hinge loss function. Yellow is the logistic loss function. Note that all surrogates give a loss penalty of 1 for yf(x) = 0\n\n* cost function: 集合粒度的函数，即 sum of loss.\n \n#### logloss计算\nlogloss计算需要避免log0的情况，可以参考kaggle中的计算方式：\n```\nmax(min(p,1−10^−15),10^−15)max(min(p,1−10^−15),10^−15).)\n```\nref: https://www.kaggle.com/wiki/LogLoss\n\n### AUC和logloss何时不一致\n在样本不均衡的情况下，AUC和logloss会出现很大的偏差。\n1. logloss低但是AUC也低\n当负样本过多的时候，人为全部预测为负样本，可以实现低logloss，但是AUC=0.5，并不优秀。\n\n2. AUC高但是logloss也高\n负样本过多，当位置pCTR顺序不变，AUC不变，pCTR统一扩大到接近1时候，导致logloss会变得非常的高。\n\n### 定向模式 VS 推荐模式\n1. PUSH系统：广告为中心，为广告找用户，并push；展示可有可无。\n2. 推荐系统：人为中心，为人找推荐项，并展示；用户来了必须展示。\n\n### 线下AUC和线上不一致\n有三种AUC，很多不一致是因为AUC的描述不同造成的\n假设有user-item-pCTR矩阵，那么可以计算\n* 横向AUC：每用户AUC，适用于推荐系统\n* 纵向AUC：每广告AUC，适用于PUSH系统\n* 全局AUC：统一大模型的AUC\n\n存在很多种情况：\n* 纵向AUC高，横向AUC不一定高\n单广告训练做推荐的典型的问题，举一个例子\n\n|  | Item1 | Item2 | Item3 | Item4 |\n| --- | ---| --- | --- | --- |\n| UserA | 0.7(0) | 0.7(1) | 0.7(1) | 0.7(1) |\n| UserB | 0.6(0) | 0.6(0) | 0.6(1) | 0.6(1) |\n| UserC | 0.5(0) | 0.5(0) | 0.5(0) | 0.5(1) |\n\n从纵向来看，每个单Item模型的AUC=1.0，但是横向的AUC=0.5，因此纵向AUC高，并不代表横向AUC高。即：\n从单广告训练的AUC，集合起来，变成真正用户X过来，对用户X进行广告排序，AUC不一定高。\n这种不一致是由于基于Item的模型并没有发现用户的对比其他人“更”偏好什么。\n\n* 横向AUC高，纵向AUC不一定高\n上图翻转，同理。\n\n### AUC@topk VS AUC人数加权\n为了和线上的情况保持一致，最好的方式是：\n* 用户来了必须展示，因此AUC的计算方式是横向AUC，即每个用户计算AUC，然后加权；\n* 用户往往只看头部，因此只计算AUC@topk，衡量头部排序能力。\n\n\n","source":"_posts/auc-n-logloss.md","raw":"---\ntitle: auc_n_logloss\ndate: 2017-10-06 15:31:48\ntags:\ncategories: 机器学习\n---\n\n### 评估指标\n评估指标大致分为两种，值评估和序评估。\n\n<!-- more -->\n\n| 值评估 | 序评估 |\n|--- |---|\n| MSE/RMSE | AUC/AUPR |\n| R^2 | P@k/MAP/nDCG |\n| logloss | Precision/Recall |\n| MAE | TP/FP/TN/FN/F1 |\n\n### 序准 VS 值准\n* 指标和目的\n    * 序评估目的是为了序准\n    * 值评估目的是为了值准\n* 应用场景\n    * 序准适用于推荐系统，pCTR相对准确，目的是用户价值最大化\n    * 值准适用于商业化系统，pCTR绝对准确，pCTR*cpc，目的是商业价值最大化\n    * 综合公式 score=pCTR^a * cpc^b，进行调权重\n\n### AUC的由来和计算\nauc的一些基础知识，可以参考维基百科的解释:\n\nhttps://en.wikipedia.org/wiki/Receiver_operating_characteristic\n\n这里需要提到一些常见的错误：\n* 错误1：auc是一条光滑曲线\nauc是一条折线，如下图\n {% asset_img \"1.gif\" [1.gif] %}\n\n* 错误2：auc是和预估值有关系的\nauc只和序有关系，和值无关。\n\n* 错误3：求auc需要画出roc曲线\nauc计算部分，除了画出roc曲线，还可以直接计算：\n {% asset_img \"2.png\" [2.png] %}\n其中,\nM为正类样本的数目，N为负类样本的数目\nrank是用的tiedrank\n\n#### AUC的物理意义\n\n和Wilcoxon-Mann-Witney Test有关，即:\nauc=“测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score”，也即auc的物理意义。\n\n#### AUC的计算\n* spark\n```\n// Compute raw scores on the test set\nval predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n  val prediction = model.predict(features)\n  (prediction, label)\n}\n\n// Instantiate metrics object\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)\n\n// AUROC\nval auROC = metrics.areaUnderROC\nprintln(\"Area under ROC = \" + auROC)\n```\n\n* hivemall\n {% asset_img \"3.png\" [3.png] %}\n\n* C语言\nRef: https://github.com/liuzhiqiangruc/dml/blob/master/regr/auc.c\n```\ndouble auc(int n, double *x, double *y) {\n    if (!y || !x) return 0.0;\n    double *rk = (double*) malloc(sizeof(double) * n);\n    AucP *aucp = (AucP *)malloc(sizeof(AucP) * n);\n    int i, tsum;\n    double rksum, auc;\n    for (i = 0; i < n; ++i) {\n        aucp[i].x = x[i];\n        aucp[i].id = i;\n    }\n    tiedrank(n, aucp, rk);\n    for (rksum = 0., tsum = 0, i = 0; i < n; ++i) {\n        if (y[i] >= 1. - 1e-10) {\n            rksum += rk[i];\n            tsum += 1;\n        }\n    }\n    double mn, pst;\n    mn = (double) (n - tsum);\n    mn *= (double) tsum;\n    pst = (double) tsum;\n    pst *= (double) tsum + 1;\n    auc = (rksum - pst / 2.) / mn;\n    free(rk);\n    free(aucp);\n    return auc;\n}\n```\n\n#### AUC的弊端和AUPR\n {% asset_img \"4.png\" [4.png] %}\n {% asset_img \"6.png\" [6.png] %}\n {% asset_img \"7.png\" [7.png] %}\n\n### logloss的由来和计算\n\n#### logloss由来\nlogloss是根据最大似然推导得到的，可参考：\nhttp://www.csuldw.com/2016/03/26/2016-03-26-loss-function/\n\n有些概念需要区分一下\n* loss function: 样本粒度的函数，如logloss, hingeloss等。\n引用一张名图：\n{% asset_img \"9.png\" [9.png] %}\n\n>Plot of various loss functions. Blue is the 0–1 indicator function. Green is the square loss function. Purple is the hinge loss function. Yellow is the logistic loss function. Note that all surrogates give a loss penalty of 1 for yf(x) = 0\n\n* cost function: 集合粒度的函数，即 sum of loss.\n \n#### logloss计算\nlogloss计算需要避免log0的情况，可以参考kaggle中的计算方式：\n```\nmax(min(p,1−10^−15),10^−15)max(min(p,1−10^−15),10^−15).)\n```\nref: https://www.kaggle.com/wiki/LogLoss\n\n### AUC和logloss何时不一致\n在样本不均衡的情况下，AUC和logloss会出现很大的偏差。\n1. logloss低但是AUC也低\n当负样本过多的时候，人为全部预测为负样本，可以实现低logloss，但是AUC=0.5，并不优秀。\n\n2. AUC高但是logloss也高\n负样本过多，当位置pCTR顺序不变，AUC不变，pCTR统一扩大到接近1时候，导致logloss会变得非常的高。\n\n### 定向模式 VS 推荐模式\n1. PUSH系统：广告为中心，为广告找用户，并push；展示可有可无。\n2. 推荐系统：人为中心，为人找推荐项，并展示；用户来了必须展示。\n\n### 线下AUC和线上不一致\n有三种AUC，很多不一致是因为AUC的描述不同造成的\n假设有user-item-pCTR矩阵，那么可以计算\n* 横向AUC：每用户AUC，适用于推荐系统\n* 纵向AUC：每广告AUC，适用于PUSH系统\n* 全局AUC：统一大模型的AUC\n\n存在很多种情况：\n* 纵向AUC高，横向AUC不一定高\n单广告训练做推荐的典型的问题，举一个例子\n\n|  | Item1 | Item2 | Item3 | Item4 |\n| --- | ---| --- | --- | --- |\n| UserA | 0.7(0) | 0.7(1) | 0.7(1) | 0.7(1) |\n| UserB | 0.6(0) | 0.6(0) | 0.6(1) | 0.6(1) |\n| UserC | 0.5(0) | 0.5(0) | 0.5(0) | 0.5(1) |\n\n从纵向来看，每个单Item模型的AUC=1.0，但是横向的AUC=0.5，因此纵向AUC高，并不代表横向AUC高。即：\n从单广告训练的AUC，集合起来，变成真正用户X过来，对用户X进行广告排序，AUC不一定高。\n这种不一致是由于基于Item的模型并没有发现用户的对比其他人“更”偏好什么。\n\n* 横向AUC高，纵向AUC不一定高\n上图翻转，同理。\n\n### AUC@topk VS AUC人数加权\n为了和线上的情况保持一致，最好的方式是：\n* 用户来了必须展示，因此AUC的计算方式是横向AUC，即每个用户计算AUC，然后加权；\n* 用户往往只看头部，因此只计算AUC@topk，衡量头部排序能力。\n\n\n","slug":"auc-n-logloss","published":1,"updated":"2017-10-09T14:26:28.361Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8g900005tcins45dq19i","content":"<h3 id=\"评估指标\"><a href=\"#评估指标\" class=\"headerlink\" title=\"评估指标\"></a>评估指标</h3><p>评估指标大致分为两种，值评估和序评估。</p>\n<a id=\"more\"></a>\n<table>\n<thead>\n<tr>\n<th>值评估</th>\n<th>序评估</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MSE/RMSE</td>\n<td>AUC/AUPR</td>\n</tr>\n<tr>\n<td>R^2</td>\n<td>P@k/MAP/nDCG</td>\n</tr>\n<tr>\n<td>logloss</td>\n<td>Precision/Recall</td>\n</tr>\n<tr>\n<td>MAE</td>\n<td>TP/FP/TN/FN/F1</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"序准-VS-值准\"><a href=\"#序准-VS-值准\" class=\"headerlink\" title=\"序准 VS 值准\"></a>序准 VS 值准</h3><ul>\n<li>指标和目的<ul>\n<li>序评估目的是为了序准</li>\n<li>值评估目的是为了值准</li>\n</ul>\n</li>\n<li>应用场景<ul>\n<li>序准适用于推荐系统，pCTR相对准确，目的是用户价值最大化</li>\n<li>值准适用于商业化系统，pCTR绝对准确，pCTR*cpc，目的是商业价值最大化</li>\n<li>综合公式 score=pCTR^a * cpc^b，进行调权重</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"AUC的由来和计算\"><a href=\"#AUC的由来和计算\" class=\"headerlink\" title=\"AUC的由来和计算\"></a>AUC的由来和计算</h3><p>auc的一些基础知识，可以参考维基百科的解释:</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Receiver_operating_characteristic</a></p>\n<p>这里需要提到一些常见的错误：</p>\n<ul>\n<li><p>错误1：auc是一条光滑曲线<br>auc是一条折线，如下图</p>\n<img src=\"/2017/10/06/auc-n-logloss/1.gif\" alt=\"[1.gif]\" title=\"[1.gif]\">\n</li>\n<li><p>错误2：auc是和预估值有关系的<br>auc只和序有关系，和值无关。</p>\n</li>\n<li><p>错误3：求auc需要画出roc曲线<br>auc计算部分，除了画出roc曲线，还可以直接计算：</p>\n<img src=\"/2017/10/06/auc-n-logloss/2.png\" alt=\"[2.png]\" title=\"[2.png]\">\n<p>其中,<br>M为正类样本的数目，N为负类样本的数目<br>rank是用的tiedrank</p>\n</li>\n</ul>\n<h4 id=\"AUC的物理意义\"><a href=\"#AUC的物理意义\" class=\"headerlink\" title=\"AUC的物理意义\"></a>AUC的物理意义</h4><p>和Wilcoxon-Mann-Witney Test有关，即:<br>auc=“测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score”，也即auc的物理意义。</p>\n<h4 id=\"AUC的计算\"><a href=\"#AUC的计算\" class=\"headerlink\" title=\"AUC的计算\"></a>AUC的计算</h4><ul>\n<li><p>spark</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Compute raw scores on the test set</div><div class=\"line\">val predictionAndLabels = test.map &#123; case LabeledPoint(label, features) =&gt;</div><div class=\"line\">  val prediction = model.predict(features)</div><div class=\"line\">  (prediction, label)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">// Instantiate metrics object</div><div class=\"line\">val metrics = new BinaryClassificationMetrics(predictionAndLabels)</div><div class=\"line\"></div><div class=\"line\">// AUROC</div><div class=\"line\">val auROC = metrics.areaUnderROC</div><div class=\"line\">println(&quot;Area under ROC = &quot; + auROC)</div></pre></td></tr></table></figure>\n</li>\n<li><p>hivemall</p>\n<img src=\"/2017/10/06/auc-n-logloss/3.png\" alt=\"[3.png]\" title=\"[3.png]\">\n</li>\n<li><p>C语言<br>Ref: <a href=\"https://github.com/liuzhiqiangruc/dml/blob/master/regr/auc.c\" target=\"_blank\" rel=\"external\">https://github.com/liuzhiqiangruc/dml/blob/master/regr/auc.c</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">double auc(int n, double *x, double *y) &#123;</div><div class=\"line\">    if (!y || !x) return 0.0;</div><div class=\"line\">    double *rk = (double*) malloc(sizeof(double) * n);</div><div class=\"line\">    AucP *aucp = (AucP *)malloc(sizeof(AucP) * n);</div><div class=\"line\">    int i, tsum;</div><div class=\"line\">    double rksum, auc;</div><div class=\"line\">    for (i = 0; i &lt; n; ++i) &#123;</div><div class=\"line\">        aucp[i].x = x[i];</div><div class=\"line\">        aucp[i].id = i;</div><div class=\"line\">    &#125;</div><div class=\"line\">    tiedrank(n, aucp, rk);</div><div class=\"line\">    for (rksum = 0., tsum = 0, i = 0; i &lt; n; ++i) &#123;</div><div class=\"line\">        if (y[i] &gt;= 1. - 1e-10) &#123;</div><div class=\"line\">            rksum += rk[i];</div><div class=\"line\">            tsum += 1;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    double mn, pst;</div><div class=\"line\">    mn = (double) (n - tsum);</div><div class=\"line\">    mn *= (double) tsum;</div><div class=\"line\">    pst = (double) tsum;</div><div class=\"line\">    pst *= (double) tsum + 1;</div><div class=\"line\">    auc = (rksum - pst / 2.) / mn;</div><div class=\"line\">    free(rk);</div><div class=\"line\">    free(aucp);</div><div class=\"line\">    return auc;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h4 id=\"AUC的弊端和AUPR\"><a href=\"#AUC的弊端和AUPR\" class=\"headerlink\" title=\"AUC的弊端和AUPR\"></a>AUC的弊端和AUPR</h4> <img src=\"/2017/10/06/auc-n-logloss/4.png\" alt=\"[4.png]\" title=\"[4.png]\">\n <img src=\"/2017/10/06/auc-n-logloss/6.png\" alt=\"[6.png]\" title=\"[6.png]\">\n <img src=\"/2017/10/06/auc-n-logloss/7.png\" alt=\"[7.png]\" title=\"[7.png]\">\n<h3 id=\"logloss的由来和计算\"><a href=\"#logloss的由来和计算\" class=\"headerlink\" title=\"logloss的由来和计算\"></a>logloss的由来和计算</h3><h4 id=\"logloss由来\"><a href=\"#logloss由来\" class=\"headerlink\" title=\"logloss由来\"></a>logloss由来</h4><p>logloss是根据最大似然推导得到的，可参考：<br><a href=\"http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/\" target=\"_blank\" rel=\"external\">http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/</a></p>\n<p>有些概念需要区分一下</p>\n<ul>\n<li>loss function: 样本粒度的函数，如logloss, hingeloss等。<br>引用一张名图：<img src=\"/2017/10/06/auc-n-logloss/9.png\" alt=\"[9.png]\" title=\"[9.png]\">\n</li>\n</ul>\n<blockquote>\n<p>Plot of various loss functions. Blue is the 0–1 indicator function. Green is the square loss function. Purple is the hinge loss function. Yellow is the logistic loss function. Note that all surrogates give a loss penalty of 1 for yf(x) = 0</p>\n</blockquote>\n<ul>\n<li>cost function: 集合粒度的函数，即 sum of loss.</li>\n</ul>\n<h4 id=\"logloss计算\"><a href=\"#logloss计算\" class=\"headerlink\" title=\"logloss计算\"></a>logloss计算</h4><p>logloss计算需要避免log0的情况，可以参考kaggle中的计算方式：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">max(min(p,1−10^−15),10^−15)max(min(p,1−10^−15),10^−15).)</div></pre></td></tr></table></figure></p>\n<p>ref: <a href=\"https://www.kaggle.com/wiki/LogLoss\" target=\"_blank\" rel=\"external\">https://www.kaggle.com/wiki/LogLoss</a></p>\n<h3 id=\"AUC和logloss何时不一致\"><a href=\"#AUC和logloss何时不一致\" class=\"headerlink\" title=\"AUC和logloss何时不一致\"></a>AUC和logloss何时不一致</h3><p>在样本不均衡的情况下，AUC和logloss会出现很大的偏差。</p>\n<ol>\n<li><p>logloss低但是AUC也低<br>当负样本过多的时候，人为全部预测为负样本，可以实现低logloss，但是AUC=0.5，并不优秀。</p>\n</li>\n<li><p>AUC高但是logloss也高<br>负样本过多，当位置pCTR顺序不变，AUC不变，pCTR统一扩大到接近1时候，导致logloss会变得非常的高。</p>\n</li>\n</ol>\n<h3 id=\"定向模式-VS-推荐模式\"><a href=\"#定向模式-VS-推荐模式\" class=\"headerlink\" title=\"定向模式 VS 推荐模式\"></a>定向模式 VS 推荐模式</h3><ol>\n<li>PUSH系统：广告为中心，为广告找用户，并push；展示可有可无。</li>\n<li>推荐系统：人为中心，为人找推荐项，并展示；用户来了必须展示。</li>\n</ol>\n<h3 id=\"线下AUC和线上不一致\"><a href=\"#线下AUC和线上不一致\" class=\"headerlink\" title=\"线下AUC和线上不一致\"></a>线下AUC和线上不一致</h3><p>有三种AUC，很多不一致是因为AUC的描述不同造成的<br>假设有user-item-pCTR矩阵，那么可以计算</p>\n<ul>\n<li>横向AUC：每用户AUC，适用于推荐系统</li>\n<li>纵向AUC：每广告AUC，适用于PUSH系统</li>\n<li>全局AUC：统一大模型的AUC</li>\n</ul>\n<p>存在很多种情况：</p>\n<ul>\n<li>纵向AUC高，横向AUC不一定高<br>单广告训练做推荐的典型的问题，举一个例子</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Item1</th>\n<th>Item2</th>\n<th>Item3</th>\n<th>Item4</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>UserA</td>\n<td>0.7(0)</td>\n<td>0.7(1)</td>\n<td>0.7(1)</td>\n<td>0.7(1)</td>\n</tr>\n<tr>\n<td>UserB</td>\n<td>0.6(0)</td>\n<td>0.6(0)</td>\n<td>0.6(1)</td>\n<td>0.6(1)</td>\n</tr>\n<tr>\n<td>UserC</td>\n<td>0.5(0)</td>\n<td>0.5(0)</td>\n<td>0.5(0)</td>\n<td>0.5(1)</td>\n</tr>\n</tbody>\n</table>\n<p>从纵向来看，每个单Item模型的AUC=1.0，但是横向的AUC=0.5，因此纵向AUC高，并不代表横向AUC高。即：<br>从单广告训练的AUC，集合起来，变成真正用户X过来，对用户X进行广告排序，AUC不一定高。<br>这种不一致是由于基于Item的模型并没有发现用户的对比其他人“更”偏好什么。</p>\n<ul>\n<li>横向AUC高，纵向AUC不一定高<br>上图翻转，同理。</li>\n</ul>\n<h3 id=\"AUC-topk-VS-AUC人数加权\"><a href=\"#AUC-topk-VS-AUC人数加权\" class=\"headerlink\" title=\"AUC@topk VS AUC人数加权\"></a>AUC@topk VS AUC人数加权</h3><p>为了和线上的情况保持一致，最好的方式是：</p>\n<ul>\n<li>用户来了必须展示，因此AUC的计算方式是横向AUC，即每个用户计算AUC，然后加权；</li>\n<li>用户往往只看头部，因此只计算AUC@topk，衡量头部排序能力。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h3 id=\"评估指标\"><a href=\"#评估指标\" class=\"headerlink\" title=\"评估指标\"></a>评估指标</h3><p>评估指标大致分为两种，值评估和序评估。</p>","more":"<table>\n<thead>\n<tr>\n<th>值评估</th>\n<th>序评估</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MSE/RMSE</td>\n<td>AUC/AUPR</td>\n</tr>\n<tr>\n<td>R^2</td>\n<td>P@k/MAP/nDCG</td>\n</tr>\n<tr>\n<td>logloss</td>\n<td>Precision/Recall</td>\n</tr>\n<tr>\n<td>MAE</td>\n<td>TP/FP/TN/FN/F1</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"序准-VS-值准\"><a href=\"#序准-VS-值准\" class=\"headerlink\" title=\"序准 VS 值准\"></a>序准 VS 值准</h3><ul>\n<li>指标和目的<ul>\n<li>序评估目的是为了序准</li>\n<li>值评估目的是为了值准</li>\n</ul>\n</li>\n<li>应用场景<ul>\n<li>序准适用于推荐系统，pCTR相对准确，目的是用户价值最大化</li>\n<li>值准适用于商业化系统，pCTR绝对准确，pCTR*cpc，目的是商业价值最大化</li>\n<li>综合公式 score=pCTR^a * cpc^b，进行调权重</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"AUC的由来和计算\"><a href=\"#AUC的由来和计算\" class=\"headerlink\" title=\"AUC的由来和计算\"></a>AUC的由来和计算</h3><p>auc的一些基础知识，可以参考维基百科的解释:</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Receiver_operating_characteristic</a></p>\n<p>这里需要提到一些常见的错误：</p>\n<ul>\n<li><p>错误1：auc是一条光滑曲线<br>auc是一条折线，如下图</p>\n<img src=\"/2017/10/06/auc-n-logloss/1.gif\" alt=\"[1.gif]\" title=\"[1.gif]\">\n</li>\n<li><p>错误2：auc是和预估值有关系的<br>auc只和序有关系，和值无关。</p>\n</li>\n<li><p>错误3：求auc需要画出roc曲线<br>auc计算部分，除了画出roc曲线，还可以直接计算：</p>\n<img src=\"/2017/10/06/auc-n-logloss/2.png\" alt=\"[2.png]\" title=\"[2.png]\">\n<p>其中,<br>M为正类样本的数目，N为负类样本的数目<br>rank是用的tiedrank</p>\n</li>\n</ul>\n<h4 id=\"AUC的物理意义\"><a href=\"#AUC的物理意义\" class=\"headerlink\" title=\"AUC的物理意义\"></a>AUC的物理意义</h4><p>和Wilcoxon-Mann-Witney Test有关，即:<br>auc=“测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score”，也即auc的物理意义。</p>\n<h4 id=\"AUC的计算\"><a href=\"#AUC的计算\" class=\"headerlink\" title=\"AUC的计算\"></a>AUC的计算</h4><ul>\n<li><p>spark</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Compute raw scores on the test set</div><div class=\"line\">val predictionAndLabels = test.map &#123; case LabeledPoint(label, features) =&gt;</div><div class=\"line\">  val prediction = model.predict(features)</div><div class=\"line\">  (prediction, label)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">// Instantiate metrics object</div><div class=\"line\">val metrics = new BinaryClassificationMetrics(predictionAndLabels)</div><div class=\"line\"></div><div class=\"line\">// AUROC</div><div class=\"line\">val auROC = metrics.areaUnderROC</div><div class=\"line\">println(&quot;Area under ROC = &quot; + auROC)</div></pre></td></tr></table></figure>\n</li>\n<li><p>hivemall</p>\n<img src=\"/2017/10/06/auc-n-logloss/3.png\" alt=\"[3.png]\" title=\"[3.png]\">\n</li>\n<li><p>C语言<br>Ref: <a href=\"https://github.com/liuzhiqiangruc/dml/blob/master/regr/auc.c\" target=\"_blank\" rel=\"external\">https://github.com/liuzhiqiangruc/dml/blob/master/regr/auc.c</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">double auc(int n, double *x, double *y) &#123;</div><div class=\"line\">    if (!y || !x) return 0.0;</div><div class=\"line\">    double *rk = (double*) malloc(sizeof(double) * n);</div><div class=\"line\">    AucP *aucp = (AucP *)malloc(sizeof(AucP) * n);</div><div class=\"line\">    int i, tsum;</div><div class=\"line\">    double rksum, auc;</div><div class=\"line\">    for (i = 0; i &lt; n; ++i) &#123;</div><div class=\"line\">        aucp[i].x = x[i];</div><div class=\"line\">        aucp[i].id = i;</div><div class=\"line\">    &#125;</div><div class=\"line\">    tiedrank(n, aucp, rk);</div><div class=\"line\">    for (rksum = 0., tsum = 0, i = 0; i &lt; n; ++i) &#123;</div><div class=\"line\">        if (y[i] &gt;= 1. - 1e-10) &#123;</div><div class=\"line\">            rksum += rk[i];</div><div class=\"line\">            tsum += 1;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    double mn, pst;</div><div class=\"line\">    mn = (double) (n - tsum);</div><div class=\"line\">    mn *= (double) tsum;</div><div class=\"line\">    pst = (double) tsum;</div><div class=\"line\">    pst *= (double) tsum + 1;</div><div class=\"line\">    auc = (rksum - pst / 2.) / mn;</div><div class=\"line\">    free(rk);</div><div class=\"line\">    free(aucp);</div><div class=\"line\">    return auc;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h4 id=\"AUC的弊端和AUPR\"><a href=\"#AUC的弊端和AUPR\" class=\"headerlink\" title=\"AUC的弊端和AUPR\"></a>AUC的弊端和AUPR</h4> <img src=\"/2017/10/06/auc-n-logloss/4.png\" alt=\"[4.png]\" title=\"[4.png]\">\n <img src=\"/2017/10/06/auc-n-logloss/6.png\" alt=\"[6.png]\" title=\"[6.png]\">\n <img src=\"/2017/10/06/auc-n-logloss/7.png\" alt=\"[7.png]\" title=\"[7.png]\">\n<h3 id=\"logloss的由来和计算\"><a href=\"#logloss的由来和计算\" class=\"headerlink\" title=\"logloss的由来和计算\"></a>logloss的由来和计算</h3><h4 id=\"logloss由来\"><a href=\"#logloss由来\" class=\"headerlink\" title=\"logloss由来\"></a>logloss由来</h4><p>logloss是根据最大似然推导得到的，可参考：<br><a href=\"http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/\" target=\"_blank\" rel=\"external\">http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/</a></p>\n<p>有些概念需要区分一下</p>\n<ul>\n<li>loss function: 样本粒度的函数，如logloss, hingeloss等。<br>引用一张名图：<img src=\"/2017/10/06/auc-n-logloss/9.png\" alt=\"[9.png]\" title=\"[9.png]\">\n</li>\n</ul>\n<blockquote>\n<p>Plot of various loss functions. Blue is the 0–1 indicator function. Green is the square loss function. Purple is the hinge loss function. Yellow is the logistic loss function. Note that all surrogates give a loss penalty of 1 for yf(x) = 0</p>\n</blockquote>\n<ul>\n<li>cost function: 集合粒度的函数，即 sum of loss.</li>\n</ul>\n<h4 id=\"logloss计算\"><a href=\"#logloss计算\" class=\"headerlink\" title=\"logloss计算\"></a>logloss计算</h4><p>logloss计算需要避免log0的情况，可以参考kaggle中的计算方式：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">max(min(p,1−10^−15),10^−15)max(min(p,1−10^−15),10^−15).)</div></pre></td></tr></table></figure></p>\n<p>ref: <a href=\"https://www.kaggle.com/wiki/LogLoss\" target=\"_blank\" rel=\"external\">https://www.kaggle.com/wiki/LogLoss</a></p>\n<h3 id=\"AUC和logloss何时不一致\"><a href=\"#AUC和logloss何时不一致\" class=\"headerlink\" title=\"AUC和logloss何时不一致\"></a>AUC和logloss何时不一致</h3><p>在样本不均衡的情况下，AUC和logloss会出现很大的偏差。</p>\n<ol>\n<li><p>logloss低但是AUC也低<br>当负样本过多的时候，人为全部预测为负样本，可以实现低logloss，但是AUC=0.5，并不优秀。</p>\n</li>\n<li><p>AUC高但是logloss也高<br>负样本过多，当位置pCTR顺序不变，AUC不变，pCTR统一扩大到接近1时候，导致logloss会变得非常的高。</p>\n</li>\n</ol>\n<h3 id=\"定向模式-VS-推荐模式\"><a href=\"#定向模式-VS-推荐模式\" class=\"headerlink\" title=\"定向模式 VS 推荐模式\"></a>定向模式 VS 推荐模式</h3><ol>\n<li>PUSH系统：广告为中心，为广告找用户，并push；展示可有可无。</li>\n<li>推荐系统：人为中心，为人找推荐项，并展示；用户来了必须展示。</li>\n</ol>\n<h3 id=\"线下AUC和线上不一致\"><a href=\"#线下AUC和线上不一致\" class=\"headerlink\" title=\"线下AUC和线上不一致\"></a>线下AUC和线上不一致</h3><p>有三种AUC，很多不一致是因为AUC的描述不同造成的<br>假设有user-item-pCTR矩阵，那么可以计算</p>\n<ul>\n<li>横向AUC：每用户AUC，适用于推荐系统</li>\n<li>纵向AUC：每广告AUC，适用于PUSH系统</li>\n<li>全局AUC：统一大模型的AUC</li>\n</ul>\n<p>存在很多种情况：</p>\n<ul>\n<li>纵向AUC高，横向AUC不一定高<br>单广告训练做推荐的典型的问题，举一个例子</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Item1</th>\n<th>Item2</th>\n<th>Item3</th>\n<th>Item4</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>UserA</td>\n<td>0.7(0)</td>\n<td>0.7(1)</td>\n<td>0.7(1)</td>\n<td>0.7(1)</td>\n</tr>\n<tr>\n<td>UserB</td>\n<td>0.6(0)</td>\n<td>0.6(0)</td>\n<td>0.6(1)</td>\n<td>0.6(1)</td>\n</tr>\n<tr>\n<td>UserC</td>\n<td>0.5(0)</td>\n<td>0.5(0)</td>\n<td>0.5(0)</td>\n<td>0.5(1)</td>\n</tr>\n</tbody>\n</table>\n<p>从纵向来看，每个单Item模型的AUC=1.0，但是横向的AUC=0.5，因此纵向AUC高，并不代表横向AUC高。即：<br>从单广告训练的AUC，集合起来，变成真正用户X过来，对用户X进行广告排序，AUC不一定高。<br>这种不一致是由于基于Item的模型并没有发现用户的对比其他人“更”偏好什么。</p>\n<ul>\n<li>横向AUC高，纵向AUC不一定高<br>上图翻转，同理。</li>\n</ul>\n<h3 id=\"AUC-topk-VS-AUC人数加权\"><a href=\"#AUC-topk-VS-AUC人数加权\" class=\"headerlink\" title=\"AUC@topk VS AUC人数加权\"></a>AUC@topk VS AUC人数加权</h3><p>为了和线上的情况保持一致，最好的方式是：</p>\n<ul>\n<li>用户来了必须展示，因此AUC的计算方式是横向AUC，即每个用户计算AUC，然后加权；</li>\n<li>用户往往只看头部，因此只计算AUC@topk，衡量头部排序能力。</li>\n</ul>"},{"title":"bayes","date":"2018-01-16T03:26:02.000Z","_content":"\n### Discriminative VS Generative\n* Discriminative models 判别模型\n直接建模P(c|x)\n* Generative models 生成模型\nP(c|x) = P(x, c) / P(x) = P(c)*P(x|c)/P(x)\n其中，\n\t* P(c)可以通过统计各类样本比例频率来估计 --频率学派\n\t* P(x|c)因为样本x的数据量太小，很难估计准确\n\t\n### 频率学派 VS 贝叶斯学派\n* Frequentist 频率学派 \n参数是一个未知但客观存在的固定值\n* Bayesian 贝叶斯学派 \n参数本身是一个分布 \n\n### Naive Bayes \n* 属性条件独立性假设（假设每个属性独立地对分类结果发生影响）\n* Smoothing: 拉普拉斯修正（Laplacian Correction）\n* Lazy Learning\n\n### semi-naive Bayes classifier\n","source":"_posts/bayes.md","raw":"---\ntitle: bayes\ndate: 2018-01-16 11:26:02\ntags:\n---\n\n### Discriminative VS Generative\n* Discriminative models 判别模型\n直接建模P(c|x)\n* Generative models 生成模型\nP(c|x) = P(x, c) / P(x) = P(c)*P(x|c)/P(x)\n其中，\n\t* P(c)可以通过统计各类样本比例频率来估计 --频率学派\n\t* P(x|c)因为样本x的数据量太小，很难估计准确\n\t\n### 频率学派 VS 贝叶斯学派\n* Frequentist 频率学派 \n参数是一个未知但客观存在的固定值\n* Bayesian 贝叶斯学派 \n参数本身是一个分布 \n\n### Naive Bayes \n* 属性条件独立性假设（假设每个属性独立地对分类结果发生影响）\n* Smoothing: 拉普拉斯修正（Laplacian Correction）\n* Lazy Learning\n\n### semi-naive Bayes classifier\n","slug":"bayes","published":1,"updated":"2018-03-11T07:07:07.168Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8g9k0007tcin3u4lm8yy","content":"<h3 id=\"Discriminative-VS-Generative\"><a href=\"#Discriminative-VS-Generative\" class=\"headerlink\" title=\"Discriminative VS Generative\"></a>Discriminative VS Generative</h3><ul>\n<li>Discriminative models 判别模型<br>直接建模P(c|x)</li>\n<li>Generative models 生成模型<br>P(c|x) = P(x, c) / P(x) = P(c)*P(x|c)/P(x)<br>其中，<ul>\n<li>P(c)可以通过统计各类样本比例频率来估计 –频率学派</li>\n<li>P(x|c)因为样本x的数据量太小，很难估计准确</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"频率学派-VS-贝叶斯学派\"><a href=\"#频率学派-VS-贝叶斯学派\" class=\"headerlink\" title=\"频率学派 VS 贝叶斯学派\"></a>频率学派 VS 贝叶斯学派</h3><ul>\n<li>Frequentist 频率学派<br>参数是一个未知但客观存在的固定值</li>\n<li>Bayesian 贝叶斯学派<br>参数本身是一个分布 </li>\n</ul>\n<h3 id=\"Naive-Bayes\"><a href=\"#Naive-Bayes\" class=\"headerlink\" title=\"Naive Bayes\"></a>Naive Bayes</h3><ul>\n<li>属性条件独立性假设（假设每个属性独立地对分类结果发生影响）</li>\n<li>Smoothing: 拉普拉斯修正（Laplacian Correction）</li>\n<li>Lazy Learning</li>\n</ul>\n<h3 id=\"semi-naive-Bayes-classifier\"><a href=\"#semi-naive-Bayes-classifier\" class=\"headerlink\" title=\"semi-naive Bayes classifier\"></a>semi-naive Bayes classifier</h3>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Discriminative-VS-Generative\"><a href=\"#Discriminative-VS-Generative\" class=\"headerlink\" title=\"Discriminative VS Generative\"></a>Discriminative VS Generative</h3><ul>\n<li>Discriminative models 判别模型<br>直接建模P(c|x)</li>\n<li>Generative models 生成模型<br>P(c|x) = P(x, c) / P(x) = P(c)*P(x|c)/P(x)<br>其中，<ul>\n<li>P(c)可以通过统计各类样本比例频率来估计 –频率学派</li>\n<li>P(x|c)因为样本x的数据量太小，很难估计准确</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"频率学派-VS-贝叶斯学派\"><a href=\"#频率学派-VS-贝叶斯学派\" class=\"headerlink\" title=\"频率学派 VS 贝叶斯学派\"></a>频率学派 VS 贝叶斯学派</h3><ul>\n<li>Frequentist 频率学派<br>参数是一个未知但客观存在的固定值</li>\n<li>Bayesian 贝叶斯学派<br>参数本身是一个分布 </li>\n</ul>\n<h3 id=\"Naive-Bayes\"><a href=\"#Naive-Bayes\" class=\"headerlink\" title=\"Naive Bayes\"></a>Naive Bayes</h3><ul>\n<li>属性条件独立性假设（假设每个属性独立地对分类结果发生影响）</li>\n<li>Smoothing: 拉普拉斯修正（Laplacian Correction）</li>\n<li>Lazy Learning</li>\n</ul>\n<h3 id=\"semi-naive-Bayes-classifier\"><a href=\"#semi-naive-Bayes-classifier\" class=\"headerlink\" title=\"semi-naive Bayes classifier\"></a>semi-naive Bayes classifier</h3>"},{"title":"MachineLearning_ZhouZhihua","date":"2017-11-27T03:10:36.000Z","_content":"\n### chapter 1\n\n* No Free Lunch Theorem(NFL)\n\n算法应用于问题要具体问题具体分析\n\n### chapture 2: 模型评估和选择\n\n#### Basic Concepts\n* error\n\t* empirical error/training error 经验误差（训练误差）\n\t* generalization error 泛化误差（预测集误差）\n* fitting\n\t* overfitting\n\t* underfitting\n* set\n\t* train / dev / test set\n* cross validation\n\t* k-fold\n* bootstrapping\n\t* 适用小数据集\n\t* 会改变数据分布\n* parameter tuning\n* performance measure\n\t* MSE: mean squared error \n\t* error rate\n\t* accuracy rate\n\t* Precision/Recall/F1/ROC/AUC\n\t\n#### 常见错误\n* 训练集测试集分布不一致(采样？Group By?)\n* 数据穿越\n\n#### 代码敏感错误率与代价曲线\n* 代价曲线与期望总体代价\n* 假设检验\n\n#### 多分类\n* Linear Regression/Logistic Regression/LDA(Linear Discriminant Analysis)\n* OVO, OVR\n\n#### DT\n* entropy/IG/Gini\n* 预减枝/后减枝\n\n#### ANN\n* Perceptron\n\n","source":"_posts/MachineLearning-ZhouZhihua.md","raw":"---\ntitle: MachineLearning_ZhouZhihua\ndate: 2017-11-27 11:10:36\ntags:\n---\n\n### chapter 1\n\n* No Free Lunch Theorem(NFL)\n\n算法应用于问题要具体问题具体分析\n\n### chapture 2: 模型评估和选择\n\n#### Basic Concepts\n* error\n\t* empirical error/training error 经验误差（训练误差）\n\t* generalization error 泛化误差（预测集误差）\n* fitting\n\t* overfitting\n\t* underfitting\n* set\n\t* train / dev / test set\n* cross validation\n\t* k-fold\n* bootstrapping\n\t* 适用小数据集\n\t* 会改变数据分布\n* parameter tuning\n* performance measure\n\t* MSE: mean squared error \n\t* error rate\n\t* accuracy rate\n\t* Precision/Recall/F1/ROC/AUC\n\t\n#### 常见错误\n* 训练集测试集分布不一致(采样？Group By?)\n* 数据穿越\n\n#### 代码敏感错误率与代价曲线\n* 代价曲线与期望总体代价\n* 假设检验\n\n#### 多分类\n* Linear Regression/Logistic Regression/LDA(Linear Discriminant Analysis)\n* OVO, OVR\n\n#### DT\n* entropy/IG/Gini\n* 预减枝/后减枝\n\n#### ANN\n* Perceptron\n\n","slug":"MachineLearning-ZhouZhihua","published":1,"updated":"2017-12-06T06:46:58.376Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8g9p0009tcinhq8j487z","content":"<h3 id=\"chapter-1\"><a href=\"#chapter-1\" class=\"headerlink\" title=\"chapter 1\"></a>chapter 1</h3><ul>\n<li>No Free Lunch Theorem(NFL)</li>\n</ul>\n<p>算法应用于问题要具体问题具体分析</p>\n<h3 id=\"chapture-2-模型评估和选择\"><a href=\"#chapture-2-模型评估和选择\" class=\"headerlink\" title=\"chapture 2: 模型评估和选择\"></a>chapture 2: 模型评估和选择</h3><h4 id=\"Basic-Concepts\"><a href=\"#Basic-Concepts\" class=\"headerlink\" title=\"Basic Concepts\"></a>Basic Concepts</h4><ul>\n<li>error<ul>\n<li>empirical error/training error 经验误差（训练误差）</li>\n<li>generalization error 泛化误差（预测集误差）</li>\n</ul>\n</li>\n<li>fitting<ul>\n<li>overfitting</li>\n<li>underfitting</li>\n</ul>\n</li>\n<li>set<ul>\n<li>train / dev / test set</li>\n</ul>\n</li>\n<li>cross validation<ul>\n<li>k-fold</li>\n</ul>\n</li>\n<li>bootstrapping<ul>\n<li>适用小数据集</li>\n<li>会改变数据分布</li>\n</ul>\n</li>\n<li>parameter tuning</li>\n<li>performance measure<ul>\n<li>MSE: mean squared error </li>\n<li>error rate</li>\n<li>accuracy rate</li>\n<li>Precision/Recall/F1/ROC/AUC</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"常见错误\"><a href=\"#常见错误\" class=\"headerlink\" title=\"常见错误\"></a>常见错误</h4><ul>\n<li>训练集测试集分布不一致(采样？Group By?)</li>\n<li>数据穿越</li>\n</ul>\n<h4 id=\"代码敏感错误率与代价曲线\"><a href=\"#代码敏感错误率与代价曲线\" class=\"headerlink\" title=\"代码敏感错误率与代价曲线\"></a>代码敏感错误率与代价曲线</h4><ul>\n<li>代价曲线与期望总体代价</li>\n<li>假设检验</li>\n</ul>\n<h4 id=\"多分类\"><a href=\"#多分类\" class=\"headerlink\" title=\"多分类\"></a>多分类</h4><ul>\n<li>Linear Regression/Logistic Regression/LDA(Linear Discriminant Analysis)</li>\n<li>OVO, OVR</li>\n</ul>\n<h4 id=\"DT\"><a href=\"#DT\" class=\"headerlink\" title=\"DT\"></a>DT</h4><ul>\n<li>entropy/IG/Gini</li>\n<li>预减枝/后减枝</li>\n</ul>\n<h4 id=\"ANN\"><a href=\"#ANN\" class=\"headerlink\" title=\"ANN\"></a>ANN</h4><ul>\n<li>Perceptron</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"chapter-1\"><a href=\"#chapter-1\" class=\"headerlink\" title=\"chapter 1\"></a>chapter 1</h3><ul>\n<li>No Free Lunch Theorem(NFL)</li>\n</ul>\n<p>算法应用于问题要具体问题具体分析</p>\n<h3 id=\"chapture-2-模型评估和选择\"><a href=\"#chapture-2-模型评估和选择\" class=\"headerlink\" title=\"chapture 2: 模型评估和选择\"></a>chapture 2: 模型评估和选择</h3><h4 id=\"Basic-Concepts\"><a href=\"#Basic-Concepts\" class=\"headerlink\" title=\"Basic Concepts\"></a>Basic Concepts</h4><ul>\n<li>error<ul>\n<li>empirical error/training error 经验误差（训练误差）</li>\n<li>generalization error 泛化误差（预测集误差）</li>\n</ul>\n</li>\n<li>fitting<ul>\n<li>overfitting</li>\n<li>underfitting</li>\n</ul>\n</li>\n<li>set<ul>\n<li>train / dev / test set</li>\n</ul>\n</li>\n<li>cross validation<ul>\n<li>k-fold</li>\n</ul>\n</li>\n<li>bootstrapping<ul>\n<li>适用小数据集</li>\n<li>会改变数据分布</li>\n</ul>\n</li>\n<li>parameter tuning</li>\n<li>performance measure<ul>\n<li>MSE: mean squared error </li>\n<li>error rate</li>\n<li>accuracy rate</li>\n<li>Precision/Recall/F1/ROC/AUC</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"常见错误\"><a href=\"#常见错误\" class=\"headerlink\" title=\"常见错误\"></a>常见错误</h4><ul>\n<li>训练集测试集分布不一致(采样？Group By?)</li>\n<li>数据穿越</li>\n</ul>\n<h4 id=\"代码敏感错误率与代价曲线\"><a href=\"#代码敏感错误率与代价曲线\" class=\"headerlink\" title=\"代码敏感错误率与代价曲线\"></a>代码敏感错误率与代价曲线</h4><ul>\n<li>代价曲线与期望总体代价</li>\n<li>假设检验</li>\n</ul>\n<h4 id=\"多分类\"><a href=\"#多分类\" class=\"headerlink\" title=\"多分类\"></a>多分类</h4><ul>\n<li>Linear Regression/Logistic Regression/LDA(Linear Discriminant Analysis)</li>\n<li>OVO, OVR</li>\n</ul>\n<h4 id=\"DT\"><a href=\"#DT\" class=\"headerlink\" title=\"DT\"></a>DT</h4><ul>\n<li>entropy/IG/Gini</li>\n<li>预减枝/后减枝</li>\n</ul>\n<h4 id=\"ANN\"><a href=\"#ANN\" class=\"headerlink\" title=\"ANN\"></a>ANN</h4><ul>\n<li>Perceptron</li>\n</ul>\n"},{"title":"dict-vs-bag_of_words","date":"2018-09-27T01:51:55.000Z","_content":"\n1. 词典/词库：可以加入新词等，保证切词粒度\n2. 词袋：count-based（lsa），对标predictive based（lstm）。","source":"_posts/dict-vs-bag-of-words.md","raw":"---\ntitle: dict-vs-bag_of_words\ndate: 2018-09-27 09:51:55\ntags:\n---\n\n1. 词典/词库：可以加入新词等，保证切词粒度\n2. 词袋：count-based（lsa），对标predictive based（lstm）。","slug":"dict-vs-bag-of-words","published":1,"updated":"2018-09-27T01:52:08.740Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8g9w000ctcin827k55xp","content":"<ol>\n<li>词典/词库：可以加入新词等，保证切词粒度</li>\n<li>词袋：count-based（lsa），对标predictive based（lstm）。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<ol>\n<li>词典/词库：可以加入新词等，保证切词粒度</li>\n<li>词袋：count-based（lsa），对标predictive based（lstm）。</li>\n</ol>\n"},{"title":"dt","date":"2017-12-13T13:45:15.000Z","_content":"\n\n### entropy, information gain, gain ratio\n\n","source":"_posts/dt.md","raw":"---\ntitle: dt\ndate: 2017-12-13 21:45:15\ntags:\n---\n\n\n### entropy, information gain, gain ratio\n\n","slug":"dt","published":1,"updated":"2017-12-14T07:10:51.627Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8ga2000dtcinbzrq77v4","content":"<h3 id=\"entropy-information-gain-gain-ratio\"><a href=\"#entropy-information-gain-gain-ratio\" class=\"headerlink\" title=\"entropy, information gain, gain ratio\"></a>entropy, information gain, gain ratio</h3>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"entropy-information-gain-gain-ratio\"><a href=\"#entropy-information-gain-gain-ratio\" class=\"headerlink\" title=\"entropy, information gain, gain ratio\"></a>entropy, information gain, gain ratio</h3>"},{"title":"distributed_tf","date":"2017-12-14T08:20:18.000Z","_content":"\n#### basic concepts\n* cluster: a set of tasks, one or more jobs\n* server: master+worker\n","source":"_posts/distributed-tf.md","raw":"---\ntitle: distributed_tf\ndate: 2017-12-14 16:20:18\ntags:\n---\n\n#### basic concepts\n* cluster: a set of tasks, one or more jobs\n* server: master+worker\n","slug":"distributed-tf","published":1,"updated":"2018-03-15T09:34:19.157Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8ga4000etcinwqnf4t4d","content":"<h4 id=\"basic-concepts\"><a href=\"#basic-concepts\" class=\"headerlink\" title=\"basic concepts\"></a>basic concepts</h4><ul>\n<li>cluster: a set of tasks, one or more jobs</li>\n<li>server: master+worker</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"basic-concepts\"><a href=\"#basic-concepts\" class=\"headerlink\" title=\"basic concepts\"></a>basic concepts</h4><ul>\n<li>cluster: a set of tasks, one or more jobs</li>\n<li>server: master+worker</li>\n</ul>\n"},{"title":"ctr_smooth","date":"2017-12-07T15:05:56.000Z","_content":"\n#### 最好的非个性化模型\n最好的非个性化模型，即CTR倒序模型，那如何得到准确的统计CTR，是本文的关键。\n\n#### 统计的小数效应（置信度和pCTR）\n风控模型中，有两个值非常重要：\n* confidence：用户的数据有多可靠，比如交易记录越多，越可靠\n* score: 用户的资产有多少，比如交易额度越大，资产越多\n\n两个case：\n* 高confidence低score：卖矿泉水的小商贩，转账频繁，大多都是一两块钱\n* 低confidence高score：偶尔用信用卡买了一辆车的大老板\n\n同样的在统计CTR中，也有对应的两个概念：\n* CTR可信不可信\n* CTR是多少\n\n同样的两个case：\n* itemA：10次曝光5次点击，可能受到随机影响，所以confidence低，pCTR高（随机影响也可能导致pCTR偏低）\n* itemB：10000次曝光1000次点击，大数效应，比较可信，confidence高，pCTR低\n\n#### 三个变量的权衡\n为了得到真实的CTR，可以从日志中统计得到：\nexposure, click, ctr\n三个变量\n\nitem | exposure | click | ctr\n---|---|---|---\nA | 100000 | 20000 | 0.2\nB | 10000 | 1000 | 0.1\nC | 10 | 5 | 0.5\n\n那么哪款item最优先级被推荐？\n\n理想情况下是：\n* exposure和click越高越高，confidence越大\n* ctr越高越好，score越大\n\n但是当两者矛盾的时候，就需要平衡一下，综合来看A是最佳的。\n\n#### 简单暴力的bayes平滑\n根据贝叶斯有：先验+事件=后验，那么我们为模型增加人为的知识：\n\n\"所有样本，统一增加b个样本（其中a个正样本)\"\n\na和b的相对值的确定，可以用a/b等于整体平均ctr*rate等来确定，rate常常略微小于1 \n\na和b的绝对值的确定，很有意思：\n\n我常常用excel对优质数据进行标注，看如何设置可以使得优质数据上浮顶部。\n\n#### 贝叶斯平滑物理意义 和 极大后验\n贝叶斯平滑，相当于增加先验，即增加正则，先举一个L2正则的例子：\n线性回归的loss function\n（PS：loss function和cost function是不一样的，cost function是loss function在data上的累计总和）\n\nloss function: (y - f(x))^2\nmaximum likelihood: guass_function(y-f(x))\n\nL2正则相当于对参数的分布增加一个属于高斯分布的假设，\nguass_function(theta)*guass_function(y-f(x))\n\n将这个似然最大化，\nargmax(guass_function(theta)*guass_function(y-f(x)))\n=> argmax(log(guass_function(theta)*guass_function(y-f(x))))\n=> argmax(log(guass_function(theta))+log(guass_function(y-f(x))))\n=> argmin((theta)^2+(y_f(x))^2)\n\n即极大后验。\n\n因此，从参数估计的角度，贝叶斯平滑是将极大似然估计（直接除）变成极大后验估计（分子分母各加一个值）\n\n\n#### REF\n1. http://myslide.cn/slides/977\n2. http://www.jianshu.com/p/a47c46153326","source":"_posts/ctr-smooth.md","raw":"---\ntitle: ctr_smooth\ndate: 2017-12-07 23:05:56\ntags:\n---\n\n#### 最好的非个性化模型\n最好的非个性化模型，即CTR倒序模型，那如何得到准确的统计CTR，是本文的关键。\n\n#### 统计的小数效应（置信度和pCTR）\n风控模型中，有两个值非常重要：\n* confidence：用户的数据有多可靠，比如交易记录越多，越可靠\n* score: 用户的资产有多少，比如交易额度越大，资产越多\n\n两个case：\n* 高confidence低score：卖矿泉水的小商贩，转账频繁，大多都是一两块钱\n* 低confidence高score：偶尔用信用卡买了一辆车的大老板\n\n同样的在统计CTR中，也有对应的两个概念：\n* CTR可信不可信\n* CTR是多少\n\n同样的两个case：\n* itemA：10次曝光5次点击，可能受到随机影响，所以confidence低，pCTR高（随机影响也可能导致pCTR偏低）\n* itemB：10000次曝光1000次点击，大数效应，比较可信，confidence高，pCTR低\n\n#### 三个变量的权衡\n为了得到真实的CTR，可以从日志中统计得到：\nexposure, click, ctr\n三个变量\n\nitem | exposure | click | ctr\n---|---|---|---\nA | 100000 | 20000 | 0.2\nB | 10000 | 1000 | 0.1\nC | 10 | 5 | 0.5\n\n那么哪款item最优先级被推荐？\n\n理想情况下是：\n* exposure和click越高越高，confidence越大\n* ctr越高越好，score越大\n\n但是当两者矛盾的时候，就需要平衡一下，综合来看A是最佳的。\n\n#### 简单暴力的bayes平滑\n根据贝叶斯有：先验+事件=后验，那么我们为模型增加人为的知识：\n\n\"所有样本，统一增加b个样本（其中a个正样本)\"\n\na和b的相对值的确定，可以用a/b等于整体平均ctr*rate等来确定，rate常常略微小于1 \n\na和b的绝对值的确定，很有意思：\n\n我常常用excel对优质数据进行标注，看如何设置可以使得优质数据上浮顶部。\n\n#### 贝叶斯平滑物理意义 和 极大后验\n贝叶斯平滑，相当于增加先验，即增加正则，先举一个L2正则的例子：\n线性回归的loss function\n（PS：loss function和cost function是不一样的，cost function是loss function在data上的累计总和）\n\nloss function: (y - f(x))^2\nmaximum likelihood: guass_function(y-f(x))\n\nL2正则相当于对参数的分布增加一个属于高斯分布的假设，\nguass_function(theta)*guass_function(y-f(x))\n\n将这个似然最大化，\nargmax(guass_function(theta)*guass_function(y-f(x)))\n=> argmax(log(guass_function(theta)*guass_function(y-f(x))))\n=> argmax(log(guass_function(theta))+log(guass_function(y-f(x))))\n=> argmin((theta)^2+(y_f(x))^2)\n\n即极大后验。\n\n因此，从参数估计的角度，贝叶斯平滑是将极大似然估计（直接除）变成极大后验估计（分子分母各加一个值）\n\n\n#### REF\n1. http://myslide.cn/slides/977\n2. http://www.jianshu.com/p/a47c46153326","slug":"ctr-smooth","published":1,"updated":"2017-12-08T12:54:05.404Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8ga6000gtcinrv19aslt","content":"<h4 id=\"最好的非个性化模型\"><a href=\"#最好的非个性化模型\" class=\"headerlink\" title=\"最好的非个性化模型\"></a>最好的非个性化模型</h4><p>最好的非个性化模型，即CTR倒序模型，那如何得到准确的统计CTR，是本文的关键。</p>\n<h4 id=\"统计的小数效应（置信度和pCTR）\"><a href=\"#统计的小数效应（置信度和pCTR）\" class=\"headerlink\" title=\"统计的小数效应（置信度和pCTR）\"></a>统计的小数效应（置信度和pCTR）</h4><p>风控模型中，有两个值非常重要：</p>\n<ul>\n<li>confidence：用户的数据有多可靠，比如交易记录越多，越可靠</li>\n<li>score: 用户的资产有多少，比如交易额度越大，资产越多</li>\n</ul>\n<p>两个case：</p>\n<ul>\n<li>高confidence低score：卖矿泉水的小商贩，转账频繁，大多都是一两块钱</li>\n<li>低confidence高score：偶尔用信用卡买了一辆车的大老板</li>\n</ul>\n<p>同样的在统计CTR中，也有对应的两个概念：</p>\n<ul>\n<li>CTR可信不可信</li>\n<li>CTR是多少</li>\n</ul>\n<p>同样的两个case：</p>\n<ul>\n<li>itemA：10次曝光5次点击，可能受到随机影响，所以confidence低，pCTR高（随机影响也可能导致pCTR偏低）</li>\n<li>itemB：10000次曝光1000次点击，大数效应，比较可信，confidence高，pCTR低</li>\n</ul>\n<h4 id=\"三个变量的权衡\"><a href=\"#三个变量的权衡\" class=\"headerlink\" title=\"三个变量的权衡\"></a>三个变量的权衡</h4><p>为了得到真实的CTR，可以从日志中统计得到：<br>exposure, click, ctr<br>三个变量</p>\n<table>\n<thead>\n<tr>\n<th>item</th>\n<th>exposure</th>\n<th>click</th>\n<th>ctr</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>100000</td>\n<td>20000</td>\n<td>0.2</td>\n</tr>\n<tr>\n<td>B</td>\n<td>10000</td>\n<td>1000</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>C</td>\n<td>10</td>\n<td>5</td>\n<td>0.5</td>\n</tr>\n</tbody>\n</table>\n<p>那么哪款item最优先级被推荐？</p>\n<p>理想情况下是：</p>\n<ul>\n<li>exposure和click越高越高，confidence越大</li>\n<li>ctr越高越好，score越大</li>\n</ul>\n<p>但是当两者矛盾的时候，就需要平衡一下，综合来看A是最佳的。</p>\n<h4 id=\"简单暴力的bayes平滑\"><a href=\"#简单暴力的bayes平滑\" class=\"headerlink\" title=\"简单暴力的bayes平滑\"></a>简单暴力的bayes平滑</h4><p>根据贝叶斯有：先验+事件=后验，那么我们为模型增加人为的知识：</p>\n<p>“所有样本，统一增加b个样本（其中a个正样本)”</p>\n<p>a和b的相对值的确定，可以用a/b等于整体平均ctr*rate等来确定，rate常常略微小于1 </p>\n<p>a和b的绝对值的确定，很有意思：</p>\n<p>我常常用excel对优质数据进行标注，看如何设置可以使得优质数据上浮顶部。</p>\n<h4 id=\"贝叶斯平滑物理意义-和-极大后验\"><a href=\"#贝叶斯平滑物理意义-和-极大后验\" class=\"headerlink\" title=\"贝叶斯平滑物理意义 和 极大后验\"></a>贝叶斯平滑物理意义 和 极大后验</h4><p>贝叶斯平滑，相当于增加先验，即增加正则，先举一个L2正则的例子：<br>线性回归的loss function<br>（PS：loss function和cost function是不一样的，cost function是loss function在data上的累计总和）</p>\n<p>loss function: (y - f(x))^2<br>maximum likelihood: guass_function(y-f(x))</p>\n<p>L2正则相当于对参数的分布增加一个属于高斯分布的假设，<br>guass_function(theta)*guass_function(y-f(x))</p>\n<p>将这个似然最大化，<br>argmax(guass_function(theta)<em>guass_function(y-f(x)))<br>=&gt; argmax(log(guass_function(theta)</em>guass_function(y-f(x))))<br>=&gt; argmax(log(guass_function(theta))+log(guass_function(y-f(x))))<br>=&gt; argmin((theta)^2+(y_f(x))^2)</p>\n<p>即极大后验。</p>\n<p>因此，从参数估计的角度，贝叶斯平滑是将极大似然估计（直接除）变成极大后验估计（分子分母各加一个值）</p>\n<h4 id=\"REF\"><a href=\"#REF\" class=\"headerlink\" title=\"REF\"></a>REF</h4><ol>\n<li><a href=\"http://myslide.cn/slides/977\" target=\"_blank\" rel=\"external\">http://myslide.cn/slides/977</a></li>\n<li><a href=\"http://www.jianshu.com/p/a47c46153326\" target=\"_blank\" rel=\"external\">http://www.jianshu.com/p/a47c46153326</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"最好的非个性化模型\"><a href=\"#最好的非个性化模型\" class=\"headerlink\" title=\"最好的非个性化模型\"></a>最好的非个性化模型</h4><p>最好的非个性化模型，即CTR倒序模型，那如何得到准确的统计CTR，是本文的关键。</p>\n<h4 id=\"统计的小数效应（置信度和pCTR）\"><a href=\"#统计的小数效应（置信度和pCTR）\" class=\"headerlink\" title=\"统计的小数效应（置信度和pCTR）\"></a>统计的小数效应（置信度和pCTR）</h4><p>风控模型中，有两个值非常重要：</p>\n<ul>\n<li>confidence：用户的数据有多可靠，比如交易记录越多，越可靠</li>\n<li>score: 用户的资产有多少，比如交易额度越大，资产越多</li>\n</ul>\n<p>两个case：</p>\n<ul>\n<li>高confidence低score：卖矿泉水的小商贩，转账频繁，大多都是一两块钱</li>\n<li>低confidence高score：偶尔用信用卡买了一辆车的大老板</li>\n</ul>\n<p>同样的在统计CTR中，也有对应的两个概念：</p>\n<ul>\n<li>CTR可信不可信</li>\n<li>CTR是多少</li>\n</ul>\n<p>同样的两个case：</p>\n<ul>\n<li>itemA：10次曝光5次点击，可能受到随机影响，所以confidence低，pCTR高（随机影响也可能导致pCTR偏低）</li>\n<li>itemB：10000次曝光1000次点击，大数效应，比较可信，confidence高，pCTR低</li>\n</ul>\n<h4 id=\"三个变量的权衡\"><a href=\"#三个变量的权衡\" class=\"headerlink\" title=\"三个变量的权衡\"></a>三个变量的权衡</h4><p>为了得到真实的CTR，可以从日志中统计得到：<br>exposure, click, ctr<br>三个变量</p>\n<table>\n<thead>\n<tr>\n<th>item</th>\n<th>exposure</th>\n<th>click</th>\n<th>ctr</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>100000</td>\n<td>20000</td>\n<td>0.2</td>\n</tr>\n<tr>\n<td>B</td>\n<td>10000</td>\n<td>1000</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>C</td>\n<td>10</td>\n<td>5</td>\n<td>0.5</td>\n</tr>\n</tbody>\n</table>\n<p>那么哪款item最优先级被推荐？</p>\n<p>理想情况下是：</p>\n<ul>\n<li>exposure和click越高越高，confidence越大</li>\n<li>ctr越高越好，score越大</li>\n</ul>\n<p>但是当两者矛盾的时候，就需要平衡一下，综合来看A是最佳的。</p>\n<h4 id=\"简单暴力的bayes平滑\"><a href=\"#简单暴力的bayes平滑\" class=\"headerlink\" title=\"简单暴力的bayes平滑\"></a>简单暴力的bayes平滑</h4><p>根据贝叶斯有：先验+事件=后验，那么我们为模型增加人为的知识：</p>\n<p>“所有样本，统一增加b个样本（其中a个正样本)”</p>\n<p>a和b的相对值的确定，可以用a/b等于整体平均ctr*rate等来确定，rate常常略微小于1 </p>\n<p>a和b的绝对值的确定，很有意思：</p>\n<p>我常常用excel对优质数据进行标注，看如何设置可以使得优质数据上浮顶部。</p>\n<h4 id=\"贝叶斯平滑物理意义-和-极大后验\"><a href=\"#贝叶斯平滑物理意义-和-极大后验\" class=\"headerlink\" title=\"贝叶斯平滑物理意义 和 极大后验\"></a>贝叶斯平滑物理意义 和 极大后验</h4><p>贝叶斯平滑，相当于增加先验，即增加正则，先举一个L2正则的例子：<br>线性回归的loss function<br>（PS：loss function和cost function是不一样的，cost function是loss function在data上的累计总和）</p>\n<p>loss function: (y - f(x))^2<br>maximum likelihood: guass_function(y-f(x))</p>\n<p>L2正则相当于对参数的分布增加一个属于高斯分布的假设，<br>guass_function(theta)*guass_function(y-f(x))</p>\n<p>将这个似然最大化，<br>argmax(guass_function(theta)<em>guass_function(y-f(x)))<br>=&gt; argmax(log(guass_function(theta)</em>guass_function(y-f(x))))<br>=&gt; argmax(log(guass_function(theta))+log(guass_function(y-f(x))))<br>=&gt; argmin((theta)^2+(y_f(x))^2)</p>\n<p>即极大后验。</p>\n<p>因此，从参数估计的角度，贝叶斯平滑是将极大似然估计（直接除）变成极大后验估计（分子分母各加一个值）</p>\n<h4 id=\"REF\"><a href=\"#REF\" class=\"headerlink\" title=\"REF\"></a>REF</h4><ol>\n<li><a href=\"http://myslide.cn/slides/977\" target=\"_blank\" rel=\"external\">http://myslide.cn/slides/977</a></li>\n<li><a href=\"http://www.jianshu.com/p/a47c46153326\" target=\"_blank\" rel=\"external\">http://www.jianshu.com/p/a47c46153326</a></li>\n</ol>\n"},{"title":"dnn_embedding","date":"2018-03-24T15:13:33.000Z","_content":"\n#### Keras\n*\n\n#### tensorflow\n*\n","source":"_posts/dnn-embedding.md","raw":"---\ntitle: dnn_embedding\ndate: 2018-03-24 23:13:33\ntags: 数据挖掘\n---\n\n#### Keras\n*\n\n#### tensorflow\n*\n","slug":"dnn-embedding","published":1,"updated":"2018-03-27T08:49:58.974Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8ga8000itcinhb1ldvqg","content":"<h4 id=\"Keras\"><a href=\"#Keras\" class=\"headerlink\" title=\"Keras\"></a>Keras</h4><p>*</p>\n<h4 id=\"tensorflow\"><a href=\"#tensorflow\" class=\"headerlink\" title=\"tensorflow\"></a>tensorflow</h4><p>*</p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"Keras\"><a href=\"#Keras\" class=\"headerlink\" title=\"Keras\"></a>Keras</h4><p>*</p>\n<h4 id=\"tensorflow\"><a href=\"#tensorflow\" class=\"headerlink\" title=\"tensorflow\"></a>tensorflow</h4><p>*</p>\n"},{"title":"edge-rank","date":"2018-01-15T12:37:48.000Z","_content":"\n### ","source":"_posts/edge-rank.md","raw":"---\ntitle: edge-rank\ndate: 2018-01-15 20:37:48\ntags:\n---\n\n### ","slug":"edge-rank","published":1,"updated":"2018-01-15T12:39:04.132Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gab000jtcinsu3rmtyc","content":"<p>### </p>\n","site":{"data":{}},"excerpt":"","more":"<p>### </p>\n"},{"title":"data-warehouse","date":"2018-09-10T09:22:35.000Z","_content":"\n#### 为什么要数据仓库？\n和数据库的区别，即数据仓库的特点：\n* 功能：数仓偏向于数据分析，数据库偏向于交易处理。\n* 特性：数仓强调分析效率，允许数据冗余；数据库强调事物处理，速度，完整性，一致性。\n\n#### 数仓分层\n* ods (operation data store)：操作数据日志（日志表）\n* dw (data warehouse)：数据仓库模型层（往往高表）\n* app (application)：数据仓库展示层（往往宽表）\n\n#### 注意要点\n\n#### 快照表\n\n\n####","source":"_posts/data-warehouse.md","raw":"---\ntitle: data-warehouse\ndate: 2018-09-10 17:22:35\ntags:\n---\n\n#### 为什么要数据仓库？\n和数据库的区别，即数据仓库的特点：\n* 功能：数仓偏向于数据分析，数据库偏向于交易处理。\n* 特性：数仓强调分析效率，允许数据冗余；数据库强调事物处理，速度，完整性，一致性。\n\n#### 数仓分层\n* ods (operation data store)：操作数据日志（日志表）\n* dw (data warehouse)：数据仓库模型层（往往高表）\n* app (application)：数据仓库展示层（往往宽表）\n\n#### 注意要点\n\n#### 快照表\n\n\n####","slug":"data-warehouse","published":1,"updated":"2018-09-27T01:51:32.129Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gad000ltcinrkkavpn0","content":"<h4 id=\"为什么要数据仓库？\"><a href=\"#为什么要数据仓库？\" class=\"headerlink\" title=\"为什么要数据仓库？\"></a>为什么要数据仓库？</h4><p>和数据库的区别，即数据仓库的特点：</p>\n<ul>\n<li>功能：数仓偏向于数据分析，数据库偏向于交易处理。</li>\n<li>特性：数仓强调分析效率，允许数据冗余；数据库强调事物处理，速度，完整性，一致性。</li>\n</ul>\n<h4 id=\"数仓分层\"><a href=\"#数仓分层\" class=\"headerlink\" title=\"数仓分层\"></a>数仓分层</h4><ul>\n<li>ods (operation data store)：操作数据日志（日志表）</li>\n<li>dw (data warehouse)：数据仓库模型层（往往高表）</li>\n<li>app (application)：数据仓库展示层（往往宽表）</li>\n</ul>\n<h4 id=\"注意要点\"><a href=\"#注意要点\" class=\"headerlink\" title=\"注意要点\"></a>注意要点</h4><h4 id=\"快照表\"><a href=\"#快照表\" class=\"headerlink\" title=\"快照表\"></a>快照表</h4><p>####</p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"为什么要数据仓库？\"><a href=\"#为什么要数据仓库？\" class=\"headerlink\" title=\"为什么要数据仓库？\"></a>为什么要数据仓库？</h4><p>和数据库的区别，即数据仓库的特点：</p>\n<ul>\n<li>功能：数仓偏向于数据分析，数据库偏向于交易处理。</li>\n<li>特性：数仓强调分析效率，允许数据冗余；数据库强调事物处理，速度，完整性，一致性。</li>\n</ul>\n<h4 id=\"数仓分层\"><a href=\"#数仓分层\" class=\"headerlink\" title=\"数仓分层\"></a>数仓分层</h4><ul>\n<li>ods (operation data store)：操作数据日志（日志表）</li>\n<li>dw (data warehouse)：数据仓库模型层（往往高表）</li>\n<li>app (application)：数据仓库展示层（往往宽表）</li>\n</ul>\n<h4 id=\"注意要点\"><a href=\"#注意要点\" class=\"headerlink\" title=\"注意要点\"></a>注意要点</h4><h4 id=\"快照表\"><a href=\"#快照表\" class=\"headerlink\" title=\"快照表\"></a>快照表</h4><p>####</p>\n"},{"title":"exploding_n_vanishing","date":"2017-11-23T06:33:26.000Z","_content":"\nOpinions:\n1. This two problems becomes worth as the number of layers in the architecture increases.\n2. \n\n\n\n* vanishing gradients\n假设有一个N层的DNN模型，那么y可以\n\ny = Wn * Wn-1 * Wn-2 *...* W1 * X\n\n\n","source":"_posts/exploding-n-vanishing.md","raw":"---\ntitle: exploding_n_vanishing\ndate: 2017-11-23 14:33:26\ntags: DNN\n---\n\nOpinions:\n1. This two problems becomes worth as the number of layers in the architecture increases.\n2. \n\n\n\n* vanishing gradients\n假设有一个N层的DNN模型，那么y可以\n\ny = Wn * Wn-1 * Wn-2 *...* W1 * X\n\n\n","slug":"exploding-n-vanishing","published":1,"updated":"2017-11-23T07:02:19.179Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gaf000mtcinyuvaiod8","content":"<p>Opinions:</p>\n<ol>\n<li>This two problems becomes worth as the number of layers in the architecture increases.</li>\n<li></li>\n</ol>\n<ul>\n<li>vanishing gradients<br>假设有一个N层的DNN模型，那么y可以</li>\n</ul>\n<p>y = Wn <em> Wn-1 </em> Wn-2 <em>…</em> W1 * X</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Opinions:</p>\n<ol>\n<li>This two problems becomes worth as the number of layers in the architecture increases.</li>\n<li></li>\n</ol>\n<ul>\n<li>vanishing gradients<br>假设有一个N层的DNN模型，那么y可以</li>\n</ul>\n<p>y = Wn <em> Wn-1 </em> Wn-2 <em>…</em> W1 * X</p>\n"},{"title":"elastic-search.md","date":"2018-04-02T11:21:02.000Z","_content":"\n#### Install\n* 配置公网访问\n```\n修改配置文件 config/elasticsearch.yml\nnetwork.host: 0.0.0.0\n```\n* 扩大vm.max_map_count\n```\nsysctl -w vm.max_map_count=655360\n```\n* node 安装\n```\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\n```\n* kibana install\n```\nkibana安装后外网无法访问：\n修改config/kibaba.yml下的server.host为0.0.0.0\n```\n* cluster config\n```\ncluster.name: xxx-search\n\nnode.name: node-2\nnode.master: false\nnode.data: true\n\nnetwork.host: 0.0.0.0\n\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\n\ndiscovery.zen.ping.unicast.hosts: [\"xxx.xxx.xxx.xxx\"]\n```\n*","source":"_posts/elastic-search-md.md","raw":"---\ntitle: elastic-search.md\ndate: 2018-04-02 19:21:02\ntags: Search\n---\n\n#### Install\n* 配置公网访问\n```\n修改配置文件 config/elasticsearch.yml\nnetwork.host: 0.0.0.0\n```\n* 扩大vm.max_map_count\n```\nsysctl -w vm.max_map_count=655360\n```\n* node 安装\n```\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\n```\n* kibana install\n```\nkibana安装后外网无法访问：\n修改config/kibaba.yml下的server.host为0.0.0.0\n```\n* cluster config\n```\ncluster.name: xxx-search\n\nnode.name: node-2\nnode.master: false\nnode.data: true\n\nnetwork.host: 0.0.0.0\n\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\n\ndiscovery.zen.ping.unicast.hosts: [\"xxx.xxx.xxx.xxx\"]\n```\n*","slug":"elastic-search-md","published":1,"updated":"2018-04-04T06:24:53.537Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gah000ntcinowujpj7i","content":"<h4 id=\"Install\"><a href=\"#Install\" class=\"headerlink\" title=\"Install\"></a>Install</h4><ul>\n<li><p>配置公网访问</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">修改配置文件 config/elasticsearch.yml</div><div class=\"line\">network.host: 0.0.0.0</div></pre></td></tr></table></figure>\n</li>\n<li><p>扩大vm.max_map_count</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sysctl -w vm.max_map_count=655360</div></pre></td></tr></table></figure>\n</li>\n<li><p>node 安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">http.cors.enabled: true</div><div class=\"line\">http.cors.allow-origin: &quot;*&quot;</div></pre></td></tr></table></figure>\n</li>\n<li><p>kibana install</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">kibana安装后外网无法访问：</div><div class=\"line\">修改config/kibaba.yml下的server.host为0.0.0.0</div></pre></td></tr></table></figure>\n</li>\n<li><p>cluster config</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">cluster.name: xxx-search</div><div class=\"line\"></div><div class=\"line\">node.name: node-2</div><div class=\"line\">node.master: false</div><div class=\"line\">node.data: true</div><div class=\"line\"></div><div class=\"line\">network.host: 0.0.0.0</div><div class=\"line\"></div><div class=\"line\">http.cors.enabled: true</div><div class=\"line\">http.cors.allow-origin: &quot;*&quot;</div><div class=\"line\"></div><div class=\"line\">discovery.zen.ping.unicast.hosts: [&quot;xxx.xxx.xxx.xxx&quot;]</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>*</p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"Install\"><a href=\"#Install\" class=\"headerlink\" title=\"Install\"></a>Install</h4><ul>\n<li><p>配置公网访问</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">修改配置文件 config/elasticsearch.yml</div><div class=\"line\">network.host: 0.0.0.0</div></pre></td></tr></table></figure>\n</li>\n<li><p>扩大vm.max_map_count</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sysctl -w vm.max_map_count=655360</div></pre></td></tr></table></figure>\n</li>\n<li><p>node 安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">http.cors.enabled: true</div><div class=\"line\">http.cors.allow-origin: &quot;*&quot;</div></pre></td></tr></table></figure>\n</li>\n<li><p>kibana install</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">kibana安装后外网无法访问：</div><div class=\"line\">修改config/kibaba.yml下的server.host为0.0.0.0</div></pre></td></tr></table></figure>\n</li>\n<li><p>cluster config</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">cluster.name: xxx-search</div><div class=\"line\"></div><div class=\"line\">node.name: node-2</div><div class=\"line\">node.master: false</div><div class=\"line\">node.data: true</div><div class=\"line\"></div><div class=\"line\">network.host: 0.0.0.0</div><div class=\"line\"></div><div class=\"line\">http.cors.enabled: true</div><div class=\"line\">http.cors.allow-origin: &quot;*&quot;</div><div class=\"line\"></div><div class=\"line\">discovery.zen.ping.unicast.hosts: [&quot;xxx.xxx.xxx.xxx&quot;]</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>*</p>\n"},{"title":"ee-n-dqn","date":"2017-10-24T15:59:18.000Z","_content":"\n### 先有鸡还是先有蛋？\n\n#### 数据闭环\n推荐系统根据用户日志来进行建模推荐，即：\n日志 -> 推荐算法 -> 用户\n\n<!-- more -->\n\n日志也是由用户产生的，即：\n用户 -> 日志\n\n两者拼成一个环状，我们称之为\"数据闭环\"，即：\n{% asset_img \"1.png\" [1.png] %}\n\n#### \"数据闭环\"和\"越推越窄\"\n这是一个\"先有鸡还是先有蛋？\"的问题\n> 问：为什么给A推荐\"摇滚\"歌曲？\n> 答：因为A过去听的都是\"摇滚\"歌曲，所以A喜欢\"摇滚\"。\n> 问：推荐系统不给A用户推\"非摇滚\"，用户怎么能听到\"非摇滚\"？\n\n在数据闭环中流转的都是\"老Item\"，新\"Item\"并没有多少展现机会，推荐变得越来越窄\n\n#### \"越推越窄\"解决方案\n越推越窄是典型的EE问题(explore & exploit)\n解决方案有两类：\n1. Bandit: epsilon-greedy, thompson sampling, UCB, linUCB\n2. RL\n\n#### Bandit的方案\nbandit方案可以参考 http://banditalgs.com/ ，此处不做详细解释, 常见有以下方法：\n* epsilon-greedy\n* Thompson Sampling\n* UCB\n* linUCB\n\n### RL的方案\nRL解决了ML解决不了的两大问题：\n* 延迟reward问题\n* 数据缺失问题（EE问题，先有鸡先有单\nRL有两大实体：\n* agent\n\t* agent可以从environment中得到reward\n\t* agent需要知道自己的state, agent可以选择自己的action，即是一个p(action|state)的求解过程\n* environment\n\t* environment需提供一个reward函数（往往自定义设计）\n\t* environment需进行state的状态转移（往往是黑盒子）\n\t* environment需接收agent的action\n\n两大实体互相作用，有几大重要的元素:\n* action: 动作，由agent产生，作用于environment\n* reward: 奖赏，environment针对agent的state+action产生的奖赏or惩罚\n* state: agent的状态，由action实现状态转移，即p(state_x+1|state_x, action_x)的马尔科夫转移过程\n* observation: 即state的外在表现\n\n用图可视化即\n{% asset_img \"2.png\" [2.png] %}\n\n### 两种observation\nobservation是state的外在表现，那么observation也有两种：\n1. state space: 直接表达state的空间\n\t比如cartpole中的observation(state)的定义是[position of cart, velocity of cart, angle of pole, rotation rate of pole]\n\t有意思的是，并不需要（往往也不知道）其具体的含义，只知道是一个四维数组\n2. pixels: \n\t直接从像素级别（声音，嗅觉，味觉，触觉）等得到observation\n\t有意思的是，某时刻的图片不一定能够表达全部信息（比如速度），因此可能用图片串表示observation\n\tp(action_t|pixel_t, pixel_t-1, pixel_t-2, ..., pixel_1)\n\t\n### RL\nreinforcement learning有两个比较通用的算法\n* Q learning \n* policy gradients\n\n### Q-learning\nQ-learning的核心是计算Q值，那么Q值的定义是：\nQ value =  what our return would be, if we were to take an action in a given state\n即Q是一个两维空间[observation, action]，表示在某个observation时执行某个action的总的reward和（立即的reward和之后的reward的discount）\n\n#### Q值 -> action \n假设已经有了Q值，那么如何sample出一个action，可以简单用目前observation下的最大的Q，顺便加一些随机性来探索。\n\n#### Q值更新\nQ值的更新需要用到Bellman equation，即：\nQ(s,a) = r + γ(max(Q(s’,a’))\n其中,\ns表示state，也即observation\na表示action\nr表示current reward\ns’表示next state，即state下做出action之后到达的new state\na’表示next state后的策略，max(Q(s’,a’)表示s’后的最佳策略的Q值\nγ表示future reward的一个discount\n\n有意思的是，我们用差分，设置步长，确定方向，来逼近这个值：\n```\nQ[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n```\n\nOpenAI的FrozenLake-v0完整的code如下：\n```\nimport gym\nimport numpy as np\nenv = gym.make('FrozenLake-v0')\n#Initialize table with all zeros\nQ = np.zeros([env.observation_space.n,env.action_space.n])\n# Set learning parameters\nlr = .8\ny = .95\nnum_episodes = 2000\n#create lists to contain total rewards and steps per episode\n#jList = []\nrList = []\nfor i in range(num_episodes):\n    #Reset environment and get first new observation\n    s = env.reset()\n    rAll = 0\n    d = False\n    j = 0\n    #The Q-Table learning algorithm\n    while j < 99:\n        j+=1\n        #Choose an action by greedily (with noise) picking from Q table\n        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n        #Get new state and reward from environment\n        s1,r,d,_ = env.step(a)\n        #Update Q-Table with new knowledge\n        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n        rAll += r\n        s = s1\n        if d == True:\n            break\n    #jList.append(j)\n    rList.append(rAll)\n```\n\n\n### DQN(Deep Q Network)\n比如利用CNN来做observation来表达state，即是DQN，后续再更新。\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/ee-n-dqn.md","raw":"---\ntitle: ee-n-dqn\ndate: 2017-10-24 23:59:18\ntags:\n---\n\n### 先有鸡还是先有蛋？\n\n#### 数据闭环\n推荐系统根据用户日志来进行建模推荐，即：\n日志 -> 推荐算法 -> 用户\n\n<!-- more -->\n\n日志也是由用户产生的，即：\n用户 -> 日志\n\n两者拼成一个环状，我们称之为\"数据闭环\"，即：\n{% asset_img \"1.png\" [1.png] %}\n\n#### \"数据闭环\"和\"越推越窄\"\n这是一个\"先有鸡还是先有蛋？\"的问题\n> 问：为什么给A推荐\"摇滚\"歌曲？\n> 答：因为A过去听的都是\"摇滚\"歌曲，所以A喜欢\"摇滚\"。\n> 问：推荐系统不给A用户推\"非摇滚\"，用户怎么能听到\"非摇滚\"？\n\n在数据闭环中流转的都是\"老Item\"，新\"Item\"并没有多少展现机会，推荐变得越来越窄\n\n#### \"越推越窄\"解决方案\n越推越窄是典型的EE问题(explore & exploit)\n解决方案有两类：\n1. Bandit: epsilon-greedy, thompson sampling, UCB, linUCB\n2. RL\n\n#### Bandit的方案\nbandit方案可以参考 http://banditalgs.com/ ，此处不做详细解释, 常见有以下方法：\n* epsilon-greedy\n* Thompson Sampling\n* UCB\n* linUCB\n\n### RL的方案\nRL解决了ML解决不了的两大问题：\n* 延迟reward问题\n* 数据缺失问题（EE问题，先有鸡先有单\nRL有两大实体：\n* agent\n\t* agent可以从environment中得到reward\n\t* agent需要知道自己的state, agent可以选择自己的action，即是一个p(action|state)的求解过程\n* environment\n\t* environment需提供一个reward函数（往往自定义设计）\n\t* environment需进行state的状态转移（往往是黑盒子）\n\t* environment需接收agent的action\n\n两大实体互相作用，有几大重要的元素:\n* action: 动作，由agent产生，作用于environment\n* reward: 奖赏，environment针对agent的state+action产生的奖赏or惩罚\n* state: agent的状态，由action实现状态转移，即p(state_x+1|state_x, action_x)的马尔科夫转移过程\n* observation: 即state的外在表现\n\n用图可视化即\n{% asset_img \"2.png\" [2.png] %}\n\n### 两种observation\nobservation是state的外在表现，那么observation也有两种：\n1. state space: 直接表达state的空间\n\t比如cartpole中的observation(state)的定义是[position of cart, velocity of cart, angle of pole, rotation rate of pole]\n\t有意思的是，并不需要（往往也不知道）其具体的含义，只知道是一个四维数组\n2. pixels: \n\t直接从像素级别（声音，嗅觉，味觉，触觉）等得到observation\n\t有意思的是，某时刻的图片不一定能够表达全部信息（比如速度），因此可能用图片串表示observation\n\tp(action_t|pixel_t, pixel_t-1, pixel_t-2, ..., pixel_1)\n\t\n### RL\nreinforcement learning有两个比较通用的算法\n* Q learning \n* policy gradients\n\n### Q-learning\nQ-learning的核心是计算Q值，那么Q值的定义是：\nQ value =  what our return would be, if we were to take an action in a given state\n即Q是一个两维空间[observation, action]，表示在某个observation时执行某个action的总的reward和（立即的reward和之后的reward的discount）\n\n#### Q值 -> action \n假设已经有了Q值，那么如何sample出一个action，可以简单用目前observation下的最大的Q，顺便加一些随机性来探索。\n\n#### Q值更新\nQ值的更新需要用到Bellman equation，即：\nQ(s,a) = r + γ(max(Q(s’,a’))\n其中,\ns表示state，也即observation\na表示action\nr表示current reward\ns’表示next state，即state下做出action之后到达的new state\na’表示next state后的策略，max(Q(s’,a’)表示s’后的最佳策略的Q值\nγ表示future reward的一个discount\n\n有意思的是，我们用差分，设置步长，确定方向，来逼近这个值：\n```\nQ[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n```\n\nOpenAI的FrozenLake-v0完整的code如下：\n```\nimport gym\nimport numpy as np\nenv = gym.make('FrozenLake-v0')\n#Initialize table with all zeros\nQ = np.zeros([env.observation_space.n,env.action_space.n])\n# Set learning parameters\nlr = .8\ny = .95\nnum_episodes = 2000\n#create lists to contain total rewards and steps per episode\n#jList = []\nrList = []\nfor i in range(num_episodes):\n    #Reset environment and get first new observation\n    s = env.reset()\n    rAll = 0\n    d = False\n    j = 0\n    #The Q-Table learning algorithm\n    while j < 99:\n        j+=1\n        #Choose an action by greedily (with noise) picking from Q table\n        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n        #Get new state and reward from environment\n        s1,r,d,_ = env.step(a)\n        #Update Q-Table with new knowledge\n        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n        rAll += r\n        s = s1\n        if d == True:\n            break\n    #jList.append(j)\n    rList.append(rAll)\n```\n\n\n### DQN(Deep Q Network)\n比如利用CNN来做observation来表达state，即是DQN，后续再更新。\n\n\n\n\n\n\n\n\n\n\n","slug":"ee-n-dqn","published":1,"updated":"2017-10-25T08:34:57.845Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gaj000qtcinj59u112i","content":"<h3 id=\"先有鸡还是先有蛋？\"><a href=\"#先有鸡还是先有蛋？\" class=\"headerlink\" title=\"先有鸡还是先有蛋？\"></a>先有鸡还是先有蛋？</h3><h4 id=\"数据闭环\"><a href=\"#数据闭环\" class=\"headerlink\" title=\"数据闭环\"></a>数据闭环</h4><p>推荐系统根据用户日志来进行建模推荐，即：<br>日志 -&gt; 推荐算法 -&gt; 用户</p>\n<a id=\"more\"></a>\n<p>日志也是由用户产生的，即：<br>用户 -&gt; 日志</p>\n<p>两者拼成一个环状，我们称之为”数据闭环”，即：<br><img src=\"/2017/10/24/ee-n-dqn/1.png\" alt=\"[1.png]\" title=\"[1.png]\"></p>\n<h4 id=\"“数据闭环”和”越推越窄”\"><a href=\"#“数据闭环”和”越推越窄”\" class=\"headerlink\" title=\"“数据闭环”和”越推越窄”\"></a>“数据闭环”和”越推越窄”</h4><p>这是一个”先有鸡还是先有蛋？”的问题</p>\n<blockquote>\n<p>问：为什么给A推荐”摇滚”歌曲？<br>答：因为A过去听的都是”摇滚”歌曲，所以A喜欢”摇滚”。<br>问：推荐系统不给A用户推”非摇滚”，用户怎么能听到”非摇滚”？</p>\n</blockquote>\n<p>在数据闭环中流转的都是”老Item”，新”Item”并没有多少展现机会，推荐变得越来越窄</p>\n<h4 id=\"“越推越窄”解决方案\"><a href=\"#“越推越窄”解决方案\" class=\"headerlink\" title=\"“越推越窄”解决方案\"></a>“越推越窄”解决方案</h4><p>越推越窄是典型的EE问题(explore &amp; exploit)<br>解决方案有两类：</p>\n<ol>\n<li>Bandit: epsilon-greedy, thompson sampling, UCB, linUCB</li>\n<li>RL</li>\n</ol>\n<h4 id=\"Bandit的方案\"><a href=\"#Bandit的方案\" class=\"headerlink\" title=\"Bandit的方案\"></a>Bandit的方案</h4><p>bandit方案可以参考 <a href=\"http://banditalgs.com/\" target=\"_blank\" rel=\"external\">http://banditalgs.com/</a> ，此处不做详细解释, 常见有以下方法：</p>\n<ul>\n<li>epsilon-greedy</li>\n<li>Thompson Sampling</li>\n<li>UCB</li>\n<li>linUCB</li>\n</ul>\n<h3 id=\"RL的方案\"><a href=\"#RL的方案\" class=\"headerlink\" title=\"RL的方案\"></a>RL的方案</h3><p>RL解决了ML解决不了的两大问题：</p>\n<ul>\n<li>延迟reward问题</li>\n<li>数据缺失问题（EE问题，先有鸡先有单<br>RL有两大实体：</li>\n<li>agent<ul>\n<li>agent可以从environment中得到reward</li>\n<li>agent需要知道自己的state, agent可以选择自己的action，即是一个p(action|state)的求解过程</li>\n</ul>\n</li>\n<li>environment<ul>\n<li>environment需提供一个reward函数（往往自定义设计）</li>\n<li>environment需进行state的状态转移（往往是黑盒子）</li>\n<li>environment需接收agent的action</li>\n</ul>\n</li>\n</ul>\n<p>两大实体互相作用，有几大重要的元素:</p>\n<ul>\n<li>action: 动作，由agent产生，作用于environment</li>\n<li>reward: 奖赏，environment针对agent的state+action产生的奖赏or惩罚</li>\n<li>state: agent的状态，由action实现状态转移，即p(state_x+1|state_x, action_x)的马尔科夫转移过程</li>\n<li>observation: 即state的外在表现</li>\n</ul>\n<p>用图可视化即<br><img src=\"/2017/10/24/ee-n-dqn/2.png\" alt=\"[2.png]\" title=\"[2.png]\"></p>\n<h3 id=\"两种observation\"><a href=\"#两种observation\" class=\"headerlink\" title=\"两种observation\"></a>两种observation</h3><p>observation是state的外在表现，那么observation也有两种：</p>\n<ol>\n<li>state space: 直接表达state的空间<br> 比如cartpole中的observation(state)的定义是[position of cart, velocity of cart, angle of pole, rotation rate of pole]<br> 有意思的是，并不需要（往往也不知道）其具体的含义，只知道是一个四维数组</li>\n<li>pixels:<br> 直接从像素级别（声音，嗅觉，味觉，触觉）等得到observation<br> 有意思的是，某时刻的图片不一定能够表达全部信息（比如速度），因此可能用图片串表示observation<br> p(action_t|pixel_t, pixel_t-1, pixel_t-2, …, pixel_1)</li>\n</ol>\n<h3 id=\"RL\"><a href=\"#RL\" class=\"headerlink\" title=\"RL\"></a>RL</h3><p>reinforcement learning有两个比较通用的算法</p>\n<ul>\n<li>Q learning </li>\n<li>policy gradients</li>\n</ul>\n<h3 id=\"Q-learning\"><a href=\"#Q-learning\" class=\"headerlink\" title=\"Q-learning\"></a>Q-learning</h3><p>Q-learning的核心是计算Q值，那么Q值的定义是：<br>Q value =  what our return would be, if we were to take an action in a given state<br>即Q是一个两维空间[observation, action]，表示在某个observation时执行某个action的总的reward和（立即的reward和之后的reward的discount）</p>\n<h4 id=\"Q值-gt-action\"><a href=\"#Q值-gt-action\" class=\"headerlink\" title=\"Q值 -&gt; action\"></a>Q值 -&gt; action</h4><p>假设已经有了Q值，那么如何sample出一个action，可以简单用目前observation下的最大的Q，顺便加一些随机性来探索。</p>\n<h4 id=\"Q值更新\"><a href=\"#Q值更新\" class=\"headerlink\" title=\"Q值更新\"></a>Q值更新</h4><p>Q值的更新需要用到Bellman equation，即：<br>Q(s,a) = r + γ(max(Q(s’,a’))<br>其中,<br>s表示state，也即observation<br>a表示action<br>r表示current reward<br>s’表示next state，即state下做出action之后到达的new state<br>a’表示next state后的策略，max(Q(s’,a’)表示s’后的最佳策略的Q值<br>γ表示future reward的一个discount</p>\n<p>有意思的是，我们用差分，设置步长，确定方向，来逼近这个值：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])</div></pre></td></tr></table></figure></p>\n<p>OpenAI的FrozenLake-v0完整的code如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">import gym</div><div class=\"line\">import numpy as np</div><div class=\"line\">env = gym.make(&apos;FrozenLake-v0&apos;)</div><div class=\"line\">#Initialize table with all zeros</div><div class=\"line\">Q = np.zeros([env.observation_space.n,env.action_space.n])</div><div class=\"line\"># Set learning parameters</div><div class=\"line\">lr = .8</div><div class=\"line\">y = .95</div><div class=\"line\">num_episodes = 2000</div><div class=\"line\">#create lists to contain total rewards and steps per episode</div><div class=\"line\">#jList = []</div><div class=\"line\">rList = []</div><div class=\"line\">for i in range(num_episodes):</div><div class=\"line\">    #Reset environment and get first new observation</div><div class=\"line\">    s = env.reset()</div><div class=\"line\">    rAll = 0</div><div class=\"line\">    d = False</div><div class=\"line\">    j = 0</div><div class=\"line\">    #The Q-Table learning algorithm</div><div class=\"line\">    while j &lt; 99:</div><div class=\"line\">        j+=1</div><div class=\"line\">        #Choose an action by greedily (with noise) picking from Q table</div><div class=\"line\">        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))</div><div class=\"line\">        #Get new state and reward from environment</div><div class=\"line\">        s1,r,d,_ = env.step(a)</div><div class=\"line\">        #Update Q-Table with new knowledge</div><div class=\"line\">        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])</div><div class=\"line\">        rAll += r</div><div class=\"line\">        s = s1</div><div class=\"line\">        if d == True:</div><div class=\"line\">            break</div><div class=\"line\">    #jList.append(j)</div><div class=\"line\">    rList.append(rAll)</div></pre></td></tr></table></figure></p>\n<h3 id=\"DQN-Deep-Q-Network\"><a href=\"#DQN-Deep-Q-Network\" class=\"headerlink\" title=\"DQN(Deep Q Network)\"></a>DQN(Deep Q Network)</h3><p>比如利用CNN来做observation来表达state，即是DQN，后续再更新。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"先有鸡还是先有蛋？\"><a href=\"#先有鸡还是先有蛋？\" class=\"headerlink\" title=\"先有鸡还是先有蛋？\"></a>先有鸡还是先有蛋？</h3><h4 id=\"数据闭环\"><a href=\"#数据闭环\" class=\"headerlink\" title=\"数据闭环\"></a>数据闭环</h4><p>推荐系统根据用户日志来进行建模推荐，即：<br>日志 -&gt; 推荐算法 -&gt; 用户</p>","more":"<p>日志也是由用户产生的，即：<br>用户 -&gt; 日志</p>\n<p>两者拼成一个环状，我们称之为”数据闭环”，即：<br><img src=\"/2017/10/24/ee-n-dqn/1.png\" alt=\"[1.png]\" title=\"[1.png]\"></p>\n<h4 id=\"“数据闭环”和”越推越窄”\"><a href=\"#“数据闭环”和”越推越窄”\" class=\"headerlink\" title=\"“数据闭环”和”越推越窄”\"></a>“数据闭环”和”越推越窄”</h4><p>这是一个”先有鸡还是先有蛋？”的问题</p>\n<blockquote>\n<p>问：为什么给A推荐”摇滚”歌曲？<br>答：因为A过去听的都是”摇滚”歌曲，所以A喜欢”摇滚”。<br>问：推荐系统不给A用户推”非摇滚”，用户怎么能听到”非摇滚”？</p>\n</blockquote>\n<p>在数据闭环中流转的都是”老Item”，新”Item”并没有多少展现机会，推荐变得越来越窄</p>\n<h4 id=\"“越推越窄”解决方案\"><a href=\"#“越推越窄”解决方案\" class=\"headerlink\" title=\"“越推越窄”解决方案\"></a>“越推越窄”解决方案</h4><p>越推越窄是典型的EE问题(explore &amp; exploit)<br>解决方案有两类：</p>\n<ol>\n<li>Bandit: epsilon-greedy, thompson sampling, UCB, linUCB</li>\n<li>RL</li>\n</ol>\n<h4 id=\"Bandit的方案\"><a href=\"#Bandit的方案\" class=\"headerlink\" title=\"Bandit的方案\"></a>Bandit的方案</h4><p>bandit方案可以参考 <a href=\"http://banditalgs.com/\" target=\"_blank\" rel=\"external\">http://banditalgs.com/</a> ，此处不做详细解释, 常见有以下方法：</p>\n<ul>\n<li>epsilon-greedy</li>\n<li>Thompson Sampling</li>\n<li>UCB</li>\n<li>linUCB</li>\n</ul>\n<h3 id=\"RL的方案\"><a href=\"#RL的方案\" class=\"headerlink\" title=\"RL的方案\"></a>RL的方案</h3><p>RL解决了ML解决不了的两大问题：</p>\n<ul>\n<li>延迟reward问题</li>\n<li>数据缺失问题（EE问题，先有鸡先有单<br>RL有两大实体：</li>\n<li>agent<ul>\n<li>agent可以从environment中得到reward</li>\n<li>agent需要知道自己的state, agent可以选择自己的action，即是一个p(action|state)的求解过程</li>\n</ul>\n</li>\n<li>environment<ul>\n<li>environment需提供一个reward函数（往往自定义设计）</li>\n<li>environment需进行state的状态转移（往往是黑盒子）</li>\n<li>environment需接收agent的action</li>\n</ul>\n</li>\n</ul>\n<p>两大实体互相作用，有几大重要的元素:</p>\n<ul>\n<li>action: 动作，由agent产生，作用于environment</li>\n<li>reward: 奖赏，environment针对agent的state+action产生的奖赏or惩罚</li>\n<li>state: agent的状态，由action实现状态转移，即p(state_x+1|state_x, action_x)的马尔科夫转移过程</li>\n<li>observation: 即state的外在表现</li>\n</ul>\n<p>用图可视化即<br><img src=\"/2017/10/24/ee-n-dqn/2.png\" alt=\"[2.png]\" title=\"[2.png]\"></p>\n<h3 id=\"两种observation\"><a href=\"#两种observation\" class=\"headerlink\" title=\"两种observation\"></a>两种observation</h3><p>observation是state的外在表现，那么observation也有两种：</p>\n<ol>\n<li>state space: 直接表达state的空间<br> 比如cartpole中的observation(state)的定义是[position of cart, velocity of cart, angle of pole, rotation rate of pole]<br> 有意思的是，并不需要（往往也不知道）其具体的含义，只知道是一个四维数组</li>\n<li>pixels:<br> 直接从像素级别（声音，嗅觉，味觉，触觉）等得到observation<br> 有意思的是，某时刻的图片不一定能够表达全部信息（比如速度），因此可能用图片串表示observation<br> p(action_t|pixel_t, pixel_t-1, pixel_t-2, …, pixel_1)</li>\n</ol>\n<h3 id=\"RL\"><a href=\"#RL\" class=\"headerlink\" title=\"RL\"></a>RL</h3><p>reinforcement learning有两个比较通用的算法</p>\n<ul>\n<li>Q learning </li>\n<li>policy gradients</li>\n</ul>\n<h3 id=\"Q-learning\"><a href=\"#Q-learning\" class=\"headerlink\" title=\"Q-learning\"></a>Q-learning</h3><p>Q-learning的核心是计算Q值，那么Q值的定义是：<br>Q value =  what our return would be, if we were to take an action in a given state<br>即Q是一个两维空间[observation, action]，表示在某个observation时执行某个action的总的reward和（立即的reward和之后的reward的discount）</p>\n<h4 id=\"Q值-gt-action\"><a href=\"#Q值-gt-action\" class=\"headerlink\" title=\"Q值 -&gt; action\"></a>Q值 -&gt; action</h4><p>假设已经有了Q值，那么如何sample出一个action，可以简单用目前observation下的最大的Q，顺便加一些随机性来探索。</p>\n<h4 id=\"Q值更新\"><a href=\"#Q值更新\" class=\"headerlink\" title=\"Q值更新\"></a>Q值更新</h4><p>Q值的更新需要用到Bellman equation，即：<br>Q(s,a) = r + γ(max(Q(s’,a’))<br>其中,<br>s表示state，也即observation<br>a表示action<br>r表示current reward<br>s’表示next state，即state下做出action之后到达的new state<br>a’表示next state后的策略，max(Q(s’,a’)表示s’后的最佳策略的Q值<br>γ表示future reward的一个discount</p>\n<p>有意思的是，我们用差分，设置步长，确定方向，来逼近这个值：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])</div></pre></td></tr></table></figure></p>\n<p>OpenAI的FrozenLake-v0完整的code如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">import gym</div><div class=\"line\">import numpy as np</div><div class=\"line\">env = gym.make(&apos;FrozenLake-v0&apos;)</div><div class=\"line\">#Initialize table with all zeros</div><div class=\"line\">Q = np.zeros([env.observation_space.n,env.action_space.n])</div><div class=\"line\"># Set learning parameters</div><div class=\"line\">lr = .8</div><div class=\"line\">y = .95</div><div class=\"line\">num_episodes = 2000</div><div class=\"line\">#create lists to contain total rewards and steps per episode</div><div class=\"line\">#jList = []</div><div class=\"line\">rList = []</div><div class=\"line\">for i in range(num_episodes):</div><div class=\"line\">    #Reset environment and get first new observation</div><div class=\"line\">    s = env.reset()</div><div class=\"line\">    rAll = 0</div><div class=\"line\">    d = False</div><div class=\"line\">    j = 0</div><div class=\"line\">    #The Q-Table learning algorithm</div><div class=\"line\">    while j &lt; 99:</div><div class=\"line\">        j+=1</div><div class=\"line\">        #Choose an action by greedily (with noise) picking from Q table</div><div class=\"line\">        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))</div><div class=\"line\">        #Get new state and reward from environment</div><div class=\"line\">        s1,r,d,_ = env.step(a)</div><div class=\"line\">        #Update Q-Table with new knowledge</div><div class=\"line\">        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])</div><div class=\"line\">        rAll += r</div><div class=\"line\">        s = s1</div><div class=\"line\">        if d == True:</div><div class=\"line\">            break</div><div class=\"line\">    #jList.append(j)</div><div class=\"line\">    rList.append(rAll)</div></pre></td></tr></table></figure></p>\n<h3 id=\"DQN-Deep-Q-Network\"><a href=\"#DQN-Deep-Q-Network\" class=\"headerlink\" title=\"DQN(Deep Q Network)\"></a>DQN(Deep Q Network)</h3><p>比如利用CNN来做observation来表达state，即是DQN，后续再更新。</p>"},{"title":"feature-yy-list","date":"2018-09-30T09:47:14.000Z","_content":"","source":"_posts/feature-yy-list.md","raw":"---\ntitle: feature-yy-list\ndate: 2018-09-30 17:47:14\ntags:\n---\n","slug":"feature-yy-list","published":1,"updated":"2018-09-30T09:47:14.587Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gak000rtcinhxt0786l","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"cnn","date":"2018-03-14T05:51:41.000Z","_content":"\n#### Detect vertical/horizontal edges\n\nCNN kernal below can be used for detecting vertical edges\n\n1 | 0 | -1\n--- | --- | ---\n1 | 0 | -1\n1 | 0 | -1\n\n#### stride & padding\n\nparams | values\n--- | ---\ninput volume size | W\nstride | S\npadding | P\nfilter size | F\noutput volume size | (W−F+2P)/S+1(W−F+2P)/S+1\n\n#### advantages\n* parameter sharing\n* sparsity of connections\n\n#### xavier_initializer\n* uniform distribution: x = sqrt(6. / (in + out)); [-x, x]\n* normal distribution: x = sqrt(2. / (in + out)); [-x, x]\n\n#### Convolution Demo\n* W: width = 5\n* H: Height = 5\n* D: Depth = 3\n* K: number of filters = 2\n* F: Filter size = 3\n* S: Stride = 2\n* P: Padding = 1\n\n {% asset_img \"cnn002.png\" [cnn002.png] %}\n\n<<<<<<< HEAD\n#### Pooling Demo\n\n {% asset_img \"cnn003.png\" [cnn003.png] %}\n\n#### LeNet\n* http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\n\n {% asset_img \"lenet.png\" [lenet.png] %}\n\n* CONV\n* POOL\n* FC\n\n#### AlexNet\n* http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n\n {% asset_img \"lenet.png\" [lenet.png] %}\n\n * Bigger\n * Deeper\n=======\n#### LeNet\n\n#### AlexNet\n>>>>>>> 1be2ebd4ec9a7cfdbd6c6e6d48a5059b0bc4ede5\n\n#### VGGNet\n\n#### GoogleNet\n\n#### ResNet\n\n#### REF\n* http://cs231n.github.io/convolutional-networks/\n","source":"_posts/cnn.md","raw":"---\ntitle: cnn\ndate: 2018-03-14 13:51:41\ntags:\n---\n\n#### Detect vertical/horizontal edges\n\nCNN kernal below can be used for detecting vertical edges\n\n1 | 0 | -1\n--- | --- | ---\n1 | 0 | -1\n1 | 0 | -1\n\n#### stride & padding\n\nparams | values\n--- | ---\ninput volume size | W\nstride | S\npadding | P\nfilter size | F\noutput volume size | (W−F+2P)/S+1(W−F+2P)/S+1\n\n#### advantages\n* parameter sharing\n* sparsity of connections\n\n#### xavier_initializer\n* uniform distribution: x = sqrt(6. / (in + out)); [-x, x]\n* normal distribution: x = sqrt(2. / (in + out)); [-x, x]\n\n#### Convolution Demo\n* W: width = 5\n* H: Height = 5\n* D: Depth = 3\n* K: number of filters = 2\n* F: Filter size = 3\n* S: Stride = 2\n* P: Padding = 1\n\n {% asset_img \"cnn002.png\" [cnn002.png] %}\n\n<<<<<<< HEAD\n#### Pooling Demo\n\n {% asset_img \"cnn003.png\" [cnn003.png] %}\n\n#### LeNet\n* http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\n\n {% asset_img \"lenet.png\" [lenet.png] %}\n\n* CONV\n* POOL\n* FC\n\n#### AlexNet\n* http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n\n {% asset_img \"lenet.png\" [lenet.png] %}\n\n * Bigger\n * Deeper\n=======\n#### LeNet\n\n#### AlexNet\n>>>>>>> 1be2ebd4ec9a7cfdbd6c6e6d48a5059b0bc4ede5\n\n#### VGGNet\n\n#### GoogleNet\n\n#### ResNet\n\n#### REF\n* http://cs231n.github.io/convolutional-networks/\n","slug":"cnn","published":1,"updated":"2018-03-19T02:52:49.297Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gam000stcing5cwgvfq","content":"<h4 id=\"Detect-vertical-horizontal-edges\"><a href=\"#Detect-vertical-horizontal-edges\" class=\"headerlink\" title=\"Detect vertical/horizontal edges\"></a>Detect vertical/horizontal edges</h4><p>CNN kernal below can be used for detecting vertical edges</p>\n<table>\n<thead>\n<tr>\n<th>1</th>\n<th>0</th>\n<th>-1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0</td>\n<td>-1</td>\n</tr>\n<tr>\n<td>1</td>\n<td>0</td>\n<td>-1</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"stride-amp-padding\"><a href=\"#stride-amp-padding\" class=\"headerlink\" title=\"stride &amp; padding\"></a>stride &amp; padding</h4><table>\n<thead>\n<tr>\n<th>params</th>\n<th>values</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>input volume size</td>\n<td>W</td>\n</tr>\n<tr>\n<td>stride</td>\n<td>S</td>\n</tr>\n<tr>\n<td>padding</td>\n<td>P</td>\n</tr>\n<tr>\n<td>filter size</td>\n<td>F</td>\n</tr>\n<tr>\n<td>output volume size</td>\n<td>(W−F+2P)/S+1(W−F+2P)/S+1</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"advantages\"><a href=\"#advantages\" class=\"headerlink\" title=\"advantages\"></a>advantages</h4><ul>\n<li>parameter sharing</li>\n<li>sparsity of connections</li>\n</ul>\n<h4 id=\"xavier-initializer\"><a href=\"#xavier-initializer\" class=\"headerlink\" title=\"xavier_initializer\"></a>xavier_initializer</h4><ul>\n<li>uniform distribution: x = sqrt(6. / (in + out)); [-x, x]</li>\n<li>normal distribution: x = sqrt(2. / (in + out)); [-x, x]</li>\n</ul>\n<h4 id=\"Convolution-Demo\"><a href=\"#Convolution-Demo\" class=\"headerlink\" title=\"Convolution Demo\"></a>Convolution Demo</h4><ul>\n<li>W: width = 5</li>\n<li>H: Height = 5</li>\n<li>D: Depth = 3</li>\n<li>K: number of filters = 2</li>\n<li>F: Filter size = 3</li>\n<li>S: Stride = 2</li>\n<li><p>P: Padding = 1</p>\n<img src=\"/2018/03/14/cnn/cnn002.png\" alt=\"[cnn002.png]\" title=\"[cnn002.png]\">\n</li>\n</ul>\n<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>\n<h4 id=\"Pooling-Demo\"><a href=\"#Pooling-Demo\" class=\"headerlink\" title=\"Pooling Demo\"></a>Pooling Demo</h4> <img src=\"/2018/03/14/cnn/cnn003.png\" alt=\"[cnn003.png]\" title=\"[cnn003.png]\">\n<h4 id=\"LeNet\"><a href=\"#LeNet\" class=\"headerlink\" title=\"LeNet\"></a>LeNet</h4><ul>\n<li><p><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\" target=\"_blank\" rel=\"external\">http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</a></p>\n<img src=\"/2018/03/14/cnn/lenet.png\" alt=\"[lenet.png]\" title=\"[lenet.png]\">\n</li>\n<li><p>CONV</p>\n</li>\n<li>POOL</li>\n<li>FC</li>\n</ul>\n<h4 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4><ul>\n<li><p><a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"external\">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>\n<img src=\"/2018/03/14/cnn/lenet.png\" alt=\"[lenet.png]\" title=\"[lenet.png]\">\n<ul>\n<li>Bigger</li>\n<li><h1 id=\"Deeper\"><a href=\"#Deeper\" class=\"headerlink\" title=\"Deeper\"></a>Deeper</h1><h4 id=\"LeNet-1\"><a href=\"#LeNet-1\" class=\"headerlink\" title=\"LeNet\"></a>LeNet</h4></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"AlexNet-1\"><a href=\"#AlexNet-1\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4><blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>1be2ebd4ec9a7cfdbd6c6e6d48a5059b0bc4ede5</p>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n<h4 id=\"VGGNet\"><a href=\"#VGGNet\" class=\"headerlink\" title=\"VGGNet\"></a>VGGNet</h4><h4 id=\"GoogleNet\"><a href=\"#GoogleNet\" class=\"headerlink\" title=\"GoogleNet\"></a>GoogleNet</h4><h4 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h4><h4 id=\"REF\"><a href=\"#REF\" class=\"headerlink\" title=\"REF\"></a>REF</h4><ul>\n<li><a href=\"http://cs231n.github.io/convolutional-networks/\" target=\"_blank\" rel=\"external\">http://cs231n.github.io/convolutional-networks/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"Detect-vertical-horizontal-edges\"><a href=\"#Detect-vertical-horizontal-edges\" class=\"headerlink\" title=\"Detect vertical/horizontal edges\"></a>Detect vertical/horizontal edges</h4><p>CNN kernal below can be used for detecting vertical edges</p>\n<table>\n<thead>\n<tr>\n<th>1</th>\n<th>0</th>\n<th>-1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0</td>\n<td>-1</td>\n</tr>\n<tr>\n<td>1</td>\n<td>0</td>\n<td>-1</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"stride-amp-padding\"><a href=\"#stride-amp-padding\" class=\"headerlink\" title=\"stride &amp; padding\"></a>stride &amp; padding</h4><table>\n<thead>\n<tr>\n<th>params</th>\n<th>values</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>input volume size</td>\n<td>W</td>\n</tr>\n<tr>\n<td>stride</td>\n<td>S</td>\n</tr>\n<tr>\n<td>padding</td>\n<td>P</td>\n</tr>\n<tr>\n<td>filter size</td>\n<td>F</td>\n</tr>\n<tr>\n<td>output volume size</td>\n<td>(W−F+2P)/S+1(W−F+2P)/S+1</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"advantages\"><a href=\"#advantages\" class=\"headerlink\" title=\"advantages\"></a>advantages</h4><ul>\n<li>parameter sharing</li>\n<li>sparsity of connections</li>\n</ul>\n<h4 id=\"xavier-initializer\"><a href=\"#xavier-initializer\" class=\"headerlink\" title=\"xavier_initializer\"></a>xavier_initializer</h4><ul>\n<li>uniform distribution: x = sqrt(6. / (in + out)); [-x, x]</li>\n<li>normal distribution: x = sqrt(2. / (in + out)); [-x, x]</li>\n</ul>\n<h4 id=\"Convolution-Demo\"><a href=\"#Convolution-Demo\" class=\"headerlink\" title=\"Convolution Demo\"></a>Convolution Demo</h4><ul>\n<li>W: width = 5</li>\n<li>H: Height = 5</li>\n<li>D: Depth = 3</li>\n<li>K: number of filters = 2</li>\n<li>F: Filter size = 3</li>\n<li>S: Stride = 2</li>\n<li><p>P: Padding = 1</p>\n<img src=\"/2018/03/14/cnn/cnn002.png\" alt=\"[cnn002.png]\" title=\"[cnn002.png]\">\n</li>\n</ul>\n<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>\n<h4 id=\"Pooling-Demo\"><a href=\"#Pooling-Demo\" class=\"headerlink\" title=\"Pooling Demo\"></a>Pooling Demo</h4> <img src=\"/2018/03/14/cnn/cnn003.png\" alt=\"[cnn003.png]\" title=\"[cnn003.png]\">\n<h4 id=\"LeNet\"><a href=\"#LeNet\" class=\"headerlink\" title=\"LeNet\"></a>LeNet</h4><ul>\n<li><p><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\" target=\"_blank\" rel=\"external\">http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</a></p>\n<img src=\"/2018/03/14/cnn/lenet.png\" alt=\"[lenet.png]\" title=\"[lenet.png]\">\n</li>\n<li><p>CONV</p>\n</li>\n<li>POOL</li>\n<li>FC</li>\n</ul>\n<h4 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4><ul>\n<li><p><a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"external\">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>\n<img src=\"/2018/03/14/cnn/lenet.png\" alt=\"[lenet.png]\" title=\"[lenet.png]\">\n<ul>\n<li>Bigger</li>\n<li><h1 id=\"Deeper\"><a href=\"#Deeper\" class=\"headerlink\" title=\"Deeper\"></a>Deeper</h1><h4 id=\"LeNet-1\"><a href=\"#LeNet-1\" class=\"headerlink\" title=\"LeNet\"></a>LeNet</h4></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"AlexNet-1\"><a href=\"#AlexNet-1\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4><blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>1be2ebd4ec9a7cfdbd6c6e6d48a5059b0bc4ede5</p>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n<h4 id=\"VGGNet\"><a href=\"#VGGNet\" class=\"headerlink\" title=\"VGGNet\"></a>VGGNet</h4><h4 id=\"GoogleNet\"><a href=\"#GoogleNet\" class=\"headerlink\" title=\"GoogleNet\"></a>GoogleNet</h4><h4 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h4><h4 id=\"REF\"><a href=\"#REF\" class=\"headerlink\" title=\"REF\"></a>REF</h4><ul>\n<li><a href=\"http://cs231n.github.io/convolutional-networks/\" target=\"_blank\" rel=\"external\">http://cs231n.github.io/convolutional-networks/</a></li>\n</ul>\n"},{"title":"聊聊特征工程","date":"2017-10-02T09:52:35.000Z","_content":"\n### 特征工程\n\n在机器学习项目中，往往受到关注的是高大上的机器学习模型，特征工程很少有人问津，可能唯一提到的便是浮夸的一句“我们的模型使用了百万级别的特征”。然而特征工程对于线上效果的贡献，往往远远大于模型，所以一个健全的特征工程方法论非常的重要。\n\n<!-- more -->\n\n### 最有效的特征是什么\n\n在pCTR项目中，决定是否点击的最重要的因素，是Item本身和User本身，即ItemID和UserID特征。\n* ItemID: 推荐“王者荣耀”和“全民超神”，大家都会选择“王者荣耀”，因为你的朋友都在这款游戏里，所以个人偏好远远小于物品属性的影响。\n* UserID: 用户需求明确（就是来找一款MOBA手游），点击率自然高；用户就是来逛逛，刷刷页面，那点击率自然低。\n* 其他的特征: 时间地点场景年龄性别星级类型等对模型的影响是次要的。\n\n### ID特征太多怎么办\n如果ID数量太多不便处理，可以简单用统计CTR特征来代替，纯粹ID特征等价于纯粹CTR特征，从理论推导和代码实践上皆可证明。\n\n#### 实验经验\n自己曾经做了一次实验，单CTR特征模型AUC=0.7+，其他所有特征（单单排除CTR特征）模型AUC=0.6+，所有特征一起建模AUC=0.8+。\n\n#### CTR特征的坏处\n但是用CTR特征的坏处是，交叉的时候相对于ID特征，会丢失信息。\n\n### 最好的非个性化模型\n对于某个UserX来说，ItemID特征（CTR特征）起到主导作用，其他特征只是辅助，那么最好的非个性化模型即是CTR排序模型（或只有ItemID的特征的模型）。\n\n### Item个数和提升天花板\n当Item数量越少，Item之间差别越大的时候，个性化的能够提升的空间越小（比如某业务只有40+个特征，个性化模型只能比CTR热门提升6%左右）；当Item数量非常庞大的时候（如淘宝），或者用户偏好非常分散的时候（如书籍，各个年龄性别行业都不同），推荐才有大的发挥空间。\n\n### 连续特征 VS 离散特征\n在工程实践中，有2种类型的特征：连续特征和离散特征。而“百万级别特征”里往往大部分是离散特征，以App推荐为例，有User/Item ID，城市，地区，标签特征，分类特征，厂商等等，经过one-hot之后，数量急剧爆炸；而连续特征有很多是人造统计特征，比如：下载量，访问量，ltv，arpu，实时ctr等等，成本高，数量少。\n\n### 特征工程\n\n#### 人工特征工程\n##### 特征提取\n特征的提取，很大程度上是人的工作（除去一些端到端的NN方案），初期依照业务知识，自行YY出一些特征出来。以APP推荐为例，CTR特征保证高转化，下载量特征保证热门，星级特征保证质量，用户安装使用/APP类别特征保证个性化。\n从划分来看，特征可以有以下来源：\n1. 基础属性：不随时间变化的属性。如User的性别，年龄，职业，住址等；Item的自身属性（如APP的星级，公司，包大小等）\n2. 统计属性：简单统计可以得到的特征。如User的下载量，点击量，CTR值等；Item的曝光，点击，下载，ARPU，LTV，留存等。\n3. 标签转移属性：标签转移是建设画像的一种重要思路。APP画像转移到用户画像上的有：点击的类型分布，下载的类型分布等；用户画像转移到APP画像上的有：男女使用分布，性别安装分布，地域点击率分布等。\n4. 场景属性：事情发生的时间，地点，场景等，如：APP的某个页面ID，猜你喜欢的第X位等。\n5. 设备属性：手机的好坏。ROM，RAM大小等非常影响用户的游戏下载属性。\n6. 迁移属性：画像的特点就是知识迁移方便。广告业务的特征用到APP业务上，WiFi的特征用到流量业务上，非常的常见。\n7. （人工）交叉特征：比如User的三级分类画像和APP的三级分类画像，每一个相对应的特征，交叉一遍，得到的人工交叉特征。\n8. 实时特征：讲上述的特征，尤其是统计特征，实时化。获取当前热点信息。\n\n##### 特征选择（特征重要性）\n特征选择有非常多的方法，一个常见的错误是将LR的权重作为特征选择的依据。因为LR中每个Feature的量纲是不同的（比如年龄1-100，温度是-10-40，下载量是几十万），所以特征值大权重小，特征值小权重大。所以LR的权重只有参考意义，不能盲目信任。\n个人列举一些常用的选择的方法：\n1. 单特征AUC（最常用）\n2. 单特征gini index（信息增益，信息增益率）\n3. 相关系数，卡方检验\n4. L1模型自动选择\n5. RF/GBDT打印Feature Importance\n6. wrapper：1-n逐个增加特征，有用就加，无用就抛弃（同事用过，个人经验不足）\n\n##### 特征归一化\n即Z-score，minmax，log变换等，在这里不再赘述。\n需要了解的是：归一化本身并不增加模型精读，只是将特征统一量纲，加速训练。\n\n##### 特征分段\n1. 等宽：1-10,11-20,21-30等距离分。\n2. 等频：先rank，top1-100,top101-200,top201-300等频率分。\n3. 人工：0-17未成年，18-25青年，25-35工作，35-45中年，45以上...\n4. Label决定：如先分桶，通过gini index求最佳分隔点；如使用如下CTR图\n\n{% asset_img \"年龄画段.png\" [年龄画段] %}\n\n##### 特征组合\n1. one-hot特征交叉：01交叉得0, 11交叉得1\n2. one-real特征交叉：0-real交叉得0, 1-real交叉得real\n3. 强强联合：两个强特征进行交叉\n\n#### 自动化特征工程\n上述人工特征工程实在是费心费力，所以建议不使用人工特征工程，全部使用”最原始“特征交给模型来做。首先将特征分成”连续特征“和”离散特征“两种，然后将特征扔进GBDT，GBDT自动进行：\n1. 特征选择：不好的特征，根本进不去树里面。\n2. 特征分段：树的split的分支，即是分段方案。\n3. 特征组合：叶子节点路径，即使特征组合。\n强烈推荐。\n\n### 0 or missing?\n最后讨论一个小问题，libsvm中被稀疏掉的特征，表示0还是表示missing？\n答案是0，libsvm中默认没有missing。\n但是xgboost中对libsvm的处理，是按照missing来处理的，将0和missing分开的方法是：\n1. 连续特征：增加隐控制变量表达是否missing，另一个变量表示值。\n2. 离散特征：将missing枚举为一个离散值。\n\n### Learn From IJCAI\n* 基础特征\n* 简单特征：转化率，排名，占比，趋势\n* 复杂特征：query交互，用户交互，竞争特征，业务特征\n\n\n","source":"_posts/feature-engineer.md","raw":"---\ntitle: 聊聊特征工程\ndate: 2017-10-02 17:52:35\ntags:\ncategories: 机器学习\n---\n\n### 特征工程\n\n在机器学习项目中，往往受到关注的是高大上的机器学习模型，特征工程很少有人问津，可能唯一提到的便是浮夸的一句“我们的模型使用了百万级别的特征”。然而特征工程对于线上效果的贡献，往往远远大于模型，所以一个健全的特征工程方法论非常的重要。\n\n<!-- more -->\n\n### 最有效的特征是什么\n\n在pCTR项目中，决定是否点击的最重要的因素，是Item本身和User本身，即ItemID和UserID特征。\n* ItemID: 推荐“王者荣耀”和“全民超神”，大家都会选择“王者荣耀”，因为你的朋友都在这款游戏里，所以个人偏好远远小于物品属性的影响。\n* UserID: 用户需求明确（就是来找一款MOBA手游），点击率自然高；用户就是来逛逛，刷刷页面，那点击率自然低。\n* 其他的特征: 时间地点场景年龄性别星级类型等对模型的影响是次要的。\n\n### ID特征太多怎么办\n如果ID数量太多不便处理，可以简单用统计CTR特征来代替，纯粹ID特征等价于纯粹CTR特征，从理论推导和代码实践上皆可证明。\n\n#### 实验经验\n自己曾经做了一次实验，单CTR特征模型AUC=0.7+，其他所有特征（单单排除CTR特征）模型AUC=0.6+，所有特征一起建模AUC=0.8+。\n\n#### CTR特征的坏处\n但是用CTR特征的坏处是，交叉的时候相对于ID特征，会丢失信息。\n\n### 最好的非个性化模型\n对于某个UserX来说，ItemID特征（CTR特征）起到主导作用，其他特征只是辅助，那么最好的非个性化模型即是CTR排序模型（或只有ItemID的特征的模型）。\n\n### Item个数和提升天花板\n当Item数量越少，Item之间差别越大的时候，个性化的能够提升的空间越小（比如某业务只有40+个特征，个性化模型只能比CTR热门提升6%左右）；当Item数量非常庞大的时候（如淘宝），或者用户偏好非常分散的时候（如书籍，各个年龄性别行业都不同），推荐才有大的发挥空间。\n\n### 连续特征 VS 离散特征\n在工程实践中，有2种类型的特征：连续特征和离散特征。而“百万级别特征”里往往大部分是离散特征，以App推荐为例，有User/Item ID，城市，地区，标签特征，分类特征，厂商等等，经过one-hot之后，数量急剧爆炸；而连续特征有很多是人造统计特征，比如：下载量，访问量，ltv，arpu，实时ctr等等，成本高，数量少。\n\n### 特征工程\n\n#### 人工特征工程\n##### 特征提取\n特征的提取，很大程度上是人的工作（除去一些端到端的NN方案），初期依照业务知识，自行YY出一些特征出来。以APP推荐为例，CTR特征保证高转化，下载量特征保证热门，星级特征保证质量，用户安装使用/APP类别特征保证个性化。\n从划分来看，特征可以有以下来源：\n1. 基础属性：不随时间变化的属性。如User的性别，年龄，职业，住址等；Item的自身属性（如APP的星级，公司，包大小等）\n2. 统计属性：简单统计可以得到的特征。如User的下载量，点击量，CTR值等；Item的曝光，点击，下载，ARPU，LTV，留存等。\n3. 标签转移属性：标签转移是建设画像的一种重要思路。APP画像转移到用户画像上的有：点击的类型分布，下载的类型分布等；用户画像转移到APP画像上的有：男女使用分布，性别安装分布，地域点击率分布等。\n4. 场景属性：事情发生的时间，地点，场景等，如：APP的某个页面ID，猜你喜欢的第X位等。\n5. 设备属性：手机的好坏。ROM，RAM大小等非常影响用户的游戏下载属性。\n6. 迁移属性：画像的特点就是知识迁移方便。广告业务的特征用到APP业务上，WiFi的特征用到流量业务上，非常的常见。\n7. （人工）交叉特征：比如User的三级分类画像和APP的三级分类画像，每一个相对应的特征，交叉一遍，得到的人工交叉特征。\n8. 实时特征：讲上述的特征，尤其是统计特征，实时化。获取当前热点信息。\n\n##### 特征选择（特征重要性）\n特征选择有非常多的方法，一个常见的错误是将LR的权重作为特征选择的依据。因为LR中每个Feature的量纲是不同的（比如年龄1-100，温度是-10-40，下载量是几十万），所以特征值大权重小，特征值小权重大。所以LR的权重只有参考意义，不能盲目信任。\n个人列举一些常用的选择的方法：\n1. 单特征AUC（最常用）\n2. 单特征gini index（信息增益，信息增益率）\n3. 相关系数，卡方检验\n4. L1模型自动选择\n5. RF/GBDT打印Feature Importance\n6. wrapper：1-n逐个增加特征，有用就加，无用就抛弃（同事用过，个人经验不足）\n\n##### 特征归一化\n即Z-score，minmax，log变换等，在这里不再赘述。\n需要了解的是：归一化本身并不增加模型精读，只是将特征统一量纲，加速训练。\n\n##### 特征分段\n1. 等宽：1-10,11-20,21-30等距离分。\n2. 等频：先rank，top1-100,top101-200,top201-300等频率分。\n3. 人工：0-17未成年，18-25青年，25-35工作，35-45中年，45以上...\n4. Label决定：如先分桶，通过gini index求最佳分隔点；如使用如下CTR图\n\n{% asset_img \"年龄画段.png\" [年龄画段] %}\n\n##### 特征组合\n1. one-hot特征交叉：01交叉得0, 11交叉得1\n2. one-real特征交叉：0-real交叉得0, 1-real交叉得real\n3. 强强联合：两个强特征进行交叉\n\n#### 自动化特征工程\n上述人工特征工程实在是费心费力，所以建议不使用人工特征工程，全部使用”最原始“特征交给模型来做。首先将特征分成”连续特征“和”离散特征“两种，然后将特征扔进GBDT，GBDT自动进行：\n1. 特征选择：不好的特征，根本进不去树里面。\n2. 特征分段：树的split的分支，即是分段方案。\n3. 特征组合：叶子节点路径，即使特征组合。\n强烈推荐。\n\n### 0 or missing?\n最后讨论一个小问题，libsvm中被稀疏掉的特征，表示0还是表示missing？\n答案是0，libsvm中默认没有missing。\n但是xgboost中对libsvm的处理，是按照missing来处理的，将0和missing分开的方法是：\n1. 连续特征：增加隐控制变量表达是否missing，另一个变量表示值。\n2. 离散特征：将missing枚举为一个离散值。\n\n### Learn From IJCAI\n* 基础特征\n* 简单特征：转化率，排名，占比，趋势\n* 复杂特征：query交互，用户交互，竞争特征，业务特征\n\n\n","slug":"feature-engineer","published":1,"updated":"2018-08-09T06:40:57.848Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gaz000vtcinyyg5ys4i","content":"<h3 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><p>在机器学习项目中，往往受到关注的是高大上的机器学习模型，特征工程很少有人问津，可能唯一提到的便是浮夸的一句“我们的模型使用了百万级别的特征”。然而特征工程对于线上效果的贡献，往往远远大于模型，所以一个健全的特征工程方法论非常的重要。</p>\n<a id=\"more\"></a>\n<h3 id=\"最有效的特征是什么\"><a href=\"#最有效的特征是什么\" class=\"headerlink\" title=\"最有效的特征是什么\"></a>最有效的特征是什么</h3><p>在pCTR项目中，决定是否点击的最重要的因素，是Item本身和User本身，即ItemID和UserID特征。</p>\n<ul>\n<li>ItemID: 推荐“王者荣耀”和“全民超神”，大家都会选择“王者荣耀”，因为你的朋友都在这款游戏里，所以个人偏好远远小于物品属性的影响。</li>\n<li>UserID: 用户需求明确（就是来找一款MOBA手游），点击率自然高；用户就是来逛逛，刷刷页面，那点击率自然低。</li>\n<li>其他的特征: 时间地点场景年龄性别星级类型等对模型的影响是次要的。</li>\n</ul>\n<h3 id=\"ID特征太多怎么办\"><a href=\"#ID特征太多怎么办\" class=\"headerlink\" title=\"ID特征太多怎么办\"></a>ID特征太多怎么办</h3><p>如果ID数量太多不便处理，可以简单用统计CTR特征来代替，纯粹ID特征等价于纯粹CTR特征，从理论推导和代码实践上皆可证明。</p>\n<h4 id=\"实验经验\"><a href=\"#实验经验\" class=\"headerlink\" title=\"实验经验\"></a>实验经验</h4><p>自己曾经做了一次实验，单CTR特征模型AUC=0.7+，其他所有特征（单单排除CTR特征）模型AUC=0.6+，所有特征一起建模AUC=0.8+。</p>\n<h4 id=\"CTR特征的坏处\"><a href=\"#CTR特征的坏处\" class=\"headerlink\" title=\"CTR特征的坏处\"></a>CTR特征的坏处</h4><p>但是用CTR特征的坏处是，交叉的时候相对于ID特征，会丢失信息。</p>\n<h3 id=\"最好的非个性化模型\"><a href=\"#最好的非个性化模型\" class=\"headerlink\" title=\"最好的非个性化模型\"></a>最好的非个性化模型</h3><p>对于某个UserX来说，ItemID特征（CTR特征）起到主导作用，其他特征只是辅助，那么最好的非个性化模型即是CTR排序模型（或只有ItemID的特征的模型）。</p>\n<h3 id=\"Item个数和提升天花板\"><a href=\"#Item个数和提升天花板\" class=\"headerlink\" title=\"Item个数和提升天花板\"></a>Item个数和提升天花板</h3><p>当Item数量越少，Item之间差别越大的时候，个性化的能够提升的空间越小（比如某业务只有40+个特征，个性化模型只能比CTR热门提升6%左右）；当Item数量非常庞大的时候（如淘宝），或者用户偏好非常分散的时候（如书籍，各个年龄性别行业都不同），推荐才有大的发挥空间。</p>\n<h3 id=\"连续特征-VS-离散特征\"><a href=\"#连续特征-VS-离散特征\" class=\"headerlink\" title=\"连续特征 VS 离散特征\"></a>连续特征 VS 离散特征</h3><p>在工程实践中，有2种类型的特征：连续特征和离散特征。而“百万级别特征”里往往大部分是离散特征，以App推荐为例，有User/Item ID，城市，地区，标签特征，分类特征，厂商等等，经过one-hot之后，数量急剧爆炸；而连续特征有很多是人造统计特征，比如：下载量，访问量，ltv，arpu，实时ctr等等，成本高，数量少。</p>\n<h3 id=\"特征工程-1\"><a href=\"#特征工程-1\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><h4 id=\"人工特征工程\"><a href=\"#人工特征工程\" class=\"headerlink\" title=\"人工特征工程\"></a>人工特征工程</h4><h5 id=\"特征提取\"><a href=\"#特征提取\" class=\"headerlink\" title=\"特征提取\"></a>特征提取</h5><p>特征的提取，很大程度上是人的工作（除去一些端到端的NN方案），初期依照业务知识，自行YY出一些特征出来。以APP推荐为例，CTR特征保证高转化，下载量特征保证热门，星级特征保证质量，用户安装使用/APP类别特征保证个性化。<br>从划分来看，特征可以有以下来源：</p>\n<ol>\n<li>基础属性：不随时间变化的属性。如User的性别，年龄，职业，住址等；Item的自身属性（如APP的星级，公司，包大小等）</li>\n<li>统计属性：简单统计可以得到的特征。如User的下载量，点击量，CTR值等；Item的曝光，点击，下载，ARPU，LTV，留存等。</li>\n<li>标签转移属性：标签转移是建设画像的一种重要思路。APP画像转移到用户画像上的有：点击的类型分布，下载的类型分布等；用户画像转移到APP画像上的有：男女使用分布，性别安装分布，地域点击率分布等。</li>\n<li>场景属性：事情发生的时间，地点，场景等，如：APP的某个页面ID，猜你喜欢的第X位等。</li>\n<li>设备属性：手机的好坏。ROM，RAM大小等非常影响用户的游戏下载属性。</li>\n<li>迁移属性：画像的特点就是知识迁移方便。广告业务的特征用到APP业务上，WiFi的特征用到流量业务上，非常的常见。</li>\n<li>（人工）交叉特征：比如User的三级分类画像和APP的三级分类画像，每一个相对应的特征，交叉一遍，得到的人工交叉特征。</li>\n<li>实时特征：讲上述的特征，尤其是统计特征，实时化。获取当前热点信息。</li>\n</ol>\n<h5 id=\"特征选择（特征重要性）\"><a href=\"#特征选择（特征重要性）\" class=\"headerlink\" title=\"特征选择（特征重要性）\"></a>特征选择（特征重要性）</h5><p>特征选择有非常多的方法，一个常见的错误是将LR的权重作为特征选择的依据。因为LR中每个Feature的量纲是不同的（比如年龄1-100，温度是-10-40，下载量是几十万），所以特征值大权重小，特征值小权重大。所以LR的权重只有参考意义，不能盲目信任。<br>个人列举一些常用的选择的方法：</p>\n<ol>\n<li>单特征AUC（最常用）</li>\n<li>单特征gini index（信息增益，信息增益率）</li>\n<li>相关系数，卡方检验</li>\n<li>L1模型自动选择</li>\n<li>RF/GBDT打印Feature Importance</li>\n<li>wrapper：1-n逐个增加特征，有用就加，无用就抛弃（同事用过，个人经验不足）</li>\n</ol>\n<h5 id=\"特征归一化\"><a href=\"#特征归一化\" class=\"headerlink\" title=\"特征归一化\"></a>特征归一化</h5><p>即Z-score，minmax，log变换等，在这里不再赘述。<br>需要了解的是：归一化本身并不增加模型精读，只是将特征统一量纲，加速训练。</p>\n<h5 id=\"特征分段\"><a href=\"#特征分段\" class=\"headerlink\" title=\"特征分段\"></a>特征分段</h5><ol>\n<li>等宽：1-10,11-20,21-30等距离分。</li>\n<li>等频：先rank，top1-100,top101-200,top201-300等频率分。</li>\n<li>人工：0-17未成年，18-25青年，25-35工作，35-45中年，45以上…</li>\n<li>Label决定：如先分桶，通过gini index求最佳分隔点；如使用如下CTR图</li>\n</ol>\n\n<h5 id=\"特征组合\"><a href=\"#特征组合\" class=\"headerlink\" title=\"特征组合\"></a>特征组合</h5><ol>\n<li>one-hot特征交叉：01交叉得0, 11交叉得1</li>\n<li>one-real特征交叉：0-real交叉得0, 1-real交叉得real</li>\n<li>强强联合：两个强特征进行交叉</li>\n</ol>\n<h4 id=\"自动化特征工程\"><a href=\"#自动化特征工程\" class=\"headerlink\" title=\"自动化特征工程\"></a>自动化特征工程</h4><p>上述人工特征工程实在是费心费力，所以建议不使用人工特征工程，全部使用”最原始“特征交给模型来做。首先将特征分成”连续特征“和”离散特征“两种，然后将特征扔进GBDT，GBDT自动进行：</p>\n<ol>\n<li>特征选择：不好的特征，根本进不去树里面。</li>\n<li>特征分段：树的split的分支，即是分段方案。</li>\n<li>特征组合：叶子节点路径，即使特征组合。<br>强烈推荐。</li>\n</ol>\n<h3 id=\"0-or-missing\"><a href=\"#0-or-missing\" class=\"headerlink\" title=\"0 or missing?\"></a>0 or missing?</h3><p>最后讨论一个小问题，libsvm中被稀疏掉的特征，表示0还是表示missing？<br>答案是0，libsvm中默认没有missing。<br>但是xgboost中对libsvm的处理，是按照missing来处理的，将0和missing分开的方法是：</p>\n<ol>\n<li>连续特征：增加隐控制变量表达是否missing，另一个变量表示值。</li>\n<li>离散特征：将missing枚举为一个离散值。</li>\n</ol>\n<h3 id=\"Learn-From-IJCAI\"><a href=\"#Learn-From-IJCAI\" class=\"headerlink\" title=\"Learn From IJCAI\"></a>Learn From IJCAI</h3><ul>\n<li>基础特征</li>\n<li>简单特征：转化率，排名，占比，趋势</li>\n<li>复杂特征：query交互，用户交互，竞争特征，业务特征</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h3 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><p>在机器学习项目中，往往受到关注的是高大上的机器学习模型，特征工程很少有人问津，可能唯一提到的便是浮夸的一句“我们的模型使用了百万级别的特征”。然而特征工程对于线上效果的贡献，往往远远大于模型，所以一个健全的特征工程方法论非常的重要。</p>","more":"<h3 id=\"最有效的特征是什么\"><a href=\"#最有效的特征是什么\" class=\"headerlink\" title=\"最有效的特征是什么\"></a>最有效的特征是什么</h3><p>在pCTR项目中，决定是否点击的最重要的因素，是Item本身和User本身，即ItemID和UserID特征。</p>\n<ul>\n<li>ItemID: 推荐“王者荣耀”和“全民超神”，大家都会选择“王者荣耀”，因为你的朋友都在这款游戏里，所以个人偏好远远小于物品属性的影响。</li>\n<li>UserID: 用户需求明确（就是来找一款MOBA手游），点击率自然高；用户就是来逛逛，刷刷页面，那点击率自然低。</li>\n<li>其他的特征: 时间地点场景年龄性别星级类型等对模型的影响是次要的。</li>\n</ul>\n<h3 id=\"ID特征太多怎么办\"><a href=\"#ID特征太多怎么办\" class=\"headerlink\" title=\"ID特征太多怎么办\"></a>ID特征太多怎么办</h3><p>如果ID数量太多不便处理，可以简单用统计CTR特征来代替，纯粹ID特征等价于纯粹CTR特征，从理论推导和代码实践上皆可证明。</p>\n<h4 id=\"实验经验\"><a href=\"#实验经验\" class=\"headerlink\" title=\"实验经验\"></a>实验经验</h4><p>自己曾经做了一次实验，单CTR特征模型AUC=0.7+，其他所有特征（单单排除CTR特征）模型AUC=0.6+，所有特征一起建模AUC=0.8+。</p>\n<h4 id=\"CTR特征的坏处\"><a href=\"#CTR特征的坏处\" class=\"headerlink\" title=\"CTR特征的坏处\"></a>CTR特征的坏处</h4><p>但是用CTR特征的坏处是，交叉的时候相对于ID特征，会丢失信息。</p>\n<h3 id=\"最好的非个性化模型\"><a href=\"#最好的非个性化模型\" class=\"headerlink\" title=\"最好的非个性化模型\"></a>最好的非个性化模型</h3><p>对于某个UserX来说，ItemID特征（CTR特征）起到主导作用，其他特征只是辅助，那么最好的非个性化模型即是CTR排序模型（或只有ItemID的特征的模型）。</p>\n<h3 id=\"Item个数和提升天花板\"><a href=\"#Item个数和提升天花板\" class=\"headerlink\" title=\"Item个数和提升天花板\"></a>Item个数和提升天花板</h3><p>当Item数量越少，Item之间差别越大的时候，个性化的能够提升的空间越小（比如某业务只有40+个特征，个性化模型只能比CTR热门提升6%左右）；当Item数量非常庞大的时候（如淘宝），或者用户偏好非常分散的时候（如书籍，各个年龄性别行业都不同），推荐才有大的发挥空间。</p>\n<h3 id=\"连续特征-VS-离散特征\"><a href=\"#连续特征-VS-离散特征\" class=\"headerlink\" title=\"连续特征 VS 离散特征\"></a>连续特征 VS 离散特征</h3><p>在工程实践中，有2种类型的特征：连续特征和离散特征。而“百万级别特征”里往往大部分是离散特征，以App推荐为例，有User/Item ID，城市，地区，标签特征，分类特征，厂商等等，经过one-hot之后，数量急剧爆炸；而连续特征有很多是人造统计特征，比如：下载量，访问量，ltv，arpu，实时ctr等等，成本高，数量少。</p>\n<h3 id=\"特征工程-1\"><a href=\"#特征工程-1\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><h4 id=\"人工特征工程\"><a href=\"#人工特征工程\" class=\"headerlink\" title=\"人工特征工程\"></a>人工特征工程</h4><h5 id=\"特征提取\"><a href=\"#特征提取\" class=\"headerlink\" title=\"特征提取\"></a>特征提取</h5><p>特征的提取，很大程度上是人的工作（除去一些端到端的NN方案），初期依照业务知识，自行YY出一些特征出来。以APP推荐为例，CTR特征保证高转化，下载量特征保证热门，星级特征保证质量，用户安装使用/APP类别特征保证个性化。<br>从划分来看，特征可以有以下来源：</p>\n<ol>\n<li>基础属性：不随时间变化的属性。如User的性别，年龄，职业，住址等；Item的自身属性（如APP的星级，公司，包大小等）</li>\n<li>统计属性：简单统计可以得到的特征。如User的下载量，点击量，CTR值等；Item的曝光，点击，下载，ARPU，LTV，留存等。</li>\n<li>标签转移属性：标签转移是建设画像的一种重要思路。APP画像转移到用户画像上的有：点击的类型分布，下载的类型分布等；用户画像转移到APP画像上的有：男女使用分布，性别安装分布，地域点击率分布等。</li>\n<li>场景属性：事情发生的时间，地点，场景等，如：APP的某个页面ID，猜你喜欢的第X位等。</li>\n<li>设备属性：手机的好坏。ROM，RAM大小等非常影响用户的游戏下载属性。</li>\n<li>迁移属性：画像的特点就是知识迁移方便。广告业务的特征用到APP业务上，WiFi的特征用到流量业务上，非常的常见。</li>\n<li>（人工）交叉特征：比如User的三级分类画像和APP的三级分类画像，每一个相对应的特征，交叉一遍，得到的人工交叉特征。</li>\n<li>实时特征：讲上述的特征，尤其是统计特征，实时化。获取当前热点信息。</li>\n</ol>\n<h5 id=\"特征选择（特征重要性）\"><a href=\"#特征选择（特征重要性）\" class=\"headerlink\" title=\"特征选择（特征重要性）\"></a>特征选择（特征重要性）</h5><p>特征选择有非常多的方法，一个常见的错误是将LR的权重作为特征选择的依据。因为LR中每个Feature的量纲是不同的（比如年龄1-100，温度是-10-40，下载量是几十万），所以特征值大权重小，特征值小权重大。所以LR的权重只有参考意义，不能盲目信任。<br>个人列举一些常用的选择的方法：</p>\n<ol>\n<li>单特征AUC（最常用）</li>\n<li>单特征gini index（信息增益，信息增益率）</li>\n<li>相关系数，卡方检验</li>\n<li>L1模型自动选择</li>\n<li>RF/GBDT打印Feature Importance</li>\n<li>wrapper：1-n逐个增加特征，有用就加，无用就抛弃（同事用过，个人经验不足）</li>\n</ol>\n<h5 id=\"特征归一化\"><a href=\"#特征归一化\" class=\"headerlink\" title=\"特征归一化\"></a>特征归一化</h5><p>即Z-score，minmax，log变换等，在这里不再赘述。<br>需要了解的是：归一化本身并不增加模型精读，只是将特征统一量纲，加速训练。</p>\n<h5 id=\"特征分段\"><a href=\"#特征分段\" class=\"headerlink\" title=\"特征分段\"></a>特征分段</h5><ol>\n<li>等宽：1-10,11-20,21-30等距离分。</li>\n<li>等频：先rank，top1-100,top101-200,top201-300等频率分。</li>\n<li>人工：0-17未成年，18-25青年，25-35工作，35-45中年，45以上…</li>\n<li>Label决定：如先分桶，通过gini index求最佳分隔点；如使用如下CTR图</li>\n</ol>\n\n<h5 id=\"特征组合\"><a href=\"#特征组合\" class=\"headerlink\" title=\"特征组合\"></a>特征组合</h5><ol>\n<li>one-hot特征交叉：01交叉得0, 11交叉得1</li>\n<li>one-real特征交叉：0-real交叉得0, 1-real交叉得real</li>\n<li>强强联合：两个强特征进行交叉</li>\n</ol>\n<h4 id=\"自动化特征工程\"><a href=\"#自动化特征工程\" class=\"headerlink\" title=\"自动化特征工程\"></a>自动化特征工程</h4><p>上述人工特征工程实在是费心费力，所以建议不使用人工特征工程，全部使用”最原始“特征交给模型来做。首先将特征分成”连续特征“和”离散特征“两种，然后将特征扔进GBDT，GBDT自动进行：</p>\n<ol>\n<li>特征选择：不好的特征，根本进不去树里面。</li>\n<li>特征分段：树的split的分支，即是分段方案。</li>\n<li>特征组合：叶子节点路径，即使特征组合。<br>强烈推荐。</li>\n</ol>\n<h3 id=\"0-or-missing\"><a href=\"#0-or-missing\" class=\"headerlink\" title=\"0 or missing?\"></a>0 or missing?</h3><p>最后讨论一个小问题，libsvm中被稀疏掉的特征，表示0还是表示missing？<br>答案是0，libsvm中默认没有missing。<br>但是xgboost中对libsvm的处理，是按照missing来处理的，将0和missing分开的方法是：</p>\n<ol>\n<li>连续特征：增加隐控制变量表达是否missing，另一个变量表示值。</li>\n<li>离散特征：将missing枚举为一个离散值。</li>\n</ol>\n<h3 id=\"Learn-From-IJCAI\"><a href=\"#Learn-From-IJCAI\" class=\"headerlink\" title=\"Learn From IJCAI\"></a>Learn From IJCAI</h3><ul>\n<li>基础特征</li>\n<li>简单特征：转化率，排名，占比，趋势</li>\n<li>复杂特征：query交互，用户交互，竞争特征，业务特征</li>\n</ul>"},{"title":"hive-param-tune","date":"2018-06-21T07:06:21.000Z","_content":"遇到一个hive调优的case，运行时长从原来的2h缩短到了10min，现总结下经验。\n\n* hive运行时长太长的问题\n先看源码：\n```\ninsert overwrite table dm_music_prd.t_7d_imusic_iting_user2item_reclist_redis partition(ds='$DATE', item_type=1, scene='daily_rec', algo='arcf_001')\nselect concat('arcf_001_iting_', user_id) as user_id,  --step 5\n  concat_ws(',', collect_list(item_id)) as reclist\nfrom\n(\n  select user_id, --step4\n    item_id,\n    rank2\n  from\n  (\n      select user_id, item_id, row_number() over (partition by user_id order by rank asc) as rank2  --step 3\n      from\n      (\n          select user_id, item_id, rank   --step 1\n          from dm_music_prd.t_7d_imusic_iting_user2item_reclist\n          where ds='$DATE' and item_type=1 and scene='daily_rec' and rank<=$REC_MAX\n\n          union all\n\n          select /*+mapjoin(b)*/ user_id, item_id, rank   --step 2\n          from\n          (\n            select distinct user_id from dm_music_prd.t_7d_imusic_iting_user2item_reclist_default  where ds='$DATE' and item_type=1 and scene='daily_rec'\n          ) a\n          join\n          (\n            select item_id, rank+1000 as rank from dm_music_prd.t_7d_imusic_iting_random_hot where ds='$DATE' and item_type=1 and rank<=$REC_MAX\n          ) b\n      ) tt\n  ) f\n  where rank2<=$REC_MAX\n  distribute by user_id\n  sort by rank2 asc\n) t\nwhere rank2<=$REC_MAX\ngroup by user_id;\n```\n上述代码中标注了一些step，总结下经验：\n1. step1：3kw用户*300item，只有选择操作，速度很快\n2. step2：3kw用户和300item做笛卡尔积，用mapjoin把300item放到内存，速度很快\n3. step3：3kw用户，每个用户内的600个item排序，3kw*log(600)的复杂度，耗时巨大\n4. step4：按照user_id分桶，桶内进行排序，复杂度是reduce个数*log（reduce内数据量），耗时不确定\n5. step5：按照user_id做group by的操作，速度很快\n\n* 代码分析\n核心的step3，按照上述代码运行的reduce个数是：157个\n因为reduce是根据数据量来确定个数的，因此我们需要通过改变参数，增大reduce的个数\n\n* 改进方案\n```\nset hive.map.aggr=false;\nset mapreduce.input.fileinputformat.split.minsize=8000000;\nset mapreduce.input.fileinputformat.split.minsize.per.node=8000000;\nset mapreduce.input.fileinputformat.split.minsize.per.rack=8000000;\nset mapreduce.input.fileinputformat.split.maxsize=16000000;\nset hive.exec.reducers.bytes.per.reducer=67108864;\n```\n通过设置hive.exec.reducers.bytes.per.reducer为一个较小的值（上述代码是67M，默认是256M），来增多reduce个数，增加并行度。\n最终reduce个数为600+个，10min跑完step3.\n","source":"_posts/hive-param-tune.md","raw":"---\ntitle: hive-param-tune\ndate: 2018-06-21 15:06:21\ntags:\n---\n遇到一个hive调优的case，运行时长从原来的2h缩短到了10min，现总结下经验。\n\n* hive运行时长太长的问题\n先看源码：\n```\ninsert overwrite table dm_music_prd.t_7d_imusic_iting_user2item_reclist_redis partition(ds='$DATE', item_type=1, scene='daily_rec', algo='arcf_001')\nselect concat('arcf_001_iting_', user_id) as user_id,  --step 5\n  concat_ws(',', collect_list(item_id)) as reclist\nfrom\n(\n  select user_id, --step4\n    item_id,\n    rank2\n  from\n  (\n      select user_id, item_id, row_number() over (partition by user_id order by rank asc) as rank2  --step 3\n      from\n      (\n          select user_id, item_id, rank   --step 1\n          from dm_music_prd.t_7d_imusic_iting_user2item_reclist\n          where ds='$DATE' and item_type=1 and scene='daily_rec' and rank<=$REC_MAX\n\n          union all\n\n          select /*+mapjoin(b)*/ user_id, item_id, rank   --step 2\n          from\n          (\n            select distinct user_id from dm_music_prd.t_7d_imusic_iting_user2item_reclist_default  where ds='$DATE' and item_type=1 and scene='daily_rec'\n          ) a\n          join\n          (\n            select item_id, rank+1000 as rank from dm_music_prd.t_7d_imusic_iting_random_hot where ds='$DATE' and item_type=1 and rank<=$REC_MAX\n          ) b\n      ) tt\n  ) f\n  where rank2<=$REC_MAX\n  distribute by user_id\n  sort by rank2 asc\n) t\nwhere rank2<=$REC_MAX\ngroup by user_id;\n```\n上述代码中标注了一些step，总结下经验：\n1. step1：3kw用户*300item，只有选择操作，速度很快\n2. step2：3kw用户和300item做笛卡尔积，用mapjoin把300item放到内存，速度很快\n3. step3：3kw用户，每个用户内的600个item排序，3kw*log(600)的复杂度，耗时巨大\n4. step4：按照user_id分桶，桶内进行排序，复杂度是reduce个数*log（reduce内数据量），耗时不确定\n5. step5：按照user_id做group by的操作，速度很快\n\n* 代码分析\n核心的step3，按照上述代码运行的reduce个数是：157个\n因为reduce是根据数据量来确定个数的，因此我们需要通过改变参数，增大reduce的个数\n\n* 改进方案\n```\nset hive.map.aggr=false;\nset mapreduce.input.fileinputformat.split.minsize=8000000;\nset mapreduce.input.fileinputformat.split.minsize.per.node=8000000;\nset mapreduce.input.fileinputformat.split.minsize.per.rack=8000000;\nset mapreduce.input.fileinputformat.split.maxsize=16000000;\nset hive.exec.reducers.bytes.per.reducer=67108864;\n```\n通过设置hive.exec.reducers.bytes.per.reducer为一个较小的值（上述代码是67M，默认是256M），来增多reduce个数，增加并行度。\n最终reduce个数为600+个，10min跑完step3.\n","slug":"hive-param-tune","published":1,"updated":"2018-06-21T07:49:02.105Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gb0000wtcincsxijy4h","content":"<p>遇到一个hive调优的case，运行时长从原来的2h缩短到了10min，现总结下经验。</p>\n<ul>\n<li>hive运行时长太长的问题<br>先看源码：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div></pre></td><td class=\"code\"><pre><div class=\"line\">insert overwrite table dm_music_prd.t_7d_imusic_iting_user2item_reclist_redis partition(ds=&apos;$DATE&apos;, item_type=1, scene=&apos;daily_rec&apos;, algo=&apos;arcf_001&apos;)</div><div class=\"line\">select concat(&apos;arcf_001_iting_&apos;, user_id) as user_id,  --step 5</div><div class=\"line\">  concat_ws(&apos;,&apos;, collect_list(item_id)) as reclist</div><div class=\"line\">from</div><div class=\"line\">(</div><div class=\"line\">  select user_id, --step4</div><div class=\"line\">    item_id,</div><div class=\"line\">    rank2</div><div class=\"line\">  from</div><div class=\"line\">  (</div><div class=\"line\">      select user_id, item_id, row_number() over (partition by user_id order by rank asc) as rank2  --step 3</div><div class=\"line\">      from</div><div class=\"line\">      (</div><div class=\"line\">          select user_id, item_id, rank   --step 1</div><div class=\"line\">          from dm_music_prd.t_7d_imusic_iting_user2item_reclist</div><div class=\"line\">          where ds=&apos;$DATE&apos; and item_type=1 and scene=&apos;daily_rec&apos; and rank&lt;=$REC_MAX</div><div class=\"line\"></div><div class=\"line\">          union all</div><div class=\"line\"></div><div class=\"line\">          select /*+mapjoin(b)*/ user_id, item_id, rank   --step 2</div><div class=\"line\">          from</div><div class=\"line\">          (</div><div class=\"line\">            select distinct user_id from dm_music_prd.t_7d_imusic_iting_user2item_reclist_default  where ds=&apos;$DATE&apos; and item_type=1 and scene=&apos;daily_rec&apos;</div><div class=\"line\">          ) a</div><div class=\"line\">          join</div><div class=\"line\">          (</div><div class=\"line\">            select item_id, rank+1000 as rank from dm_music_prd.t_7d_imusic_iting_random_hot where ds=&apos;$DATE&apos; and item_type=1 and rank&lt;=$REC_MAX</div><div class=\"line\">          ) b</div><div class=\"line\">      ) tt</div><div class=\"line\">  ) f</div><div class=\"line\">  where rank2&lt;=$REC_MAX</div><div class=\"line\">  distribute by user_id</div><div class=\"line\">  sort by rank2 asc</div><div class=\"line\">) t</div><div class=\"line\">where rank2&lt;=$REC_MAX</div><div class=\"line\">group by user_id;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>上述代码中标注了一些step，总结下经验：</p>\n<ol>\n<li>step1：3kw用户*300item，只有选择操作，速度很快</li>\n<li>step2：3kw用户和300item做笛卡尔积，用mapjoin把300item放到内存，速度很快</li>\n<li>step3：3kw用户，每个用户内的600个item排序，3kw*log(600)的复杂度，耗时巨大</li>\n<li>step4：按照user_id分桶，桶内进行排序，复杂度是reduce个数*log（reduce内数据量），耗时不确定</li>\n<li>step5：按照user_id做group by的操作，速度很快</li>\n</ol>\n<ul>\n<li><p>代码分析<br>核心的step3，按照上述代码运行的reduce个数是：157个<br>因为reduce是根据数据量来确定个数的，因此我们需要通过改变参数，增大reduce的个数</p>\n</li>\n<li><p>改进方案</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">set hive.map.aggr=false;</div><div class=\"line\">set mapreduce.input.fileinputformat.split.minsize=8000000;</div><div class=\"line\">set mapreduce.input.fileinputformat.split.minsize.per.node=8000000;</div><div class=\"line\">set mapreduce.input.fileinputformat.split.minsize.per.rack=8000000;</div><div class=\"line\">set mapreduce.input.fileinputformat.split.maxsize=16000000;</div><div class=\"line\">set hive.exec.reducers.bytes.per.reducer=67108864;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>通过设置hive.exec.reducers.bytes.per.reducer为一个较小的值（上述代码是67M，默认是256M），来增多reduce个数，增加并行度。<br>最终reduce个数为600+个，10min跑完step3.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>遇到一个hive调优的case，运行时长从原来的2h缩短到了10min，现总结下经验。</p>\n<ul>\n<li>hive运行时长太长的问题<br>先看源码：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div></pre></td><td class=\"code\"><pre><div class=\"line\">insert overwrite table dm_music_prd.t_7d_imusic_iting_user2item_reclist_redis partition(ds=&apos;$DATE&apos;, item_type=1, scene=&apos;daily_rec&apos;, algo=&apos;arcf_001&apos;)</div><div class=\"line\">select concat(&apos;arcf_001_iting_&apos;, user_id) as user_id,  --step 5</div><div class=\"line\">  concat_ws(&apos;,&apos;, collect_list(item_id)) as reclist</div><div class=\"line\">from</div><div class=\"line\">(</div><div class=\"line\">  select user_id, --step4</div><div class=\"line\">    item_id,</div><div class=\"line\">    rank2</div><div class=\"line\">  from</div><div class=\"line\">  (</div><div class=\"line\">      select user_id, item_id, row_number() over (partition by user_id order by rank asc) as rank2  --step 3</div><div class=\"line\">      from</div><div class=\"line\">      (</div><div class=\"line\">          select user_id, item_id, rank   --step 1</div><div class=\"line\">          from dm_music_prd.t_7d_imusic_iting_user2item_reclist</div><div class=\"line\">          where ds=&apos;$DATE&apos; and item_type=1 and scene=&apos;daily_rec&apos; and rank&lt;=$REC_MAX</div><div class=\"line\"></div><div class=\"line\">          union all</div><div class=\"line\"></div><div class=\"line\">          select /*+mapjoin(b)*/ user_id, item_id, rank   --step 2</div><div class=\"line\">          from</div><div class=\"line\">          (</div><div class=\"line\">            select distinct user_id from dm_music_prd.t_7d_imusic_iting_user2item_reclist_default  where ds=&apos;$DATE&apos; and item_type=1 and scene=&apos;daily_rec&apos;</div><div class=\"line\">          ) a</div><div class=\"line\">          join</div><div class=\"line\">          (</div><div class=\"line\">            select item_id, rank+1000 as rank from dm_music_prd.t_7d_imusic_iting_random_hot where ds=&apos;$DATE&apos; and item_type=1 and rank&lt;=$REC_MAX</div><div class=\"line\">          ) b</div><div class=\"line\">      ) tt</div><div class=\"line\">  ) f</div><div class=\"line\">  where rank2&lt;=$REC_MAX</div><div class=\"line\">  distribute by user_id</div><div class=\"line\">  sort by rank2 asc</div><div class=\"line\">) t</div><div class=\"line\">where rank2&lt;=$REC_MAX</div><div class=\"line\">group by user_id;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>上述代码中标注了一些step，总结下经验：</p>\n<ol>\n<li>step1：3kw用户*300item，只有选择操作，速度很快</li>\n<li>step2：3kw用户和300item做笛卡尔积，用mapjoin把300item放到内存，速度很快</li>\n<li>step3：3kw用户，每个用户内的600个item排序，3kw*log(600)的复杂度，耗时巨大</li>\n<li>step4：按照user_id分桶，桶内进行排序，复杂度是reduce个数*log（reduce内数据量），耗时不确定</li>\n<li>step5：按照user_id做group by的操作，速度很快</li>\n</ol>\n<ul>\n<li><p>代码分析<br>核心的step3，按照上述代码运行的reduce个数是：157个<br>因为reduce是根据数据量来确定个数的，因此我们需要通过改变参数，增大reduce的个数</p>\n</li>\n<li><p>改进方案</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">set hive.map.aggr=false;</div><div class=\"line\">set mapreduce.input.fileinputformat.split.minsize=8000000;</div><div class=\"line\">set mapreduce.input.fileinputformat.split.minsize.per.node=8000000;</div><div class=\"line\">set mapreduce.input.fileinputformat.split.minsize.per.rack=8000000;</div><div class=\"line\">set mapreduce.input.fileinputformat.split.maxsize=16000000;</div><div class=\"line\">set hive.exec.reducers.bytes.per.reducer=67108864;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>通过设置hive.exec.reducers.bytes.per.reducer为一个较小的值（上述代码是67M，默认是256M），来增多reduce个数，增加并行度。<br>最终reduce个数为600+个，10min跑完step3.</p>\n"},{"title":"hivemall","date":"2018-03-19T02:57:35.000Z","_content":"\n##### Hivemall是什么\n* apache主页：http://hivemall.incubator.apache.org/index.html\n* 包含: regression, classification, recommendation, anomaly detection, k-nearest neighbor, and feature engineering\n* 包含ML: Soft Confidence Weighted, Adaptive Regularization of Weight Vectors, Factorization Machines, and AdaDelta\n* Support: Hive, Spark, Pig\n* Architecture\n{% asset_img \"hivemall_1.PNG\" [hivemall_architecture] %}\n\n\n##### Hive on Spark VS Hive on MR\n* configure: set hive.execution.engine=spark;\n* tutorial: https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started\n* 测试:\n```\nselect max(a) as a, max(b) as b\nfrom\n(\nselect 1 as a, 2 as b\nunion all\nselect 2 as a, 3 as b\nunion all\nselect 3 as a, 4 as b\n) t\n```\n\n##### Hive on Spark with Hivemall\n* hive check version: hive --version\n* download Hivemall: https://hivemall.incubator.apache.org/download.html\n* UDF define: https://github.com/apache/incubator-hivemall/blob/master/resources/ddl/define-all.hive\n* add jar\n```\nadd jaradd jar hdfs://footstone/data/project/dataming/contentrec/hivemall/hivemall-all-0.5.0-incubating.jar;\nsource define-all.hive;\n```\n* setting queue: set mapred.job.queue.name=root.dataming.dev;\n* set hive nonstrict mode: set hive.mapred.mode=nonstrict;\n\n##### AUC calc\n* Single node\n```\nwith data as (\n  select 0.5 as prob, 0 as label\n  union all\n  select 0.3 as prob, 1 as label\n  union all\n  select 0.2 as prob, 0 as label\n  union all\n  select 0.8 as prob, 1 as label\n  union all\n  select 0.7 as prob, 1 as label\n)\nselect\n  auc(prob, label) as auc\nfrom (\n  select prob, label\n  from data\n  ORDER BY prob DESC\n) t;\n\n```\n\n* Parallel approximate\n```\nwith data as (\n  select 0.5 as prob, 0 as label\n  union all\n  select 0.3 as prob, 1 as label\n  union all\n  select 0.2 as prob, 0 as label\n  union all\n  select 0.8 as prob, 1 as label\n  union all\n  select 0.7 as prob, 1 as label\n)\nselect\n  auc(prob, label) as auc\nfrom (\n  select prob, label\n  from data\n  DISTRIBUTE BY floor(prob / 0.2)\n  SORT BY prob DESC\n) t;\n```\n\n##### Compile from source\n* git clone https://github.com/sampsonguo/incubator-hivemall.git\n* mvn clean package -Dmaven.test.skip=true\n* create temporary function beta_dist_sample as 'com.sigmoidguo.math.BetaUDF';\n\n##### Hive & Json\nhive解析json:\n* json_split: brickhouse split array\n* get_json_object: hive udf\n","source":"_posts/hivemall.md","raw":"---\ntitle: hivemall\ndate: 2018-03-19 10:57:35\ntags:\n---\n\n##### Hivemall是什么\n* apache主页：http://hivemall.incubator.apache.org/index.html\n* 包含: regression, classification, recommendation, anomaly detection, k-nearest neighbor, and feature engineering\n* 包含ML: Soft Confidence Weighted, Adaptive Regularization of Weight Vectors, Factorization Machines, and AdaDelta\n* Support: Hive, Spark, Pig\n* Architecture\n{% asset_img \"hivemall_1.PNG\" [hivemall_architecture] %}\n\n\n##### Hive on Spark VS Hive on MR\n* configure: set hive.execution.engine=spark;\n* tutorial: https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started\n* 测试:\n```\nselect max(a) as a, max(b) as b\nfrom\n(\nselect 1 as a, 2 as b\nunion all\nselect 2 as a, 3 as b\nunion all\nselect 3 as a, 4 as b\n) t\n```\n\n##### Hive on Spark with Hivemall\n* hive check version: hive --version\n* download Hivemall: https://hivemall.incubator.apache.org/download.html\n* UDF define: https://github.com/apache/incubator-hivemall/blob/master/resources/ddl/define-all.hive\n* add jar\n```\nadd jaradd jar hdfs://footstone/data/project/dataming/contentrec/hivemall/hivemall-all-0.5.0-incubating.jar;\nsource define-all.hive;\n```\n* setting queue: set mapred.job.queue.name=root.dataming.dev;\n* set hive nonstrict mode: set hive.mapred.mode=nonstrict;\n\n##### AUC calc\n* Single node\n```\nwith data as (\n  select 0.5 as prob, 0 as label\n  union all\n  select 0.3 as prob, 1 as label\n  union all\n  select 0.2 as prob, 0 as label\n  union all\n  select 0.8 as prob, 1 as label\n  union all\n  select 0.7 as prob, 1 as label\n)\nselect\n  auc(prob, label) as auc\nfrom (\n  select prob, label\n  from data\n  ORDER BY prob DESC\n) t;\n\n```\n\n* Parallel approximate\n```\nwith data as (\n  select 0.5 as prob, 0 as label\n  union all\n  select 0.3 as prob, 1 as label\n  union all\n  select 0.2 as prob, 0 as label\n  union all\n  select 0.8 as prob, 1 as label\n  union all\n  select 0.7 as prob, 1 as label\n)\nselect\n  auc(prob, label) as auc\nfrom (\n  select prob, label\n  from data\n  DISTRIBUTE BY floor(prob / 0.2)\n  SORT BY prob DESC\n) t;\n```\n\n##### Compile from source\n* git clone https://github.com/sampsonguo/incubator-hivemall.git\n* mvn clean package -Dmaven.test.skip=true\n* create temporary function beta_dist_sample as 'com.sigmoidguo.math.BetaUDF';\n\n##### Hive & Json\nhive解析json:\n* json_split: brickhouse split array\n* get_json_object: hive udf\n","slug":"hivemall","published":1,"updated":"2018-03-22T07:36:18.054Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gb4000xtcinuadzbh8v","content":"<h5 id=\"Hivemall是什么\"><a href=\"#Hivemall是什么\" class=\"headerlink\" title=\"Hivemall是什么\"></a>Hivemall是什么</h5><ul>\n<li>apache主页：<a href=\"http://hivemall.incubator.apache.org/index.html\" target=\"_blank\" rel=\"external\">http://hivemall.incubator.apache.org/index.html</a></li>\n<li>包含: regression, classification, recommendation, anomaly detection, k-nearest neighbor, and feature engineering</li>\n<li>包含ML: Soft Confidence Weighted, Adaptive Regularization of Weight Vectors, Factorization Machines, and AdaDelta</li>\n<li>Support: Hive, Spark, Pig</li>\n<li>Architecture<img src=\"/2018/03/19/hivemall/hivemall_1.PNG\" alt=\"[hivemall_architecture]\" title=\"[hivemall_architecture]\">\n</li>\n</ul>\n<h5 id=\"Hive-on-Spark-VS-Hive-on-MR\"><a href=\"#Hive-on-Spark-VS-Hive-on-MR\" class=\"headerlink\" title=\"Hive on Spark VS Hive on MR\"></a>Hive on Spark VS Hive on MR</h5><ul>\n<li>configure: set hive.execution.engine=spark;</li>\n<li>tutorial: <a href=\"https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started\" target=\"_blank\" rel=\"external\">https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started</a></li>\n<li>测试:<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">select max(a) as a, max(b) as b</div><div class=\"line\">from</div><div class=\"line\">(</div><div class=\"line\">select 1 as a, 2 as b</div><div class=\"line\">union all</div><div class=\"line\">select 2 as a, 3 as b</div><div class=\"line\">union all</div><div class=\"line\">select 3 as a, 4 as b</div><div class=\"line\">) t</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h5 id=\"Hive-on-Spark-with-Hivemall\"><a href=\"#Hive-on-Spark-with-Hivemall\" class=\"headerlink\" title=\"Hive on Spark with Hivemall\"></a>Hive on Spark with Hivemall</h5><ul>\n<li>hive check version: hive –version</li>\n<li>download Hivemall: <a href=\"https://hivemall.incubator.apache.org/download.html\" target=\"_blank\" rel=\"external\">https://hivemall.incubator.apache.org/download.html</a></li>\n<li>UDF define: <a href=\"https://github.com/apache/incubator-hivemall/blob/master/resources/ddl/define-all.hive\" target=\"_blank\" rel=\"external\">https://github.com/apache/incubator-hivemall/blob/master/resources/ddl/define-all.hive</a></li>\n<li><p>add jar</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">add jaradd jar hdfs://footstone/data/project/dataming/contentrec/hivemall/hivemall-all-0.5.0-incubating.jar;</div><div class=\"line\">source define-all.hive;</div></pre></td></tr></table></figure>\n</li>\n<li><p>setting queue: set mapred.job.queue.name=root.dataming.dev;</p>\n</li>\n<li>set hive nonstrict mode: set hive.mapred.mode=nonstrict;</li>\n</ul>\n<h5 id=\"AUC-calc\"><a href=\"#AUC-calc\" class=\"headerlink\" title=\"AUC calc\"></a>AUC calc</h5><ul>\n<li><p>Single node</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">with data as (</div><div class=\"line\">  select 0.5 as prob, 0 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.3 as prob, 1 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.2 as prob, 0 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.8 as prob, 1 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.7 as prob, 1 as label</div><div class=\"line\">)</div><div class=\"line\">select</div><div class=\"line\">  auc(prob, label) as auc</div><div class=\"line\">from (</div><div class=\"line\">  select prob, label</div><div class=\"line\">  from data</div><div class=\"line\">  ORDER BY prob DESC</div><div class=\"line\">) t;</div></pre></td></tr></table></figure>\n</li>\n<li><p>Parallel approximate</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">with data as (</div><div class=\"line\">  select 0.5 as prob, 0 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.3 as prob, 1 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.2 as prob, 0 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.8 as prob, 1 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.7 as prob, 1 as label</div><div class=\"line\">)</div><div class=\"line\">select</div><div class=\"line\">  auc(prob, label) as auc</div><div class=\"line\">from (</div><div class=\"line\">  select prob, label</div><div class=\"line\">  from data</div><div class=\"line\">  DISTRIBUTE BY floor(prob / 0.2)</div><div class=\"line\">  SORT BY prob DESC</div><div class=\"line\">) t;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h5 id=\"Compile-from-source\"><a href=\"#Compile-from-source\" class=\"headerlink\" title=\"Compile from source\"></a>Compile from source</h5><ul>\n<li>git clone <a href=\"https://github.com/sampsonguo/incubator-hivemall.git\" target=\"_blank\" rel=\"external\">https://github.com/sampsonguo/incubator-hivemall.git</a></li>\n<li>mvn clean package -Dmaven.test.skip=true</li>\n<li>create temporary function beta_dist_sample as ‘com.sigmoidguo.math.BetaUDF’;</li>\n</ul>\n<h5 id=\"Hive-amp-Json\"><a href=\"#Hive-amp-Json\" class=\"headerlink\" title=\"Hive &amp; Json\"></a>Hive &amp; Json</h5><p>hive解析json:</p>\n<ul>\n<li>json_split: brickhouse split array</li>\n<li>get_json_object: hive udf</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h5 id=\"Hivemall是什么\"><a href=\"#Hivemall是什么\" class=\"headerlink\" title=\"Hivemall是什么\"></a>Hivemall是什么</h5><ul>\n<li>apache主页：<a href=\"http://hivemall.incubator.apache.org/index.html\" target=\"_blank\" rel=\"external\">http://hivemall.incubator.apache.org/index.html</a></li>\n<li>包含: regression, classification, recommendation, anomaly detection, k-nearest neighbor, and feature engineering</li>\n<li>包含ML: Soft Confidence Weighted, Adaptive Regularization of Weight Vectors, Factorization Machines, and AdaDelta</li>\n<li>Support: Hive, Spark, Pig</li>\n<li>Architecture<img src=\"/2018/03/19/hivemall/hivemall_1.PNG\" alt=\"[hivemall_architecture]\" title=\"[hivemall_architecture]\">\n</li>\n</ul>\n<h5 id=\"Hive-on-Spark-VS-Hive-on-MR\"><a href=\"#Hive-on-Spark-VS-Hive-on-MR\" class=\"headerlink\" title=\"Hive on Spark VS Hive on MR\"></a>Hive on Spark VS Hive on MR</h5><ul>\n<li>configure: set hive.execution.engine=spark;</li>\n<li>tutorial: <a href=\"https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started\" target=\"_blank\" rel=\"external\">https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started</a></li>\n<li>测试:<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">select max(a) as a, max(b) as b</div><div class=\"line\">from</div><div class=\"line\">(</div><div class=\"line\">select 1 as a, 2 as b</div><div class=\"line\">union all</div><div class=\"line\">select 2 as a, 3 as b</div><div class=\"line\">union all</div><div class=\"line\">select 3 as a, 4 as b</div><div class=\"line\">) t</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h5 id=\"Hive-on-Spark-with-Hivemall\"><a href=\"#Hive-on-Spark-with-Hivemall\" class=\"headerlink\" title=\"Hive on Spark with Hivemall\"></a>Hive on Spark with Hivemall</h5><ul>\n<li>hive check version: hive –version</li>\n<li>download Hivemall: <a href=\"https://hivemall.incubator.apache.org/download.html\" target=\"_blank\" rel=\"external\">https://hivemall.incubator.apache.org/download.html</a></li>\n<li>UDF define: <a href=\"https://github.com/apache/incubator-hivemall/blob/master/resources/ddl/define-all.hive\" target=\"_blank\" rel=\"external\">https://github.com/apache/incubator-hivemall/blob/master/resources/ddl/define-all.hive</a></li>\n<li><p>add jar</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">add jaradd jar hdfs://footstone/data/project/dataming/contentrec/hivemall/hivemall-all-0.5.0-incubating.jar;</div><div class=\"line\">source define-all.hive;</div></pre></td></tr></table></figure>\n</li>\n<li><p>setting queue: set mapred.job.queue.name=root.dataming.dev;</p>\n</li>\n<li>set hive nonstrict mode: set hive.mapred.mode=nonstrict;</li>\n</ul>\n<h5 id=\"AUC-calc\"><a href=\"#AUC-calc\" class=\"headerlink\" title=\"AUC calc\"></a>AUC calc</h5><ul>\n<li><p>Single node</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">with data as (</div><div class=\"line\">  select 0.5 as prob, 0 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.3 as prob, 1 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.2 as prob, 0 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.8 as prob, 1 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.7 as prob, 1 as label</div><div class=\"line\">)</div><div class=\"line\">select</div><div class=\"line\">  auc(prob, label) as auc</div><div class=\"line\">from (</div><div class=\"line\">  select prob, label</div><div class=\"line\">  from data</div><div class=\"line\">  ORDER BY prob DESC</div><div class=\"line\">) t;</div></pre></td></tr></table></figure>\n</li>\n<li><p>Parallel approximate</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">with data as (</div><div class=\"line\">  select 0.5 as prob, 0 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.3 as prob, 1 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.2 as prob, 0 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.8 as prob, 1 as label</div><div class=\"line\">  union all</div><div class=\"line\">  select 0.7 as prob, 1 as label</div><div class=\"line\">)</div><div class=\"line\">select</div><div class=\"line\">  auc(prob, label) as auc</div><div class=\"line\">from (</div><div class=\"line\">  select prob, label</div><div class=\"line\">  from data</div><div class=\"line\">  DISTRIBUTE BY floor(prob / 0.2)</div><div class=\"line\">  SORT BY prob DESC</div><div class=\"line\">) t;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h5 id=\"Compile-from-source\"><a href=\"#Compile-from-source\" class=\"headerlink\" title=\"Compile from source\"></a>Compile from source</h5><ul>\n<li>git clone <a href=\"https://github.com/sampsonguo/incubator-hivemall.git\" target=\"_blank\" rel=\"external\">https://github.com/sampsonguo/incubator-hivemall.git</a></li>\n<li>mvn clean package -Dmaven.test.skip=true</li>\n<li>create temporary function beta_dist_sample as ‘com.sigmoidguo.math.BetaUDF’;</li>\n</ul>\n<h5 id=\"Hive-amp-Json\"><a href=\"#Hive-amp-Json\" class=\"headerlink\" title=\"Hive &amp; Json\"></a>Hive &amp; Json</h5><p>hive解析json:</p>\n<ul>\n<li>json_split: brickhouse split array</li>\n<li>get_json_object: hive udf</li>\n</ul>\n"},{"title":"interview_questions","date":"2018-08-05T08:19:00.000Z","_content":"\n\n近期需要进行校招生内推面试，记录一下面试要点。\n\n### 流程\n内推面试时长30min，流程上是：\n自我介绍（5min） -> 项目介绍+基础知识测试（10min） -> 思考能力测试（15min）\n\n### 自我介绍\n自我介绍环节，重点了解候选人的特点，有以下几种：\n* 学霸：名校毕业，专业排名靠前，重点考察基础知识和学习能力\n* 名企实习：在BAT等有过实习经历，重点考察实习项目和思考\n* 热衷比赛：kaggle比赛，ACM比赛等有拿奖或排名靠前，重点考察对应比赛的经验并相应出题\n\n### 项目介绍\n* 适当打断，问为什么这么做，问实现的细节，看应变。\n\n### 基础知识测试\n在候选人介绍项目的时候，询问项目细节，适当的提出问题：\n* 机器学习\n  * 基础知识1：过拟合和欠拟合是什么？对应的模型如何解决？\n    * 过拟合：训练误差小，预测误差大\n    * 欠拟合：训练和预测误差都很大\n\n| 过拟合 | 欠拟合 |\n| ------------- | ------------- |\n| 增加训练数据量 | -  |\n| 做特征筛选 | 收集更多特征 |\n| 用更简单模型 | 用复杂模型 |\n| 增大模型正则 | 减少模型正则 |\n| DT/RF/GBDT 减小树深度 | DT/RF/GBDT 增大树深度 |\n| NN减少深/宽度，early stop，增加drop out | NN增加深/宽度，增大epoch |\n\n  * 基础知识2：特征共线性是什么？如何解决？\n    * 特征共线性是变量之间存在相关性，导致模型不稳定，泛化误差大。\n    * 1. wrapper贪心添加特征，做特征筛选  2.增加正则\n  * 基础知识3：lasso regression和ridge regression的区别是什么？\n  * 模型评估1： precision和accuracy的区别是什么？\n    * precision = TP/(TP+FP)\n    * accuracy = (TP+TN)/(TP+TN+FP+FN)\n  * 模型评估2： F1 和 AUC的区别是什么？\n    * F1 是一个准召权衡的一个点，AUC是roc曲线下面积，F1上阈值移动过程中AUC上的一个点。\n  * 模型评估3： AUC高但是logloss太大，是为什么？反过来，又是为什么？\n    * AUC高但logloss大：序准值不准，可能是采样导致的，进行后处理校准。\n    * AUC低但是logloss小：值准序不准，模型学到了平均值，欠拟合。\n  * 树模型：比较下RF和GBDT\n    * bagging vs boosting, 树越多越不过拟合 vs 越过拟合，训练快预测慢 vs 训练慢预测快，并行 vs 串行\n  * 模型融合：模型融合有哪几种方法？\n    * bagging（RF），boosting（GBDT），stacking\n  * FM：FM的基本原理是什么？如何做embedding？\n* 深度学习\n  * 介绍下梯度消失和梯度爆炸的概念，产生原因和如何解决？\n    * 梯度消失：sigmoid函数在链式求导的时候，梯度是不可能超过0.25的，同时w也比较小，因此层次越深梯度越来越小。解决方案：relu, batch normalization\n    * 梯度爆炸：在网络比较深，权重初始值大的情况下，如果梯度大于1，链式求导很容易爆炸。解决方案：正则，lstm\n\n### 基础知识\n根据特点，选择基础知识题目。\n熟悉数据库 -> Hive题目\n数学好 -> 概率题\n编程好 -> ACM题\n\n* Hive（SQL）\n有一张表t_action\n字段: user_id, item_id, action(1=click, 2=buy), timestamp\n要求：求购买次数最多的top10用户最早的购买时间，按照购买次数排序\n```\nselect f2.user_id, f2.timestamp\nfrom\n(\n  select user_id, row_number() over (order by buy_cnt desc) as rnk\n  from\n  (\n    select user_id, count(1) as buy_cnt\n    from t_action\n    where action = 2\n    group by user_id\n  ) t\n  where rnk <= 10\n) f1\njoin\n(\n  select user_id, min(timestamp) as timestamp\n  from t_action\n  where action = 2\n  group by user_id\n) f2 on f1.user_id = f2.user_id\norder by f1.rnk asc\n```\n  * 拓展：如果用户量巨大，order by全局排序时间过久怎么办？\n  * 答案：分多个map来选出各map内的top10，在reduce里再汇总总排名top10.\n* 概率题目1：token反拿问题\n  * 题目：某员工使用token登陆系统（配token图），不小心把token拿反了，但是输入里面的数字竟然成功登陆，问此情况的概率是多少？\n  * 数字对应：1-1,2-2,5-5,6-9,8-8,9-6,0-0\n    总情况：10^6\n    正反相等情况：7^3\n    正确答案：7^3/10^6\n* 概率题目2：最少晋级分数问题\n  * 题目：有8支球队打循环赛，打C（3,2）场比赛，赢3分，平1分，负0分，4支球队可以晋级，问最少多少分可以保证一定出线？\n  * 5支球队得分相同，3支球队没有得分，此种情况下是未晋级的最大分值；\n    5支球队内部每支球队都2胜2负，此种情况得分大于4场全平，即（2×3>4）；\n    未晋级的最大分值是3×3+ 2×3=15分，因此晋级的最少的分是16分。\n* 概率题目3：三门问题\n* 概率题目4：有偏好生育问题\n* ACM题目1：二分查找\n* ACM题目2：两颗树合并\n* ACM题目3：最长公共子序列 vs 最长公共子串\n\n\n","source":"_posts/interview-questions.md","raw":"---\ntitle: interview_questions\ndate: 2018-08-05 16:19:00\ntags:\n---\n\n\n近期需要进行校招生内推面试，记录一下面试要点。\n\n### 流程\n内推面试时长30min，流程上是：\n自我介绍（5min） -> 项目介绍+基础知识测试（10min） -> 思考能力测试（15min）\n\n### 自我介绍\n自我介绍环节，重点了解候选人的特点，有以下几种：\n* 学霸：名校毕业，专业排名靠前，重点考察基础知识和学习能力\n* 名企实习：在BAT等有过实习经历，重点考察实习项目和思考\n* 热衷比赛：kaggle比赛，ACM比赛等有拿奖或排名靠前，重点考察对应比赛的经验并相应出题\n\n### 项目介绍\n* 适当打断，问为什么这么做，问实现的细节，看应变。\n\n### 基础知识测试\n在候选人介绍项目的时候，询问项目细节，适当的提出问题：\n* 机器学习\n  * 基础知识1：过拟合和欠拟合是什么？对应的模型如何解决？\n    * 过拟合：训练误差小，预测误差大\n    * 欠拟合：训练和预测误差都很大\n\n| 过拟合 | 欠拟合 |\n| ------------- | ------------- |\n| 增加训练数据量 | -  |\n| 做特征筛选 | 收集更多特征 |\n| 用更简单模型 | 用复杂模型 |\n| 增大模型正则 | 减少模型正则 |\n| DT/RF/GBDT 减小树深度 | DT/RF/GBDT 增大树深度 |\n| NN减少深/宽度，early stop，增加drop out | NN增加深/宽度，增大epoch |\n\n  * 基础知识2：特征共线性是什么？如何解决？\n    * 特征共线性是变量之间存在相关性，导致模型不稳定，泛化误差大。\n    * 1. wrapper贪心添加特征，做特征筛选  2.增加正则\n  * 基础知识3：lasso regression和ridge regression的区别是什么？\n  * 模型评估1： precision和accuracy的区别是什么？\n    * precision = TP/(TP+FP)\n    * accuracy = (TP+TN)/(TP+TN+FP+FN)\n  * 模型评估2： F1 和 AUC的区别是什么？\n    * F1 是一个准召权衡的一个点，AUC是roc曲线下面积，F1上阈值移动过程中AUC上的一个点。\n  * 模型评估3： AUC高但是logloss太大，是为什么？反过来，又是为什么？\n    * AUC高但logloss大：序准值不准，可能是采样导致的，进行后处理校准。\n    * AUC低但是logloss小：值准序不准，模型学到了平均值，欠拟合。\n  * 树模型：比较下RF和GBDT\n    * bagging vs boosting, 树越多越不过拟合 vs 越过拟合，训练快预测慢 vs 训练慢预测快，并行 vs 串行\n  * 模型融合：模型融合有哪几种方法？\n    * bagging（RF），boosting（GBDT），stacking\n  * FM：FM的基本原理是什么？如何做embedding？\n* 深度学习\n  * 介绍下梯度消失和梯度爆炸的概念，产生原因和如何解决？\n    * 梯度消失：sigmoid函数在链式求导的时候，梯度是不可能超过0.25的，同时w也比较小，因此层次越深梯度越来越小。解决方案：relu, batch normalization\n    * 梯度爆炸：在网络比较深，权重初始值大的情况下，如果梯度大于1，链式求导很容易爆炸。解决方案：正则，lstm\n\n### 基础知识\n根据特点，选择基础知识题目。\n熟悉数据库 -> Hive题目\n数学好 -> 概率题\n编程好 -> ACM题\n\n* Hive（SQL）\n有一张表t_action\n字段: user_id, item_id, action(1=click, 2=buy), timestamp\n要求：求购买次数最多的top10用户最早的购买时间，按照购买次数排序\n```\nselect f2.user_id, f2.timestamp\nfrom\n(\n  select user_id, row_number() over (order by buy_cnt desc) as rnk\n  from\n  (\n    select user_id, count(1) as buy_cnt\n    from t_action\n    where action = 2\n    group by user_id\n  ) t\n  where rnk <= 10\n) f1\njoin\n(\n  select user_id, min(timestamp) as timestamp\n  from t_action\n  where action = 2\n  group by user_id\n) f2 on f1.user_id = f2.user_id\norder by f1.rnk asc\n```\n  * 拓展：如果用户量巨大，order by全局排序时间过久怎么办？\n  * 答案：分多个map来选出各map内的top10，在reduce里再汇总总排名top10.\n* 概率题目1：token反拿问题\n  * 题目：某员工使用token登陆系统（配token图），不小心把token拿反了，但是输入里面的数字竟然成功登陆，问此情况的概率是多少？\n  * 数字对应：1-1,2-2,5-5,6-9,8-8,9-6,0-0\n    总情况：10^6\n    正反相等情况：7^3\n    正确答案：7^3/10^6\n* 概率题目2：最少晋级分数问题\n  * 题目：有8支球队打循环赛，打C（3,2）场比赛，赢3分，平1分，负0分，4支球队可以晋级，问最少多少分可以保证一定出线？\n  * 5支球队得分相同，3支球队没有得分，此种情况下是未晋级的最大分值；\n    5支球队内部每支球队都2胜2负，此种情况得分大于4场全平，即（2×3>4）；\n    未晋级的最大分值是3×3+ 2×3=15分，因此晋级的最少的分是16分。\n* 概率题目3：三门问题\n* 概率题目4：有偏好生育问题\n* ACM题目1：二分查找\n* ACM题目2：两颗树合并\n* ACM题目3：最长公共子序列 vs 最长公共子串\n\n\n","slug":"interview-questions","published":1,"updated":"2018-08-09T02:08:58.979Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gb5000ztcin1g7udqaw","content":"<p>近期需要进行校招生内推面试，记录一下面试要点。</p>\n<h3 id=\"流程\"><a href=\"#流程\" class=\"headerlink\" title=\"流程\"></a>流程</h3><p>内推面试时长30min，流程上是：<br>自我介绍（5min） -&gt; 项目介绍+基础知识测试（10min） -&gt; 思考能力测试（15min）</p>\n<h3 id=\"自我介绍\"><a href=\"#自我介绍\" class=\"headerlink\" title=\"自我介绍\"></a>自我介绍</h3><p>自我介绍环节，重点了解候选人的特点，有以下几种：</p>\n<ul>\n<li>学霸：名校毕业，专业排名靠前，重点考察基础知识和学习能力</li>\n<li>名企实习：在BAT等有过实习经历，重点考察实习项目和思考</li>\n<li>热衷比赛：kaggle比赛，ACM比赛等有拿奖或排名靠前，重点考察对应比赛的经验并相应出题</li>\n</ul>\n<h3 id=\"项目介绍\"><a href=\"#项目介绍\" class=\"headerlink\" title=\"项目介绍\"></a>项目介绍</h3><ul>\n<li>适当打断，问为什么这么做，问实现的细节，看应变。</li>\n</ul>\n<h3 id=\"基础知识测试\"><a href=\"#基础知识测试\" class=\"headerlink\" title=\"基础知识测试\"></a>基础知识测试</h3><p>在候选人介绍项目的时候，询问项目细节，适当的提出问题：</p>\n<ul>\n<li>机器学习<ul>\n<li>基础知识1：过拟合和欠拟合是什么？对应的模型如何解决？<ul>\n<li>过拟合：训练误差小，预测误差大</li>\n<li>欠拟合：训练和预测误差都很大</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>过拟合</th>\n<th>欠拟合</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>增加训练数据量</td>\n<td>-</td>\n</tr>\n<tr>\n<td>做特征筛选</td>\n<td>收集更多特征</td>\n</tr>\n<tr>\n<td>用更简单模型</td>\n<td>用复杂模型</td>\n</tr>\n<tr>\n<td>增大模型正则</td>\n<td>减少模型正则</td>\n</tr>\n<tr>\n<td>DT/RF/GBDT 减小树深度</td>\n<td>DT/RF/GBDT 增大树深度</td>\n</tr>\n<tr>\n<td>NN减少深/宽度，early stop，增加drop out</td>\n<td>NN增加深/宽度，增大epoch</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>基础知识2：特征共线性是什么？如何解决？<ul>\n<li>特征共线性是变量之间存在相关性，导致模型不稳定，泛化误差大。</li>\n<li><ol>\n<li>wrapper贪心添加特征，做特征筛选  2.增加正则</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>基础知识3：lasso regression和ridge regression的区别是什么？</li>\n<li>模型评估1： precision和accuracy的区别是什么？<ul>\n<li>precision = TP/(TP+FP)</li>\n<li>accuracy = (TP+TN)/(TP+TN+FP+FN)</li>\n</ul>\n</li>\n<li>模型评估2： F1 和 AUC的区别是什么？<ul>\n<li>F1 是一个准召权衡的一个点，AUC是roc曲线下面积，F1上阈值移动过程中AUC上的一个点。</li>\n</ul>\n</li>\n<li>模型评估3： AUC高但是logloss太大，是为什么？反过来，又是为什么？<ul>\n<li>AUC高但logloss大：序准值不准，可能是采样导致的，进行后处理校准。</li>\n<li>AUC低但是logloss小：值准序不准，模型学到了平均值，欠拟合。</li>\n</ul>\n</li>\n<li>树模型：比较下RF和GBDT<ul>\n<li>bagging vs boosting, 树越多越不过拟合 vs 越过拟合，训练快预测慢 vs 训练慢预测快，并行 vs 串行</li>\n</ul>\n</li>\n<li>模型融合：模型融合有哪几种方法？<ul>\n<li>bagging（RF），boosting（GBDT），stacking</li>\n</ul>\n</li>\n<li>FM：FM的基本原理是什么？如何做embedding？<ul>\n<li>深度学习</li>\n</ul>\n</li>\n<li>介绍下梯度消失和梯度爆炸的概念，产生原因和如何解决？<ul>\n<li>梯度消失：sigmoid函数在链式求导的时候，梯度是不可能超过0.25的，同时w也比较小，因此层次越深梯度越来越小。解决方案：relu, batch normalization</li>\n<li>梯度爆炸：在网络比较深，权重初始值大的情况下，如果梯度大于1，链式求导很容易爆炸。解决方案：正则，lstm</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"基础知识\"><a href=\"#基础知识\" class=\"headerlink\" title=\"基础知识\"></a>基础知识</h3><p>根据特点，选择基础知识题目。<br>熟悉数据库 -&gt; Hive题目<br>数学好 -&gt; 概率题<br>编程好 -&gt; ACM题</p>\n<ul>\n<li><p>Hive（SQL）<br>有一张表t_action<br>字段: user_id, item_id, action(1=click, 2=buy), timestamp<br>要求：求购买次数最多的top10用户最早的购买时间，按照购买次数排序</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">select f2.user_id, f2.timestamp</div><div class=\"line\">from</div><div class=\"line\">(</div><div class=\"line\">  select user_id, row_number() over (order by buy_cnt desc) as rnk</div><div class=\"line\">  from</div><div class=\"line\">  (</div><div class=\"line\">    select user_id, count(1) as buy_cnt</div><div class=\"line\">    from t_action</div><div class=\"line\">    where action = 2</div><div class=\"line\">    group by user_id</div><div class=\"line\">  ) t</div><div class=\"line\">  where rnk &lt;= 10</div><div class=\"line\">) f1</div><div class=\"line\">join</div><div class=\"line\">(</div><div class=\"line\">  select user_id, min(timestamp) as timestamp</div><div class=\"line\">  from t_action</div><div class=\"line\">  where action = 2</div><div class=\"line\">  group by user_id</div><div class=\"line\">) f2 on f1.user_id = f2.user_id</div><div class=\"line\">order by f1.rnk asc</div></pre></td></tr></table></figure>\n<ul>\n<li>拓展：如果用户量巨大，order by全局排序时间过久怎么办？</li>\n<li>答案：分多个map来选出各map内的top10，在reduce里再汇总总排名top10.</li>\n</ul>\n</li>\n<li>概率题目1：token反拿问题<ul>\n<li>题目：某员工使用token登陆系统（配token图），不小心把token拿反了，但是输入里面的数字竟然成功登陆，问此情况的概率是多少？</li>\n<li>数字对应：1-1,2-2,5-5,6-9,8-8,9-6,0-0<br>总情况：10^6<br>正反相等情况：7^3<br>正确答案：7^3/10^6</li>\n</ul>\n</li>\n<li>概率题目2：最少晋级分数问题<ul>\n<li>题目：有8支球队打循环赛，打C（3,2）场比赛，赢3分，平1分，负0分，4支球队可以晋级，问最少多少分可以保证一定出线？</li>\n<li>5支球队得分相同，3支球队没有得分，此种情况下是未晋级的最大分值；<br>5支球队内部每支球队都2胜2负，此种情况得分大于4场全平，即（2×3&gt;4）；<br>未晋级的最大分值是3×3+ 2×3=15分，因此晋级的最少的分是16分。</li>\n</ul>\n</li>\n<li>概率题目3：三门问题</li>\n<li>概率题目4：有偏好生育问题</li>\n<li>ACM题目1：二分查找</li>\n<li>ACM题目2：两颗树合并</li>\n<li>ACM题目3：最长公共子序列 vs 最长公共子串</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>近期需要进行校招生内推面试，记录一下面试要点。</p>\n<h3 id=\"流程\"><a href=\"#流程\" class=\"headerlink\" title=\"流程\"></a>流程</h3><p>内推面试时长30min，流程上是：<br>自我介绍（5min） -&gt; 项目介绍+基础知识测试（10min） -&gt; 思考能力测试（15min）</p>\n<h3 id=\"自我介绍\"><a href=\"#自我介绍\" class=\"headerlink\" title=\"自我介绍\"></a>自我介绍</h3><p>自我介绍环节，重点了解候选人的特点，有以下几种：</p>\n<ul>\n<li>学霸：名校毕业，专业排名靠前，重点考察基础知识和学习能力</li>\n<li>名企实习：在BAT等有过实习经历，重点考察实习项目和思考</li>\n<li>热衷比赛：kaggle比赛，ACM比赛等有拿奖或排名靠前，重点考察对应比赛的经验并相应出题</li>\n</ul>\n<h3 id=\"项目介绍\"><a href=\"#项目介绍\" class=\"headerlink\" title=\"项目介绍\"></a>项目介绍</h3><ul>\n<li>适当打断，问为什么这么做，问实现的细节，看应变。</li>\n</ul>\n<h3 id=\"基础知识测试\"><a href=\"#基础知识测试\" class=\"headerlink\" title=\"基础知识测试\"></a>基础知识测试</h3><p>在候选人介绍项目的时候，询问项目细节，适当的提出问题：</p>\n<ul>\n<li>机器学习<ul>\n<li>基础知识1：过拟合和欠拟合是什么？对应的模型如何解决？<ul>\n<li>过拟合：训练误差小，预测误差大</li>\n<li>欠拟合：训练和预测误差都很大</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>过拟合</th>\n<th>欠拟合</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>增加训练数据量</td>\n<td>-</td>\n</tr>\n<tr>\n<td>做特征筛选</td>\n<td>收集更多特征</td>\n</tr>\n<tr>\n<td>用更简单模型</td>\n<td>用复杂模型</td>\n</tr>\n<tr>\n<td>增大模型正则</td>\n<td>减少模型正则</td>\n</tr>\n<tr>\n<td>DT/RF/GBDT 减小树深度</td>\n<td>DT/RF/GBDT 增大树深度</td>\n</tr>\n<tr>\n<td>NN减少深/宽度，early stop，增加drop out</td>\n<td>NN增加深/宽度，增大epoch</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>基础知识2：特征共线性是什么？如何解决？<ul>\n<li>特征共线性是变量之间存在相关性，导致模型不稳定，泛化误差大。</li>\n<li><ol>\n<li>wrapper贪心添加特征，做特征筛选  2.增加正则</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>基础知识3：lasso regression和ridge regression的区别是什么？</li>\n<li>模型评估1： precision和accuracy的区别是什么？<ul>\n<li>precision = TP/(TP+FP)</li>\n<li>accuracy = (TP+TN)/(TP+TN+FP+FN)</li>\n</ul>\n</li>\n<li>模型评估2： F1 和 AUC的区别是什么？<ul>\n<li>F1 是一个准召权衡的一个点，AUC是roc曲线下面积，F1上阈值移动过程中AUC上的一个点。</li>\n</ul>\n</li>\n<li>模型评估3： AUC高但是logloss太大，是为什么？反过来，又是为什么？<ul>\n<li>AUC高但logloss大：序准值不准，可能是采样导致的，进行后处理校准。</li>\n<li>AUC低但是logloss小：值准序不准，模型学到了平均值，欠拟合。</li>\n</ul>\n</li>\n<li>树模型：比较下RF和GBDT<ul>\n<li>bagging vs boosting, 树越多越不过拟合 vs 越过拟合，训练快预测慢 vs 训练慢预测快，并行 vs 串行</li>\n</ul>\n</li>\n<li>模型融合：模型融合有哪几种方法？<ul>\n<li>bagging（RF），boosting（GBDT），stacking</li>\n</ul>\n</li>\n<li>FM：FM的基本原理是什么？如何做embedding？<ul>\n<li>深度学习</li>\n</ul>\n</li>\n<li>介绍下梯度消失和梯度爆炸的概念，产生原因和如何解决？<ul>\n<li>梯度消失：sigmoid函数在链式求导的时候，梯度是不可能超过0.25的，同时w也比较小，因此层次越深梯度越来越小。解决方案：relu, batch normalization</li>\n<li>梯度爆炸：在网络比较深，权重初始值大的情况下，如果梯度大于1，链式求导很容易爆炸。解决方案：正则，lstm</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"基础知识\"><a href=\"#基础知识\" class=\"headerlink\" title=\"基础知识\"></a>基础知识</h3><p>根据特点，选择基础知识题目。<br>熟悉数据库 -&gt; Hive题目<br>数学好 -&gt; 概率题<br>编程好 -&gt; ACM题</p>\n<ul>\n<li><p>Hive（SQL）<br>有一张表t_action<br>字段: user_id, item_id, action(1=click, 2=buy), timestamp<br>要求：求购买次数最多的top10用户最早的购买时间，按照购买次数排序</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">select f2.user_id, f2.timestamp</div><div class=\"line\">from</div><div class=\"line\">(</div><div class=\"line\">  select user_id, row_number() over (order by buy_cnt desc) as rnk</div><div class=\"line\">  from</div><div class=\"line\">  (</div><div class=\"line\">    select user_id, count(1) as buy_cnt</div><div class=\"line\">    from t_action</div><div class=\"line\">    where action = 2</div><div class=\"line\">    group by user_id</div><div class=\"line\">  ) t</div><div class=\"line\">  where rnk &lt;= 10</div><div class=\"line\">) f1</div><div class=\"line\">join</div><div class=\"line\">(</div><div class=\"line\">  select user_id, min(timestamp) as timestamp</div><div class=\"line\">  from t_action</div><div class=\"line\">  where action = 2</div><div class=\"line\">  group by user_id</div><div class=\"line\">) f2 on f1.user_id = f2.user_id</div><div class=\"line\">order by f1.rnk asc</div></pre></td></tr></table></figure>\n<ul>\n<li>拓展：如果用户量巨大，order by全局排序时间过久怎么办？</li>\n<li>答案：分多个map来选出各map内的top10，在reduce里再汇总总排名top10.</li>\n</ul>\n</li>\n<li>概率题目1：token反拿问题<ul>\n<li>题目：某员工使用token登陆系统（配token图），不小心把token拿反了，但是输入里面的数字竟然成功登陆，问此情况的概率是多少？</li>\n<li>数字对应：1-1,2-2,5-5,6-9,8-8,9-6,0-0<br>总情况：10^6<br>正反相等情况：7^3<br>正确答案：7^3/10^6</li>\n</ul>\n</li>\n<li>概率题目2：最少晋级分数问题<ul>\n<li>题目：有8支球队打循环赛，打C（3,2）场比赛，赢3分，平1分，负0分，4支球队可以晋级，问最少多少分可以保证一定出线？</li>\n<li>5支球队得分相同，3支球队没有得分，此种情况下是未晋级的最大分值；<br>5支球队内部每支球队都2胜2负，此种情况得分大于4场全平，即（2×3&gt;4）；<br>未晋级的最大分值是3×3+ 2×3=15分，因此晋级的最少的分是16分。</li>\n</ul>\n</li>\n<li>概率题目3：三门问题</li>\n<li>概率题目4：有偏好生育问题</li>\n<li>ACM题目1：二分查找</li>\n<li>ACM题目2：两颗树合并</li>\n<li>ACM题目3：最长公共子序列 vs 最长公共子串</li>\n</ul>\n"},{"title":"kafka","date":"2017-12-15T02:49:48.000Z","_content":"\n* topics\n* producers\n* consumers\n* brokers: all node in the cluster is called a Kafka broker\n\n","source":"_posts/kafka.md","raw":"---\ntitle: kafka\ndate: 2017-12-15 10:49:48\ntags:\n---\n\n* topics\n* producers\n* consumers\n* brokers: all node in the cluster is called a Kafka broker\n\n","slug":"kafka","published":1,"updated":"2017-12-15T02:52:47.976Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gb80011tcingw6a5p3w","content":"<ul>\n<li>topics</li>\n<li>producers</li>\n<li>consumers</li>\n<li>brokers: all node in the cluster is called a Kafka broker</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<ul>\n<li>topics</li>\n<li>producers</li>\n<li>consumers</li>\n<li>brokers: all node in the cluster is called a Kafka broker</li>\n</ul>\n"},{"title":"logit-n-probit","date":"2017-10-10T11:36:54.000Z","_content":"\n### logistic VS logit \n先上两幅logistic和logit的图\n\n<!-- more -->\n\n* logistic function\nsigmoid(x) = 1/(1+e^-x)\n{% asset_img \"logistic.png\" [logistic] %}\n\n* logit function\nlogit(x) = log(x/1-x)\n{% asset_img \"logit.png\" [logit] %}\n\n* logit和logistic的关系\nlogit和logistic互为反函数，如下：\n{% asset_img \"logit-logistic-relation.png\" [logit-logistic-relation] %}\n\n\n\n","source":"_posts/logit-n-probit.md","raw":"---\ntitle: logit-n-probit\ndate: 2017-10-10 19:36:54\ntags:\n---\n\n### logistic VS logit \n先上两幅logistic和logit的图\n\n<!-- more -->\n\n* logistic function\nsigmoid(x) = 1/(1+e^-x)\n{% asset_img \"logistic.png\" [logistic] %}\n\n* logit function\nlogit(x) = log(x/1-x)\n{% asset_img \"logit.png\" [logit] %}\n\n* logit和logistic的关系\nlogit和logistic互为反函数，如下：\n{% asset_img \"logit-logistic-relation.png\" [logit-logistic-relation] %}\n\n\n\n","slug":"logit-n-probit","published":1,"updated":"2017-10-19T08:00:18.204Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gba0012tcinal0kduj2","content":"<h3 id=\"logistic-VS-logit\"><a href=\"#logistic-VS-logit\" class=\"headerlink\" title=\"logistic VS logit\"></a>logistic VS logit</h3><p>先上两幅logistic和logit的图</p>\n<a id=\"more\"></a>\n<ul>\n<li><p>logistic function<br>sigmoid(x) = 1/(1+e^-x)</p>\n<img src=\"/2017/10/10/logit-n-probit/logistic.png\" alt=\"[logistic]\" title=\"[logistic]\">\n</li>\n<li><p>logit function<br>logit(x) = log(x/1-x)</p>\n<img src=\"/2017/10/10/logit-n-probit/logit.png\" alt=\"[logit]\" title=\"[logit]\">\n</li>\n<li><p>logit和logistic的关系<br>logit和logistic互为反函数，如下：</p>\n\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h3 id=\"logistic-VS-logit\"><a href=\"#logistic-VS-logit\" class=\"headerlink\" title=\"logistic VS logit\"></a>logistic VS logit</h3><p>先上两幅logistic和logit的图</p>","more":"<ul>\n<li><p>logistic function<br>sigmoid(x) = 1/(1+e^-x)</p>\n<img src=\"/2017/10/10/logit-n-probit/logistic.png\" alt=\"[logistic]\" title=\"[logistic]\">\n</li>\n<li><p>logit function<br>logit(x) = log(x/1-x)</p>\n<img src=\"/2017/10/10/logit-n-probit/logit.png\" alt=\"[logit]\" title=\"[logit]\">\n</li>\n<li><p>logit和logistic的关系<br>logit和logistic互为反函数，如下：</p>\n\n</li>\n</ul>"},{"title":"ocpc","date":"2018-10-17T03:20:16.000Z","_content":"\n* cpc, ocpc, cpa的区别\ncpc：c计费，c为优化目标\nocpc：c计费，a为优化目标\ncpa: a计费，a为优化目标\n\n* 广告主希望优化a，为什么不直接用a计费，即cpa，而要使用ocpc？\n因为平台收益更有保障。\n* 量大：click的量明显高于action\n* 文案：能否a取决于广告的设计，平台方不好去增加文案设计等的风险。\n总之，\n* 越往前（cpm，cpt）平台方越有保障；越往后（cpa）广告主的收益约有保障，cpc是一个折中，针对cpc导致的a单价太高，推出了ocpc的方案。\n","source":"_posts/ocpc.md","raw":"---\ntitle: ocpc\ndate: 2018-10-17 11:20:16\ntags:\n---\n\n* cpc, ocpc, cpa的区别\ncpc：c计费，c为优化目标\nocpc：c计费，a为优化目标\ncpa: a计费，a为优化目标\n\n* 广告主希望优化a，为什么不直接用a计费，即cpa，而要使用ocpc？\n因为平台收益更有保障。\n* 量大：click的量明显高于action\n* 文案：能否a取决于广告的设计，平台方不好去增加文案设计等的风险。\n总之，\n* 越往前（cpm，cpt）平台方越有保障；越往后（cpa）广告主的收益约有保障，cpc是一个折中，针对cpc导致的a单价太高，推出了ocpc的方案。\n","slug":"ocpc","published":1,"updated":"2018-10-17T03:30:28.657Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gbd0013tcini5wieajh","content":"<ul>\n<li><p>cpc, ocpc, cpa的区别<br>cpc：c计费，c为优化目标<br>ocpc：c计费，a为优化目标<br>cpa: a计费，a为优化目标</p>\n</li>\n<li><p>广告主希望优化a，为什么不直接用a计费，即cpa，而要使用ocpc？<br>因为平台收益更有保障。</p>\n</li>\n<li>量大：click的量明显高于action</li>\n<li>文案：能否a取决于广告的设计，平台方不好去增加文案设计等的风险。<br>总之，</li>\n<li>越往前（cpm，cpt）平台方越有保障；越往后（cpa）广告主的收益约有保障，cpc是一个折中，针对cpc导致的a单价太高，推出了ocpc的方案。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<ul>\n<li><p>cpc, ocpc, cpa的区别<br>cpc：c计费，c为优化目标<br>ocpc：c计费，a为优化目标<br>cpa: a计费，a为优化目标</p>\n</li>\n<li><p>广告主希望优化a，为什么不直接用a计费，即cpa，而要使用ocpc？<br>因为平台收益更有保障。</p>\n</li>\n<li>量大：click的量明显高于action</li>\n<li>文案：能否a取决于广告的设计，平台方不好去增加文案设计等的风险。<br>总之，</li>\n<li>越往前（cpm，cpt）平台方越有保障；越往后（cpa）广告主的收益约有保障，cpc是一个折中，针对cpc导致的a单价太高，推出了ocpc的方案。</li>\n</ul>\n"},{"title":"lda","date":"2017-10-05T06:36:05.000Z","_content":"\n#### 理论\n* **痛点**<br>\n“乔布斯离我们而去了” 和 “苹果什么时候降价”如何关联？\n\n<!-- more -->\n\n* **思路**\n  * 将word映射到topic维度<br>\n  {% asset_img \"1.png\" [图片1] %}\n  * 概率表示<br>\n  {% asset_img \"2.png\" [图片2] %}\n  * 概率表示<br>\n  {% asset_img \"3.png\" [图片3] %}\n* **演进：Unigram Model**<br>\n  {% asset_img \"4.png\" [图片4] %}\n* **演进：Bayes Unigram Model**<br>\n  {% asset_img \"5.png\" [图片5] %}\n* **演进：PLSA**<br>\n  {% asset_img \"6.png\" [图片6] %}\n  {% asset_img \"7.png\" [图片7] %}\n* **演进：LDA**<br>\n  {% asset_img \"8.png\" [图片8] %}\n  {% asset_img \"9.png\" [图片9] %}\n* **参数估计：统计**<br>\n  {% asset_img \"100.png\" [图片9] %}\n* **参数估计：似然**<br>\n  {% asset_img \"101.png\" [图片9] %}\n* **参数估计：后验**<br>\n  {% asset_img \"102.png\" [图片9] %}\n* **参数估计：贝叶斯**<br>\n  {% asset_img \"103.png\" [图片9] %}\n* **参数估计：对比**<br>\n  {% asset_img \"104.png\" [图片9] %}\n* **马尔可夫链条**<br>\n  {% asset_img \"105.png\" [图片9] %}\n* **吉布斯采样**<br>\n  {% asset_img \"106.png\" [图片9] %}\n* **实现代码**<br>\n  {% asset_img \"201.png\" [图片9] %}\n* **Ref:**<br>\n  * Parameter estimation for text analysis （http://www.arbylon.net/publications/text-est.pdf）\n  * LDA数学八卦\n  * LDA简介 http://blog.csdn.net/huagong_adu/article/details/7937616\n  * Gibbs采样 https://www.youtube.com/watch?v=a_08GKWHFWo\n\n#### Dirichlet Distribution\n* 公式: p(P = {pi} | ai)\n* E(p) = ai/sum(ai)\n* 极大似然 = (ai-1) / sum(ai-1)\n\n#### 实践\n* 基础数据\n  * 豌豆荚软件的描述信息\n  * 星级>3星\n  * 下载数>100\n  * 安装数>100\n  * 用户数>100\n* 目的\n  * 得到基于内容（描述）的item2item\n  * 得到“词--主题--包名” 的关系\n* 代码\n  * [lda_code](../NLP/LDA原理和实践/README.md)\n\n\n* LDA工具<br>\n  https://github.com/liuzhiqiangruc/dml/tree/master/tm\n* 获取数据<br>\n```\nhive -e \"\nselect a.user_id, a.item_id, a.preference\nfrom\n(\n   ...\n)\n\" > input_lda\n```\n\n* 数据概况\n  * 基础数据获取：见hql\n  * 数据整理：cat input_lda | awk -F\"\\t\" '{ print $1\"\\t\"$2 }' > input\n  * 数据形式：user_id \\t item_id （后期可考虑tf-idf优化）\n  * 行数：1849296\n  * 用户数：678588\n  * 游戏数：3377\n* 运行命令\n```\n./lda -a 0.2 -b 0.01 -k 50 -n 1000 -s 100 -d ./input -o ./output\n\n    参数说明:\n     --------------------------------------------\n           -t               算法类型1:基本lda，2:lda-collective，3:lda_time\n           -r               运行模式，1:建模，2:burn-in\n           -a               p(z|d) 的 Dirichlet 参数\n           -b               p(w|z) 的 Dirichlet 参数\n           -k               Topic个数\n           -n               迭代次数\n           -s               每多少次迭代输出一次结果\n           -d               输入数据\n           -o               输出文件目录,实现需要存在\n\n  运行时长：10分钟左右\n```\n* 关联名称<br>\n  * 处理word_topic矩阵，将ID和名称关联起来<br>\n\n```\nHql如下，\nset hive.exec.compress.output=false;\ncreate table xxxx\n(\n    id  int\n) row format delimited\nfields terminated by '\\t';\n\nload data local inpath '/output/f_word_topic' OVERWRITE  into table xxxx;\n```\n\n* Item2Item计算<br>\n\n```\nmport sys\nimport math\nimport heapq\n\nitems_D = {} ## key: id\n\ndef load_data():\n    global items_D\n    inFp = open(\"lda_norm_10.csv\", 'r')\n    while True:\n        line = inFp.readline()\n        if not line:\n            break\n        items = line.strip().split(',')\n        if len(items) != 54:\n            continue\n        item_D = {}\n        item_D['soft_package_name'] = items[0]\n        item_D['name'] = items[1]\n        item_D['id'] = int(items[2])\n        item_D['topics'] = map(float, items[3:53])\n        item_D['sum'] = float(items[53])\n        items_D[item_D['id']] = item_D\n\n\ndef dis1(A, B):\n    return sum( A['topics'][i] * B['topics'][i] for i in range(50))\n\ndef dis2(A, B):\n    return sum( 100 - abs(A['topics'][i] - B['topics'][i]) for i in range(50))\n\ndef search_similar():\n    while True:\n        line = sys.stdin.readline()\n        idx = int(line.strip())\n        itemX = items_D[idx]\n        sim = -1.0\n        for idy, itemy in items_D.items():\n            simy = dis1(items_D[idx], items_D[idy])\n            if (simy > sim or sim < 0) and idx!=idy:\n                sim = simy\n                itemY = itemy\n        print \"%s\\tass\\t%s\"%(itemX['name'], itemY['name'])\n\nload_data()\nsearch_similar()\n```\n\n* 效果展示<br>\n{% asset_img \"302.png\" [图片1] %}\n* doc2topic<br>\n{% asset_img \"401.png\" [图片1] %}\n* topic2word<br>\n{% asset_img \"402.png\" [图片1] %}\n\n* 矩阵分解图谱<br>\n{% asset_img \"501.png\" [图片1] %}\n\n* 生成模型 VS 判别模型<br>\n  * 判别方法：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。<br>\n  * 生成方法：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)<br>\n\n#### 手写LDA\n* code<br>\n\n```\nimport sys\nimport random\n\nt_c = {}\ntw_c = {}\ntd_c = {}\n\nd_w = {}\nd_w_t = {}\nw_S = set()\n\nITER_NUM = 10000\nTOPIC_NUM = 2\nALPHA = 0.01\nBETA = 0.01\n\np_k = [0] * TOPIC_NUM\nprint p_k\n\ndef input():\n    while True:\n        line = sys.stdin.readline()\n        if not line:\n            break\n        items = line.strip().split('\\t')\n        doc = items[0]\n        word_L = items[1:]\n        for word in word_L:\n            d_w.setdefault(doc, list())\n            d_w[doc].append(word)\n            w_S.add(word)\n\ndef init():\n    for d, w_L in d_w.items():\n        for w in w_L:\n            for t in range(TOPIC_NUM):\n                t_c.setdefault(t, 0)\n                tw_c.setdefault(t, dict())\n                tw_c[t].setdefault(w, 0)\n                td_c.setdefault(t, dict())\n                td_c[t].setdefault(d, 0)\n\n    for d, w_L in d_w.items():\n        for w in w_L:\n            r = random.random()\n            if r < 0.5:\n                t = 0\n            else:\n                t = 1\n\n            d_w_t.setdefault(d, dict())\n            d_w_t[d].setdefault(w, t)\n\n            t_c[t] += 1\n            tw_c[t][w] += 1\n            td_c[t][d] += 1\n\n            print d_w_t[d][w]\n\ndef sampling():\n    for iter in range(ITER_NUM):\n        print \"iters is %d\" % iter\n        for d, w_L in d_w.items():\n            for w in w_L:\n                t = d_w_t[d][w]\n                t_c[t] -= 1\n                tw_c[t][w] -= 1\n                td_c[t][d] -= 1\n\n                for k in range(TOPIC_NUM):\n                    p_k[k] = (tw_c[k][w] + BETA) * (td_c[k][d] + ALPHA) * 1.0 / (t_c[k] + BETA*len(w_S))\n                sum = 0\n                for k in range(TOPIC_NUM):\n                    sum += p_k[k]\n                for k in range(TOPIC_NUM):\n                    p_k[k] /= sum\n                for k in range(1, TOPIC_NUM):\n                    p_k[k] += p_k[k-1]\n                r = random.random()\n                for k in range(TOPIC_NUM):\n                    if(r<=p_k[k]):\n                        t = k\n                        break\n                d_w_t[d][w] = t\n                t_c[t] += 1\n                tw_c[t][w] += 1\n                td_c[t][d] += 1\n\ndef output():\n    for d, w_L in d_w.items():\n        for w in w_L:\n            print \"%s\\t%s\\t%d\" % (d, w, d_w_t[d][w])\n\nif __name__ == \"__main__\":\n    input()\n    print \"input end...\"\n    init()\n    print \"init end...\"\n    sampling()\n    print \"samplint end...\"\n    output()\n    print \"output end...\"\n```\n\n* train corpus<br>\n```\ndoc1    枪      游戏    计算机  dota    电脑\ndoc4    娃娃    美丽    面膜    高跟鞋  裙子\ndoc5    购物    娃娃    裙子    SPA     指甲\ndoc2    枪      帅      电脑    坦克    飞机\ndoc3    游戏    坦克    飞机    数学    美丽\ndoc7    计算机  帅      枪      dota\ndoc6    美丽    购物    面膜    SPA     飘柔\n```\n\n* result<br>\n```\ndoc2    枪      1\ndoc2    帅      1\ndoc2    电脑    1\ndoc2    坦克    1\ndoc2    飞机    1\ndoc3    游戏    1\ndoc3    坦克    1\ndoc3    飞机    1\ndoc3    数学    1\ndoc3    美丽    0\ndoc1    枪      1\ndoc1    游戏    1\ndoc1    计算机  1\ndoc1    dota    1\ndoc1    电脑    1\ndoc6    美丽    0\ndoc6    购物    0\ndoc6    面膜    0\ndoc6    SPA     0\ndoc6    飘柔    0\ndoc7    计算机  1\ndoc7    帅      1\ndoc7    枪      1\ndoc7    dota    1\ndoc4    娃娃    0\ndoc4    美丽    0\ndoc4    面膜    0\ndoc4    高跟鞋  0\ndoc4    裙子    0\ndoc5    购物    0\ndoc5    娃娃    0\ndoc5    裙子    0\ndoc5    SPA     0\ndoc5    指甲    0\n```\n\n写的样例默认有2个主题，一个是男生主题，一个是女生主题，lda的结果是可以把两个topic分开的。1-男生，0-女生。","source":"_posts/lda.md","raw":"---\ntitle: lda\ndate: 2017-10-05 14:36:05\ntags:\ncategories: 数据挖掘\n---\n\n#### 理论\n* **痛点**<br>\n“乔布斯离我们而去了” 和 “苹果什么时候降价”如何关联？\n\n<!-- more -->\n\n* **思路**\n  * 将word映射到topic维度<br>\n  {% asset_img \"1.png\" [图片1] %}\n  * 概率表示<br>\n  {% asset_img \"2.png\" [图片2] %}\n  * 概率表示<br>\n  {% asset_img \"3.png\" [图片3] %}\n* **演进：Unigram Model**<br>\n  {% asset_img \"4.png\" [图片4] %}\n* **演进：Bayes Unigram Model**<br>\n  {% asset_img \"5.png\" [图片5] %}\n* **演进：PLSA**<br>\n  {% asset_img \"6.png\" [图片6] %}\n  {% asset_img \"7.png\" [图片7] %}\n* **演进：LDA**<br>\n  {% asset_img \"8.png\" [图片8] %}\n  {% asset_img \"9.png\" [图片9] %}\n* **参数估计：统计**<br>\n  {% asset_img \"100.png\" [图片9] %}\n* **参数估计：似然**<br>\n  {% asset_img \"101.png\" [图片9] %}\n* **参数估计：后验**<br>\n  {% asset_img \"102.png\" [图片9] %}\n* **参数估计：贝叶斯**<br>\n  {% asset_img \"103.png\" [图片9] %}\n* **参数估计：对比**<br>\n  {% asset_img \"104.png\" [图片9] %}\n* **马尔可夫链条**<br>\n  {% asset_img \"105.png\" [图片9] %}\n* **吉布斯采样**<br>\n  {% asset_img \"106.png\" [图片9] %}\n* **实现代码**<br>\n  {% asset_img \"201.png\" [图片9] %}\n* **Ref:**<br>\n  * Parameter estimation for text analysis （http://www.arbylon.net/publications/text-est.pdf）\n  * LDA数学八卦\n  * LDA简介 http://blog.csdn.net/huagong_adu/article/details/7937616\n  * Gibbs采样 https://www.youtube.com/watch?v=a_08GKWHFWo\n\n#### Dirichlet Distribution\n* 公式: p(P = {pi} | ai)\n* E(p) = ai/sum(ai)\n* 极大似然 = (ai-1) / sum(ai-1)\n\n#### 实践\n* 基础数据\n  * 豌豆荚软件的描述信息\n  * 星级>3星\n  * 下载数>100\n  * 安装数>100\n  * 用户数>100\n* 目的\n  * 得到基于内容（描述）的item2item\n  * 得到“词--主题--包名” 的关系\n* 代码\n  * [lda_code](../NLP/LDA原理和实践/README.md)\n\n\n* LDA工具<br>\n  https://github.com/liuzhiqiangruc/dml/tree/master/tm\n* 获取数据<br>\n```\nhive -e \"\nselect a.user_id, a.item_id, a.preference\nfrom\n(\n   ...\n)\n\" > input_lda\n```\n\n* 数据概况\n  * 基础数据获取：见hql\n  * 数据整理：cat input_lda | awk -F\"\\t\" '{ print $1\"\\t\"$2 }' > input\n  * 数据形式：user_id \\t item_id （后期可考虑tf-idf优化）\n  * 行数：1849296\n  * 用户数：678588\n  * 游戏数：3377\n* 运行命令\n```\n./lda -a 0.2 -b 0.01 -k 50 -n 1000 -s 100 -d ./input -o ./output\n\n    参数说明:\n     --------------------------------------------\n           -t               算法类型1:基本lda，2:lda-collective，3:lda_time\n           -r               运行模式，1:建模，2:burn-in\n           -a               p(z|d) 的 Dirichlet 参数\n           -b               p(w|z) 的 Dirichlet 参数\n           -k               Topic个数\n           -n               迭代次数\n           -s               每多少次迭代输出一次结果\n           -d               输入数据\n           -o               输出文件目录,实现需要存在\n\n  运行时长：10分钟左右\n```\n* 关联名称<br>\n  * 处理word_topic矩阵，将ID和名称关联起来<br>\n\n```\nHql如下，\nset hive.exec.compress.output=false;\ncreate table xxxx\n(\n    id  int\n) row format delimited\nfields terminated by '\\t';\n\nload data local inpath '/output/f_word_topic' OVERWRITE  into table xxxx;\n```\n\n* Item2Item计算<br>\n\n```\nmport sys\nimport math\nimport heapq\n\nitems_D = {} ## key: id\n\ndef load_data():\n    global items_D\n    inFp = open(\"lda_norm_10.csv\", 'r')\n    while True:\n        line = inFp.readline()\n        if not line:\n            break\n        items = line.strip().split(',')\n        if len(items) != 54:\n            continue\n        item_D = {}\n        item_D['soft_package_name'] = items[0]\n        item_D['name'] = items[1]\n        item_D['id'] = int(items[2])\n        item_D['topics'] = map(float, items[3:53])\n        item_D['sum'] = float(items[53])\n        items_D[item_D['id']] = item_D\n\n\ndef dis1(A, B):\n    return sum( A['topics'][i] * B['topics'][i] for i in range(50))\n\ndef dis2(A, B):\n    return sum( 100 - abs(A['topics'][i] - B['topics'][i]) for i in range(50))\n\ndef search_similar():\n    while True:\n        line = sys.stdin.readline()\n        idx = int(line.strip())\n        itemX = items_D[idx]\n        sim = -1.0\n        for idy, itemy in items_D.items():\n            simy = dis1(items_D[idx], items_D[idy])\n            if (simy > sim or sim < 0) and idx!=idy:\n                sim = simy\n                itemY = itemy\n        print \"%s\\tass\\t%s\"%(itemX['name'], itemY['name'])\n\nload_data()\nsearch_similar()\n```\n\n* 效果展示<br>\n{% asset_img \"302.png\" [图片1] %}\n* doc2topic<br>\n{% asset_img \"401.png\" [图片1] %}\n* topic2word<br>\n{% asset_img \"402.png\" [图片1] %}\n\n* 矩阵分解图谱<br>\n{% asset_img \"501.png\" [图片1] %}\n\n* 生成模型 VS 判别模型<br>\n  * 判别方法：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。<br>\n  * 生成方法：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)<br>\n\n#### 手写LDA\n* code<br>\n\n```\nimport sys\nimport random\n\nt_c = {}\ntw_c = {}\ntd_c = {}\n\nd_w = {}\nd_w_t = {}\nw_S = set()\n\nITER_NUM = 10000\nTOPIC_NUM = 2\nALPHA = 0.01\nBETA = 0.01\n\np_k = [0] * TOPIC_NUM\nprint p_k\n\ndef input():\n    while True:\n        line = sys.stdin.readline()\n        if not line:\n            break\n        items = line.strip().split('\\t')\n        doc = items[0]\n        word_L = items[1:]\n        for word in word_L:\n            d_w.setdefault(doc, list())\n            d_w[doc].append(word)\n            w_S.add(word)\n\ndef init():\n    for d, w_L in d_w.items():\n        for w in w_L:\n            for t in range(TOPIC_NUM):\n                t_c.setdefault(t, 0)\n                tw_c.setdefault(t, dict())\n                tw_c[t].setdefault(w, 0)\n                td_c.setdefault(t, dict())\n                td_c[t].setdefault(d, 0)\n\n    for d, w_L in d_w.items():\n        for w in w_L:\n            r = random.random()\n            if r < 0.5:\n                t = 0\n            else:\n                t = 1\n\n            d_w_t.setdefault(d, dict())\n            d_w_t[d].setdefault(w, t)\n\n            t_c[t] += 1\n            tw_c[t][w] += 1\n            td_c[t][d] += 1\n\n            print d_w_t[d][w]\n\ndef sampling():\n    for iter in range(ITER_NUM):\n        print \"iters is %d\" % iter\n        for d, w_L in d_w.items():\n            for w in w_L:\n                t = d_w_t[d][w]\n                t_c[t] -= 1\n                tw_c[t][w] -= 1\n                td_c[t][d] -= 1\n\n                for k in range(TOPIC_NUM):\n                    p_k[k] = (tw_c[k][w] + BETA) * (td_c[k][d] + ALPHA) * 1.0 / (t_c[k] + BETA*len(w_S))\n                sum = 0\n                for k in range(TOPIC_NUM):\n                    sum += p_k[k]\n                for k in range(TOPIC_NUM):\n                    p_k[k] /= sum\n                for k in range(1, TOPIC_NUM):\n                    p_k[k] += p_k[k-1]\n                r = random.random()\n                for k in range(TOPIC_NUM):\n                    if(r<=p_k[k]):\n                        t = k\n                        break\n                d_w_t[d][w] = t\n                t_c[t] += 1\n                tw_c[t][w] += 1\n                td_c[t][d] += 1\n\ndef output():\n    for d, w_L in d_w.items():\n        for w in w_L:\n            print \"%s\\t%s\\t%d\" % (d, w, d_w_t[d][w])\n\nif __name__ == \"__main__\":\n    input()\n    print \"input end...\"\n    init()\n    print \"init end...\"\n    sampling()\n    print \"samplint end...\"\n    output()\n    print \"output end...\"\n```\n\n* train corpus<br>\n```\ndoc1    枪      游戏    计算机  dota    电脑\ndoc4    娃娃    美丽    面膜    高跟鞋  裙子\ndoc5    购物    娃娃    裙子    SPA     指甲\ndoc2    枪      帅      电脑    坦克    飞机\ndoc3    游戏    坦克    飞机    数学    美丽\ndoc7    计算机  帅      枪      dota\ndoc6    美丽    购物    面膜    SPA     飘柔\n```\n\n* result<br>\n```\ndoc2    枪      1\ndoc2    帅      1\ndoc2    电脑    1\ndoc2    坦克    1\ndoc2    飞机    1\ndoc3    游戏    1\ndoc3    坦克    1\ndoc3    飞机    1\ndoc3    数学    1\ndoc3    美丽    0\ndoc1    枪      1\ndoc1    游戏    1\ndoc1    计算机  1\ndoc1    dota    1\ndoc1    电脑    1\ndoc6    美丽    0\ndoc6    购物    0\ndoc6    面膜    0\ndoc6    SPA     0\ndoc6    飘柔    0\ndoc7    计算机  1\ndoc7    帅      1\ndoc7    枪      1\ndoc7    dota    1\ndoc4    娃娃    0\ndoc4    美丽    0\ndoc4    面膜    0\ndoc4    高跟鞋  0\ndoc4    裙子    0\ndoc5    购物    0\ndoc5    娃娃    0\ndoc5    裙子    0\ndoc5    SPA     0\ndoc5    指甲    0\n```\n\n写的样例默认有2个主题，一个是男生主题，一个是女生主题，lda的结果是可以把两个topic分开的。1-男生，0-女生。","slug":"lda","published":1,"updated":"2018-09-29T07:44:17.167Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gbe0014tcinf77og2ku","content":"<h4 id=\"理论\"><a href=\"#理论\" class=\"headerlink\" title=\"理论\"></a>理论</h4><ul>\n<li><strong>痛点</strong><br><br>“乔布斯离我们而去了” 和 “苹果什么时候降价”如何关联？</li>\n</ul>\n<a id=\"more\"></a>\n<ul>\n<li><strong>思路</strong><ul>\n<li>将word映射到topic维度<br><img src=\"/2017/10/05/lda/1.png\" alt=\"[图片1]\" title=\"[图片1]\"></li>\n<li>概率表示<br><img src=\"/2017/10/05/lda/2.png\" alt=\"[图片2]\" title=\"[图片2]\"></li>\n<li>概率表示<br><img src=\"/2017/10/05/lda/3.png\" alt=\"[图片3]\" title=\"[图片3]\"></li>\n</ul>\n</li>\n<li><strong>演进：Unigram Model</strong><br><img src=\"/2017/10/05/lda/4.png\" alt=\"[图片4]\" title=\"[图片4]\"></li>\n<li><strong>演进：Bayes Unigram Model</strong><br><img src=\"/2017/10/05/lda/5.png\" alt=\"[图片5]\" title=\"[图片5]\"></li>\n<li><strong>演进：PLSA</strong><br><img src=\"/2017/10/05/lda/6.png\" alt=\"[图片6]\" title=\"[图片6]\">\n<img src=\"/2017/10/05/lda/7.png\" alt=\"[图片7]\" title=\"[图片7]\"></li>\n<li><strong>演进：LDA</strong><br><img src=\"/2017/10/05/lda/8.png\" alt=\"[图片8]\" title=\"[图片8]\">\n<img src=\"/2017/10/05/lda/9.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>参数估计：统计</strong><br><img src=\"/2017/10/05/lda/100.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>参数估计：似然</strong><br><img src=\"/2017/10/05/lda/101.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>参数估计：后验</strong><br><img src=\"/2017/10/05/lda/102.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>参数估计：贝叶斯</strong><br><img src=\"/2017/10/05/lda/103.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>参数估计：对比</strong><br><img src=\"/2017/10/05/lda/104.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>马尔可夫链条</strong><br><img src=\"/2017/10/05/lda/105.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>吉布斯采样</strong><br><img src=\"/2017/10/05/lda/106.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>实现代码</strong><br><img src=\"/2017/10/05/lda/201.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>Ref:</strong><br><ul>\n<li>Parameter estimation for text analysis （<a href=\"http://www.arbylon.net/publications/text-est.pdf）\" target=\"_blank\" rel=\"external\">http://www.arbylon.net/publications/text-est.pdf）</a></li>\n<li>LDA数学八卦</li>\n<li>LDA简介 <a href=\"http://blog.csdn.net/huagong_adu/article/details/7937616\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/huagong_adu/article/details/7937616</a></li>\n<li>Gibbs采样 <a href=\"https://www.youtube.com/watch?v=a_08GKWHFWo\" target=\"_blank\" rel=\"external\">https://www.youtube.com/watch?v=a_08GKWHFWo</a></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Dirichlet-Distribution\"><a href=\"#Dirichlet-Distribution\" class=\"headerlink\" title=\"Dirichlet Distribution\"></a>Dirichlet Distribution</h4><ul>\n<li>公式: p(P = {pi} | ai)</li>\n<li>E(p) = ai/sum(ai)</li>\n<li>极大似然 = (ai-1) / sum(ai-1)</li>\n</ul>\n<h4 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h4><ul>\n<li>基础数据<ul>\n<li>豌豆荚软件的描述信息</li>\n<li>星级&gt;3星</li>\n<li>下载数&gt;100</li>\n<li>安装数&gt;100</li>\n<li>用户数&gt;100</li>\n</ul>\n</li>\n<li>目的<ul>\n<li>得到基于内容（描述）的item2item</li>\n<li>得到“词–主题–包名” 的关系</li>\n</ul>\n</li>\n<li>代码<ul>\n<li><a href=\"../NLP/LDA原理和实践/README.md\">lda_code</a></li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>LDA工具<br><br><a href=\"https://github.com/liuzhiqiangruc/dml/tree/master/tm\" target=\"_blank\" rel=\"external\">https://github.com/liuzhiqiangruc/dml/tree/master/tm</a></li>\n<li><p>获取数据<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">hive -e &quot;</div><div class=\"line\">select a.user_id, a.item_id, a.preference</div><div class=\"line\">from</div><div class=\"line\">(</div><div class=\"line\">   ...</div><div class=\"line\">)</div><div class=\"line\">&quot; &gt; input_lda</div></pre></td></tr></table></figure>\n</li>\n<li><p>数据概况</p>\n<ul>\n<li>基础数据获取：见hql</li>\n<li>数据整理：cat input_lda | awk -F”\\t” ‘{ print $1”\\t”$2 }’ &gt; input</li>\n<li>数据形式：user_id \\t item_id （后期可考虑tf-idf优化）</li>\n<li>行数：1849296</li>\n<li>用户数：678588</li>\n<li>游戏数：3377</li>\n</ul>\n</li>\n<li><p>运行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">./lda -a 0.2 -b 0.01 -k 50 -n 1000 -s 100 -d ./input -o ./output</div><div class=\"line\"></div><div class=\"line\">    参数说明:</div><div class=\"line\">     --------------------------------------------</div><div class=\"line\">           -t               算法类型1:基本lda，2:lda-collective，3:lda_time</div><div class=\"line\">           -r               运行模式，1:建模，2:burn-in</div><div class=\"line\">           -a               p(z|d) 的 Dirichlet 参数</div><div class=\"line\">           -b               p(w|z) 的 Dirichlet 参数</div><div class=\"line\">           -k               Topic个数</div><div class=\"line\">           -n               迭代次数</div><div class=\"line\">           -s               每多少次迭代输出一次结果</div><div class=\"line\">           -d               输入数据</div><div class=\"line\">           -o               输出文件目录,实现需要存在</div><div class=\"line\"></div><div class=\"line\">  运行时长：10分钟左右</div></pre></td></tr></table></figure>\n</li>\n<li><p>关联名称<br></p>\n<ul>\n<li>处理word_topic矩阵，将ID和名称关联起来<br></li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">Hql如下，</div><div class=\"line\">set hive.exec.compress.output=false;</div><div class=\"line\">create table xxxx</div><div class=\"line\">(</div><div class=\"line\">    id  int</div><div class=\"line\">) row format delimited</div><div class=\"line\">fields terminated by &apos;\\t&apos;;</div><div class=\"line\"></div><div class=\"line\">load data local inpath &apos;/output/f_word_topic&apos; OVERWRITE  into table xxxx;</div></pre></td></tr></table></figure>\n<ul>\n<li>Item2Item计算<br></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div></pre></td><td class=\"code\"><pre><div class=\"line\">mport sys</div><div class=\"line\">import math</div><div class=\"line\">import heapq</div><div class=\"line\"></div><div class=\"line\">items_D = &#123;&#125; ## key: id</div><div class=\"line\"></div><div class=\"line\">def load_data():</div><div class=\"line\">    global items_D</div><div class=\"line\">    inFp = open(&quot;lda_norm_10.csv&quot;, &apos;r&apos;)</div><div class=\"line\">    while True:</div><div class=\"line\">        line = inFp.readline()</div><div class=\"line\">        if not line:</div><div class=\"line\">            break</div><div class=\"line\">        items = line.strip().split(&apos;,&apos;)</div><div class=\"line\">        if len(items) != 54:</div><div class=\"line\">            continue</div><div class=\"line\">        item_D = &#123;&#125;</div><div class=\"line\">        item_D[&apos;soft_package_name&apos;] = items[0]</div><div class=\"line\">        item_D[&apos;name&apos;] = items[1]</div><div class=\"line\">        item_D[&apos;id&apos;] = int(items[2])</div><div class=\"line\">        item_D[&apos;topics&apos;] = map(float, items[3:53])</div><div class=\"line\">        item_D[&apos;sum&apos;] = float(items[53])</div><div class=\"line\">        items_D[item_D[&apos;id&apos;]] = item_D</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def dis1(A, B):</div><div class=\"line\">    return sum( A[&apos;topics&apos;][i] * B[&apos;topics&apos;][i] for i in range(50))</div><div class=\"line\"></div><div class=\"line\">def dis2(A, B):</div><div class=\"line\">    return sum( 100 - abs(A[&apos;topics&apos;][i] - B[&apos;topics&apos;][i]) for i in range(50))</div><div class=\"line\"></div><div class=\"line\">def search_similar():</div><div class=\"line\">    while True:</div><div class=\"line\">        line = sys.stdin.readline()</div><div class=\"line\">        idx = int(line.strip())</div><div class=\"line\">        itemX = items_D[idx]</div><div class=\"line\">        sim = -1.0</div><div class=\"line\">        for idy, itemy in items_D.items():</div><div class=\"line\">            simy = dis1(items_D[idx], items_D[idy])</div><div class=\"line\">            if (simy &gt; sim or sim &lt; 0) and idx!=idy:</div><div class=\"line\">                sim = simy</div><div class=\"line\">                itemY = itemy</div><div class=\"line\">        print &quot;%s\\tass\\t%s&quot;%(itemX[&apos;name&apos;], itemY[&apos;name&apos;])</div><div class=\"line\"></div><div class=\"line\">load_data()</div><div class=\"line\">search_similar()</div></pre></td></tr></table></figure>\n<ul>\n<li>效果展示<br><img src=\"/2017/10/05/lda/302.png\" alt=\"[图片1]\" title=\"[图片1]\"></li>\n<li>doc2topic<br><img src=\"/2017/10/05/lda/401.png\" alt=\"[图片1]\" title=\"[图片1]\"></li>\n<li><p>topic2word<br></p>\n<img src=\"/2017/10/05/lda/402.png\" alt=\"[图片1]\" title=\"[图片1]\">\n</li>\n<li><p>矩阵分解图谱<br></p>\n<img src=\"/2017/10/05/lda/501.png\" alt=\"[图片1]\" title=\"[图片1]\">\n</li>\n<li><p>生成模型 VS 判别模型<br></p>\n<ul>\n<li>判别方法：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。<br></li>\n<li>生成方法：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)<br></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"手写LDA\"><a href=\"#手写LDA\" class=\"headerlink\" title=\"手写LDA\"></a>手写LDA</h4><ul>\n<li>code<br></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div></pre></td><td class=\"code\"><pre><div class=\"line\">import sys</div><div class=\"line\">import random</div><div class=\"line\"></div><div class=\"line\">t_c = &#123;&#125;</div><div class=\"line\">tw_c = &#123;&#125;</div><div class=\"line\">td_c = &#123;&#125;</div><div class=\"line\"></div><div class=\"line\">d_w = &#123;&#125;</div><div class=\"line\">d_w_t = &#123;&#125;</div><div class=\"line\">w_S = set()</div><div class=\"line\"></div><div class=\"line\">ITER_NUM = 10000</div><div class=\"line\">TOPIC_NUM = 2</div><div class=\"line\">ALPHA = 0.01</div><div class=\"line\">BETA = 0.01</div><div class=\"line\"></div><div class=\"line\">p_k = [0] * TOPIC_NUM</div><div class=\"line\">print p_k</div><div class=\"line\"></div><div class=\"line\">def input():</div><div class=\"line\">    while True:</div><div class=\"line\">        line = sys.stdin.readline()</div><div class=\"line\">        if not line:</div><div class=\"line\">            break</div><div class=\"line\">        items = line.strip().split(&apos;\\t&apos;)</div><div class=\"line\">        doc = items[0]</div><div class=\"line\">        word_L = items[1:]</div><div class=\"line\">        for word in word_L:</div><div class=\"line\">            d_w.setdefault(doc, list())</div><div class=\"line\">            d_w[doc].append(word)</div><div class=\"line\">            w_S.add(word)</div><div class=\"line\"></div><div class=\"line\">def init():</div><div class=\"line\">    for d, w_L in d_w.items():</div><div class=\"line\">        for w in w_L:</div><div class=\"line\">            for t in range(TOPIC_NUM):</div><div class=\"line\">                t_c.setdefault(t, 0)</div><div class=\"line\">                tw_c.setdefault(t, dict())</div><div class=\"line\">                tw_c[t].setdefault(w, 0)</div><div class=\"line\">                td_c.setdefault(t, dict())</div><div class=\"line\">                td_c[t].setdefault(d, 0)</div><div class=\"line\"></div><div class=\"line\">    for d, w_L in d_w.items():</div><div class=\"line\">        for w in w_L:</div><div class=\"line\">            r = random.random()</div><div class=\"line\">            if r &lt; 0.5:</div><div class=\"line\">                t = 0</div><div class=\"line\">            else:</div><div class=\"line\">                t = 1</div><div class=\"line\"></div><div class=\"line\">            d_w_t.setdefault(d, dict())</div><div class=\"line\">            d_w_t[d].setdefault(w, t)</div><div class=\"line\"></div><div class=\"line\">            t_c[t] += 1</div><div class=\"line\">            tw_c[t][w] += 1</div><div class=\"line\">            td_c[t][d] += 1</div><div class=\"line\"></div><div class=\"line\">            print d_w_t[d][w]</div><div class=\"line\"></div><div class=\"line\">def sampling():</div><div class=\"line\">    for iter in range(ITER_NUM):</div><div class=\"line\">        print &quot;iters is %d&quot; % iter</div><div class=\"line\">        for d, w_L in d_w.items():</div><div class=\"line\">            for w in w_L:</div><div class=\"line\">                t = d_w_t[d][w]</div><div class=\"line\">                t_c[t] -= 1</div><div class=\"line\">                tw_c[t][w] -= 1</div><div class=\"line\">                td_c[t][d] -= 1</div><div class=\"line\"></div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    p_k[k] = (tw_c[k][w] + BETA) * (td_c[k][d] + ALPHA) * 1.0 / (t_c[k] + BETA*len(w_S))</div><div class=\"line\">                sum = 0</div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    sum += p_k[k]</div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    p_k[k] /= sum</div><div class=\"line\">                for k in range(1, TOPIC_NUM):</div><div class=\"line\">                    p_k[k] += p_k[k-1]</div><div class=\"line\">                r = random.random()</div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    if(r&lt;=p_k[k]):</div><div class=\"line\">                        t = k</div><div class=\"line\">                        break</div><div class=\"line\">                d_w_t[d][w] = t</div><div class=\"line\">                t_c[t] += 1</div><div class=\"line\">                tw_c[t][w] += 1</div><div class=\"line\">                td_c[t][d] += 1</div><div class=\"line\"></div><div class=\"line\">def output():</div><div class=\"line\">    for d, w_L in d_w.items():</div><div class=\"line\">        for w in w_L:</div><div class=\"line\">            print &quot;%s\\t%s\\t%d&quot; % (d, w, d_w_t[d][w])</div><div class=\"line\"></div><div class=\"line\">if __name__ == &quot;__main__&quot;:</div><div class=\"line\">    input()</div><div class=\"line\">    print &quot;input end...&quot;</div><div class=\"line\">    init()</div><div class=\"line\">    print &quot;init end...&quot;</div><div class=\"line\">    sampling()</div><div class=\"line\">    print &quot;samplint end...&quot;</div><div class=\"line\">    output()</div><div class=\"line\">    print &quot;output end...&quot;</div></pre></td></tr></table></figure>\n<ul>\n<li><p>train corpus<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">doc1    枪      游戏    计算机  dota    电脑</div><div class=\"line\">doc4    娃娃    美丽    面膜    高跟鞋  裙子</div><div class=\"line\">doc5    购物    娃娃    裙子    SPA     指甲</div><div class=\"line\">doc2    枪      帅      电脑    坦克    飞机</div><div class=\"line\">doc3    游戏    坦克    飞机    数学    美丽</div><div class=\"line\">doc7    计算机  帅      枪      dota</div><div class=\"line\">doc6    美丽    购物    面膜    SPA     飘柔</div></pre></td></tr></table></figure>\n</li>\n<li><p>result<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div></pre></td><td class=\"code\"><pre><div class=\"line\">doc2    枪      1</div><div class=\"line\">doc2    帅      1</div><div class=\"line\">doc2    电脑    1</div><div class=\"line\">doc2    坦克    1</div><div class=\"line\">doc2    飞机    1</div><div class=\"line\">doc3    游戏    1</div><div class=\"line\">doc3    坦克    1</div><div class=\"line\">doc3    飞机    1</div><div class=\"line\">doc3    数学    1</div><div class=\"line\">doc3    美丽    0</div><div class=\"line\">doc1    枪      1</div><div class=\"line\">doc1    游戏    1</div><div class=\"line\">doc1    计算机  1</div><div class=\"line\">doc1    dota    1</div><div class=\"line\">doc1    电脑    1</div><div class=\"line\">doc6    美丽    0</div><div class=\"line\">doc6    购物    0</div><div class=\"line\">doc6    面膜    0</div><div class=\"line\">doc6    SPA     0</div><div class=\"line\">doc6    飘柔    0</div><div class=\"line\">doc7    计算机  1</div><div class=\"line\">doc7    帅      1</div><div class=\"line\">doc7    枪      1</div><div class=\"line\">doc7    dota    1</div><div class=\"line\">doc4    娃娃    0</div><div class=\"line\">doc4    美丽    0</div><div class=\"line\">doc4    面膜    0</div><div class=\"line\">doc4    高跟鞋  0</div><div class=\"line\">doc4    裙子    0</div><div class=\"line\">doc5    购物    0</div><div class=\"line\">doc5    娃娃    0</div><div class=\"line\">doc5    裙子    0</div><div class=\"line\">doc5    SPA     0</div><div class=\"line\">doc5    指甲    0</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>写的样例默认有2个主题，一个是男生主题，一个是女生主题，lda的结果是可以把两个topic分开的。1-男生，0-女生。</p>\n","site":{"data":{}},"excerpt":"<h4 id=\"理论\"><a href=\"#理论\" class=\"headerlink\" title=\"理论\"></a>理论</h4><ul>\n<li><strong>痛点</strong><br><br>“乔布斯离我们而去了” 和 “苹果什么时候降价”如何关联？</li>\n</ul>","more":"<ul>\n<li><strong>思路</strong><ul>\n<li>将word映射到topic维度<br><img src=\"/2017/10/05/lda/1.png\" alt=\"[图片1]\" title=\"[图片1]\"></li>\n<li>概率表示<br><img src=\"/2017/10/05/lda/2.png\" alt=\"[图片2]\" title=\"[图片2]\"></li>\n<li>概率表示<br><img src=\"/2017/10/05/lda/3.png\" alt=\"[图片3]\" title=\"[图片3]\"></li>\n</ul>\n</li>\n<li><strong>演进：Unigram Model</strong><br><img src=\"/2017/10/05/lda/4.png\" alt=\"[图片4]\" title=\"[图片4]\"></li>\n<li><strong>演进：Bayes Unigram Model</strong><br><img src=\"/2017/10/05/lda/5.png\" alt=\"[图片5]\" title=\"[图片5]\"></li>\n<li><strong>演进：PLSA</strong><br><img src=\"/2017/10/05/lda/6.png\" alt=\"[图片6]\" title=\"[图片6]\">\n<img src=\"/2017/10/05/lda/7.png\" alt=\"[图片7]\" title=\"[图片7]\"></li>\n<li><strong>演进：LDA</strong><br><img src=\"/2017/10/05/lda/8.png\" alt=\"[图片8]\" title=\"[图片8]\">\n<img src=\"/2017/10/05/lda/9.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>参数估计：统计</strong><br><img src=\"/2017/10/05/lda/100.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>参数估计：似然</strong><br><img src=\"/2017/10/05/lda/101.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>参数估计：后验</strong><br><img src=\"/2017/10/05/lda/102.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>参数估计：贝叶斯</strong><br><img src=\"/2017/10/05/lda/103.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>参数估计：对比</strong><br><img src=\"/2017/10/05/lda/104.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>马尔可夫链条</strong><br><img src=\"/2017/10/05/lda/105.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>吉布斯采样</strong><br><img src=\"/2017/10/05/lda/106.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>实现代码</strong><br><img src=\"/2017/10/05/lda/201.png\" alt=\"[图片9]\" title=\"[图片9]\"></li>\n<li><strong>Ref:</strong><br><ul>\n<li>Parameter estimation for text analysis （<a href=\"http://www.arbylon.net/publications/text-est.pdf）\" target=\"_blank\" rel=\"external\">http://www.arbylon.net/publications/text-est.pdf）</a></li>\n<li>LDA数学八卦</li>\n<li>LDA简介 <a href=\"http://blog.csdn.net/huagong_adu/article/details/7937616\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/huagong_adu/article/details/7937616</a></li>\n<li>Gibbs采样 <a href=\"https://www.youtube.com/watch?v=a_08GKWHFWo\" target=\"_blank\" rel=\"external\">https://www.youtube.com/watch?v=a_08GKWHFWo</a></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Dirichlet-Distribution\"><a href=\"#Dirichlet-Distribution\" class=\"headerlink\" title=\"Dirichlet Distribution\"></a>Dirichlet Distribution</h4><ul>\n<li>公式: p(P = {pi} | ai)</li>\n<li>E(p) = ai/sum(ai)</li>\n<li>极大似然 = (ai-1) / sum(ai-1)</li>\n</ul>\n<h4 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h4><ul>\n<li>基础数据<ul>\n<li>豌豆荚软件的描述信息</li>\n<li>星级&gt;3星</li>\n<li>下载数&gt;100</li>\n<li>安装数&gt;100</li>\n<li>用户数&gt;100</li>\n</ul>\n</li>\n<li>目的<ul>\n<li>得到基于内容（描述）的item2item</li>\n<li>得到“词–主题–包名” 的关系</li>\n</ul>\n</li>\n<li>代码<ul>\n<li><a href=\"../NLP/LDA原理和实践/README.md\">lda_code</a></li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>LDA工具<br><br><a href=\"https://github.com/liuzhiqiangruc/dml/tree/master/tm\" target=\"_blank\" rel=\"external\">https://github.com/liuzhiqiangruc/dml/tree/master/tm</a></li>\n<li><p>获取数据<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">hive -e &quot;</div><div class=\"line\">select a.user_id, a.item_id, a.preference</div><div class=\"line\">from</div><div class=\"line\">(</div><div class=\"line\">   ...</div><div class=\"line\">)</div><div class=\"line\">&quot; &gt; input_lda</div></pre></td></tr></table></figure>\n</li>\n<li><p>数据概况</p>\n<ul>\n<li>基础数据获取：见hql</li>\n<li>数据整理：cat input_lda | awk -F”\\t” ‘{ print $1”\\t”$2 }’ &gt; input</li>\n<li>数据形式：user_id \\t item_id （后期可考虑tf-idf优化）</li>\n<li>行数：1849296</li>\n<li>用户数：678588</li>\n<li>游戏数：3377</li>\n</ul>\n</li>\n<li><p>运行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">./lda -a 0.2 -b 0.01 -k 50 -n 1000 -s 100 -d ./input -o ./output</div><div class=\"line\"></div><div class=\"line\">    参数说明:</div><div class=\"line\">     --------------------------------------------</div><div class=\"line\">           -t               算法类型1:基本lda，2:lda-collective，3:lda_time</div><div class=\"line\">           -r               运行模式，1:建模，2:burn-in</div><div class=\"line\">           -a               p(z|d) 的 Dirichlet 参数</div><div class=\"line\">           -b               p(w|z) 的 Dirichlet 参数</div><div class=\"line\">           -k               Topic个数</div><div class=\"line\">           -n               迭代次数</div><div class=\"line\">           -s               每多少次迭代输出一次结果</div><div class=\"line\">           -d               输入数据</div><div class=\"line\">           -o               输出文件目录,实现需要存在</div><div class=\"line\"></div><div class=\"line\">  运行时长：10分钟左右</div></pre></td></tr></table></figure>\n</li>\n<li><p>关联名称<br></p>\n<ul>\n<li>处理word_topic矩阵，将ID和名称关联起来<br></li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">Hql如下，</div><div class=\"line\">set hive.exec.compress.output=false;</div><div class=\"line\">create table xxxx</div><div class=\"line\">(</div><div class=\"line\">    id  int</div><div class=\"line\">) row format delimited</div><div class=\"line\">fields terminated by &apos;\\t&apos;;</div><div class=\"line\"></div><div class=\"line\">load data local inpath &apos;/output/f_word_topic&apos; OVERWRITE  into table xxxx;</div></pre></td></tr></table></figure>\n<ul>\n<li>Item2Item计算<br></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div></pre></td><td class=\"code\"><pre><div class=\"line\">mport sys</div><div class=\"line\">import math</div><div class=\"line\">import heapq</div><div class=\"line\"></div><div class=\"line\">items_D = &#123;&#125; ## key: id</div><div class=\"line\"></div><div class=\"line\">def load_data():</div><div class=\"line\">    global items_D</div><div class=\"line\">    inFp = open(&quot;lda_norm_10.csv&quot;, &apos;r&apos;)</div><div class=\"line\">    while True:</div><div class=\"line\">        line = inFp.readline()</div><div class=\"line\">        if not line:</div><div class=\"line\">            break</div><div class=\"line\">        items = line.strip().split(&apos;,&apos;)</div><div class=\"line\">        if len(items) != 54:</div><div class=\"line\">            continue</div><div class=\"line\">        item_D = &#123;&#125;</div><div class=\"line\">        item_D[&apos;soft_package_name&apos;] = items[0]</div><div class=\"line\">        item_D[&apos;name&apos;] = items[1]</div><div class=\"line\">        item_D[&apos;id&apos;] = int(items[2])</div><div class=\"line\">        item_D[&apos;topics&apos;] = map(float, items[3:53])</div><div class=\"line\">        item_D[&apos;sum&apos;] = float(items[53])</div><div class=\"line\">        items_D[item_D[&apos;id&apos;]] = item_D</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def dis1(A, B):</div><div class=\"line\">    return sum( A[&apos;topics&apos;][i] * B[&apos;topics&apos;][i] for i in range(50))</div><div class=\"line\"></div><div class=\"line\">def dis2(A, B):</div><div class=\"line\">    return sum( 100 - abs(A[&apos;topics&apos;][i] - B[&apos;topics&apos;][i]) for i in range(50))</div><div class=\"line\"></div><div class=\"line\">def search_similar():</div><div class=\"line\">    while True:</div><div class=\"line\">        line = sys.stdin.readline()</div><div class=\"line\">        idx = int(line.strip())</div><div class=\"line\">        itemX = items_D[idx]</div><div class=\"line\">        sim = -1.0</div><div class=\"line\">        for idy, itemy in items_D.items():</div><div class=\"line\">            simy = dis1(items_D[idx], items_D[idy])</div><div class=\"line\">            if (simy &gt; sim or sim &lt; 0) and idx!=idy:</div><div class=\"line\">                sim = simy</div><div class=\"line\">                itemY = itemy</div><div class=\"line\">        print &quot;%s\\tass\\t%s&quot;%(itemX[&apos;name&apos;], itemY[&apos;name&apos;])</div><div class=\"line\"></div><div class=\"line\">load_data()</div><div class=\"line\">search_similar()</div></pre></td></tr></table></figure>\n<ul>\n<li>效果展示<br><img src=\"/2017/10/05/lda/302.png\" alt=\"[图片1]\" title=\"[图片1]\"></li>\n<li>doc2topic<br><img src=\"/2017/10/05/lda/401.png\" alt=\"[图片1]\" title=\"[图片1]\"></li>\n<li><p>topic2word<br></p>\n<img src=\"/2017/10/05/lda/402.png\" alt=\"[图片1]\" title=\"[图片1]\">\n</li>\n<li><p>矩阵分解图谱<br></p>\n<img src=\"/2017/10/05/lda/501.png\" alt=\"[图片1]\" title=\"[图片1]\">\n</li>\n<li><p>生成模型 VS 判别模型<br></p>\n<ul>\n<li>判别方法：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。<br></li>\n<li>生成方法：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)<br></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"手写LDA\"><a href=\"#手写LDA\" class=\"headerlink\" title=\"手写LDA\"></a>手写LDA</h4><ul>\n<li>code<br></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div></pre></td><td class=\"code\"><pre><div class=\"line\">import sys</div><div class=\"line\">import random</div><div class=\"line\"></div><div class=\"line\">t_c = &#123;&#125;</div><div class=\"line\">tw_c = &#123;&#125;</div><div class=\"line\">td_c = &#123;&#125;</div><div class=\"line\"></div><div class=\"line\">d_w = &#123;&#125;</div><div class=\"line\">d_w_t = &#123;&#125;</div><div class=\"line\">w_S = set()</div><div class=\"line\"></div><div class=\"line\">ITER_NUM = 10000</div><div class=\"line\">TOPIC_NUM = 2</div><div class=\"line\">ALPHA = 0.01</div><div class=\"line\">BETA = 0.01</div><div class=\"line\"></div><div class=\"line\">p_k = [0] * TOPIC_NUM</div><div class=\"line\">print p_k</div><div class=\"line\"></div><div class=\"line\">def input():</div><div class=\"line\">    while True:</div><div class=\"line\">        line = sys.stdin.readline()</div><div class=\"line\">        if not line:</div><div class=\"line\">            break</div><div class=\"line\">        items = line.strip().split(&apos;\\t&apos;)</div><div class=\"line\">        doc = items[0]</div><div class=\"line\">        word_L = items[1:]</div><div class=\"line\">        for word in word_L:</div><div class=\"line\">            d_w.setdefault(doc, list())</div><div class=\"line\">            d_w[doc].append(word)</div><div class=\"line\">            w_S.add(word)</div><div class=\"line\"></div><div class=\"line\">def init():</div><div class=\"line\">    for d, w_L in d_w.items():</div><div class=\"line\">        for w in w_L:</div><div class=\"line\">            for t in range(TOPIC_NUM):</div><div class=\"line\">                t_c.setdefault(t, 0)</div><div class=\"line\">                tw_c.setdefault(t, dict())</div><div class=\"line\">                tw_c[t].setdefault(w, 0)</div><div class=\"line\">                td_c.setdefault(t, dict())</div><div class=\"line\">                td_c[t].setdefault(d, 0)</div><div class=\"line\"></div><div class=\"line\">    for d, w_L in d_w.items():</div><div class=\"line\">        for w in w_L:</div><div class=\"line\">            r = random.random()</div><div class=\"line\">            if r &lt; 0.5:</div><div class=\"line\">                t = 0</div><div class=\"line\">            else:</div><div class=\"line\">                t = 1</div><div class=\"line\"></div><div class=\"line\">            d_w_t.setdefault(d, dict())</div><div class=\"line\">            d_w_t[d].setdefault(w, t)</div><div class=\"line\"></div><div class=\"line\">            t_c[t] += 1</div><div class=\"line\">            tw_c[t][w] += 1</div><div class=\"line\">            td_c[t][d] += 1</div><div class=\"line\"></div><div class=\"line\">            print d_w_t[d][w]</div><div class=\"line\"></div><div class=\"line\">def sampling():</div><div class=\"line\">    for iter in range(ITER_NUM):</div><div class=\"line\">        print &quot;iters is %d&quot; % iter</div><div class=\"line\">        for d, w_L in d_w.items():</div><div class=\"line\">            for w in w_L:</div><div class=\"line\">                t = d_w_t[d][w]</div><div class=\"line\">                t_c[t] -= 1</div><div class=\"line\">                tw_c[t][w] -= 1</div><div class=\"line\">                td_c[t][d] -= 1</div><div class=\"line\"></div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    p_k[k] = (tw_c[k][w] + BETA) * (td_c[k][d] + ALPHA) * 1.0 / (t_c[k] + BETA*len(w_S))</div><div class=\"line\">                sum = 0</div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    sum += p_k[k]</div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    p_k[k] /= sum</div><div class=\"line\">                for k in range(1, TOPIC_NUM):</div><div class=\"line\">                    p_k[k] += p_k[k-1]</div><div class=\"line\">                r = random.random()</div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    if(r&lt;=p_k[k]):</div><div class=\"line\">                        t = k</div><div class=\"line\">                        break</div><div class=\"line\">                d_w_t[d][w] = t</div><div class=\"line\">                t_c[t] += 1</div><div class=\"line\">                tw_c[t][w] += 1</div><div class=\"line\">                td_c[t][d] += 1</div><div class=\"line\"></div><div class=\"line\">def output():</div><div class=\"line\">    for d, w_L in d_w.items():</div><div class=\"line\">        for w in w_L:</div><div class=\"line\">            print &quot;%s\\t%s\\t%d&quot; % (d, w, d_w_t[d][w])</div><div class=\"line\"></div><div class=\"line\">if __name__ == &quot;__main__&quot;:</div><div class=\"line\">    input()</div><div class=\"line\">    print &quot;input end...&quot;</div><div class=\"line\">    init()</div><div class=\"line\">    print &quot;init end...&quot;</div><div class=\"line\">    sampling()</div><div class=\"line\">    print &quot;samplint end...&quot;</div><div class=\"line\">    output()</div><div class=\"line\">    print &quot;output end...&quot;</div></pre></td></tr></table></figure>\n<ul>\n<li><p>train corpus<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">doc1    枪      游戏    计算机  dota    电脑</div><div class=\"line\">doc4    娃娃    美丽    面膜    高跟鞋  裙子</div><div class=\"line\">doc5    购物    娃娃    裙子    SPA     指甲</div><div class=\"line\">doc2    枪      帅      电脑    坦克    飞机</div><div class=\"line\">doc3    游戏    坦克    飞机    数学    美丽</div><div class=\"line\">doc7    计算机  帅      枪      dota</div><div class=\"line\">doc6    美丽    购物    面膜    SPA     飘柔</div></pre></td></tr></table></figure>\n</li>\n<li><p>result<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div></pre></td><td class=\"code\"><pre><div class=\"line\">doc2    枪      1</div><div class=\"line\">doc2    帅      1</div><div class=\"line\">doc2    电脑    1</div><div class=\"line\">doc2    坦克    1</div><div class=\"line\">doc2    飞机    1</div><div class=\"line\">doc3    游戏    1</div><div class=\"line\">doc3    坦克    1</div><div class=\"line\">doc3    飞机    1</div><div class=\"line\">doc3    数学    1</div><div class=\"line\">doc3    美丽    0</div><div class=\"line\">doc1    枪      1</div><div class=\"line\">doc1    游戏    1</div><div class=\"line\">doc1    计算机  1</div><div class=\"line\">doc1    dota    1</div><div class=\"line\">doc1    电脑    1</div><div class=\"line\">doc6    美丽    0</div><div class=\"line\">doc6    购物    0</div><div class=\"line\">doc6    面膜    0</div><div class=\"line\">doc6    SPA     0</div><div class=\"line\">doc6    飘柔    0</div><div class=\"line\">doc7    计算机  1</div><div class=\"line\">doc7    帅      1</div><div class=\"line\">doc7    枪      1</div><div class=\"line\">doc7    dota    1</div><div class=\"line\">doc4    娃娃    0</div><div class=\"line\">doc4    美丽    0</div><div class=\"line\">doc4    面膜    0</div><div class=\"line\">doc4    高跟鞋  0</div><div class=\"line\">doc4    裙子    0</div><div class=\"line\">doc5    购物    0</div><div class=\"line\">doc5    娃娃    0</div><div class=\"line\">doc5    裙子    0</div><div class=\"line\">doc5    SPA     0</div><div class=\"line\">doc5    指甲    0</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>写的样例默认有2个主题，一个是男生主题，一个是女生主题，lda的结果是可以把两个topic分开的。1-男生，0-女生。</p>"},{"title":"pr","date":"2017-10-15T07:40:41.000Z","_content":"\nPR有三个含义，差点儿搞晕了：\n* Public Relation: 公共关系，即公关\n* Peer Review: 同事评估，往往是代码的peer review, 代码的话常常code review(CR)\n* Pull Request: git上initiate discussion about your commits","source":"_posts/pr.md","raw":"---\ntitle: pr\ndate: 2017-10-15 15:40:41\ntags:\n---\n\nPR有三个含义，差点儿搞晕了：\n* Public Relation: 公共关系，即公关\n* Peer Review: 同事评估，往往是代码的peer review, 代码的话常常code review(CR)\n* Pull Request: git上initiate discussion about your commits","slug":"pr","published":1,"updated":"2017-10-19T02:12:38.013Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gbg0015tcinlbi41csk","content":"<p>PR有三个含义，差点儿搞晕了：</p>\n<ul>\n<li>Public Relation: 公共关系，即公关</li>\n<li>Peer Review: 同事评估，往往是代码的peer review, 代码的话常常code review(CR)</li>\n<li>Pull Request: git上initiate discussion about your commits</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>PR有三个含义，差点儿搞晕了：</p>\n<ul>\n<li>Public Relation: 公共关系，即公关</li>\n<li>Peer Review: 同事评估，往往是代码的peer review, 代码的话常常code review(CR)</li>\n<li>Pull Request: git上initiate discussion about your commits</li>\n</ul>\n"},{"title":"spark-streaming","date":"2017-11-17T06:38:36.000Z","_content":"\n### spark streaming k-means\n* decay(forgetfulness)\n* mini-batch k-means\n    * c_t+1 = [(c_t * n_t * a) + (x_t * m_t)] / [n_t + m_t]\n    * n_t+t = n_t * a + m_t\n\n\n### Broadcast Variables\n\n* ref\nhttps://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\n\n\n","source":"_posts/spark-streaming.md","raw":"---\ntitle: spark-streaming\ndate: 2017-11-17 14:38:36\ntags:\n---\n\n### spark streaming k-means\n* decay(forgetfulness)\n* mini-batch k-means\n    * c_t+1 = [(c_t * n_t * a) + (x_t * m_t)] / [n_t + m_t]\n    * n_t+t = n_t * a + m_t\n\n\n### Broadcast Variables\n\n* ref\nhttps://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\n\n\n","slug":"spark-streaming","published":1,"updated":"2018-04-12T08:18:07.005Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gbj0017tcinzyljfkvd","content":"<h3 id=\"spark-streaming-k-means\"><a href=\"#spark-streaming-k-means\" class=\"headerlink\" title=\"spark streaming k-means\"></a>spark streaming k-means</h3><ul>\n<li>decay(forgetfulness)</li>\n<li>mini-batch k-means<ul>\n<li>c_t+1 = [(c_t <em> n_t </em> a) + (x_t * m_t)] / [n_t + m_t]</li>\n<li>n_t+t = n_t * a + m_t</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Broadcast-Variables\"><a href=\"#Broadcast-Variables\" class=\"headerlink\" title=\"Broadcast Variables\"></a>Broadcast Variables</h3><ul>\n<li>ref<br><a href=\"https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\" target=\"_blank\" rel=\"external\">https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"spark-streaming-k-means\"><a href=\"#spark-streaming-k-means\" class=\"headerlink\" title=\"spark streaming k-means\"></a>spark streaming k-means</h3><ul>\n<li>decay(forgetfulness)</li>\n<li>mini-batch k-means<ul>\n<li>c_t+1 = [(c_t <em> n_t </em> a) + (x_t * m_t)] / [n_t + m_t]</li>\n<li>n_t+t = n_t * a + m_t</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Broadcast-Variables\"><a href=\"#Broadcast-Variables\" class=\"headerlink\" title=\"Broadcast Variables\"></a>Broadcast Variables</h3><ul>\n<li>ref<br><a href=\"https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\" target=\"_blank\" rel=\"external\">https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html</a></li>\n</ul>\n"},{"title":"query_parse","date":"2018-03-20T12:02:29.000Z","_content":"\n#### 引流产品\n* 暗文\n* 热词\n* 下拉词\n* QueryParse\n    * Understanding\n    * Rewrite\n* 相关搜索\n* 业务相关\n    * 大促销(11.11)\n* 算法\n    * 引入KPI因子（客单价，客单数）\n    * 语义分析（送给爸爸的礼物）\n    * 序列识别\n    * 主题聚类（育儿场景）\n    * 个性化（性别，类目，历史词）\n* 数据\n    * 归一化（命名实体识别）\n    * 数据清洗（反作弊）\n    * 违禁词（菜刀）\n    * 不完整词（阿迪达）\n    * 季节词（茶叶，季节词提权重）\n    * 纠错\n* 基础 \n    * ABTest分流\n\n#### 基础数据\n* \n\n#### QueryParse\n* QP Server\n*\n\n#### 下拉&相关搜索\n8 ","source":"_posts/query-parse.md","raw":"---\ntitle: query_parse\ndate: 2018-03-20 20:02:29\ntags:\n---\n\n#### 引流产品\n* 暗文\n* 热词\n* 下拉词\n* QueryParse\n    * Understanding\n    * Rewrite\n* 相关搜索\n* 业务相关\n    * 大促销(11.11)\n* 算法\n    * 引入KPI因子（客单价，客单数）\n    * 语义分析（送给爸爸的礼物）\n    * 序列识别\n    * 主题聚类（育儿场景）\n    * 个性化（性别，类目，历史词）\n* 数据\n    * 归一化（命名实体识别）\n    * 数据清洗（反作弊）\n    * 违禁词（菜刀）\n    * 不完整词（阿迪达）\n    * 季节词（茶叶，季节词提权重）\n    * 纠错\n* 基础 \n    * ABTest分流\n\n#### 基础数据\n* \n\n#### QueryParse\n* QP Server\n*\n\n#### 下拉&相关搜索\n8 ","slug":"query-parse","published":1,"updated":"2018-03-21T02:28:15.581Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gbk0018tcinkejg4i0b","content":"<h4 id=\"引流产品\"><a href=\"#引流产品\" class=\"headerlink\" title=\"引流产品\"></a>引流产品</h4><ul>\n<li>暗文</li>\n<li>热词</li>\n<li>下拉词</li>\n<li>QueryParse<ul>\n<li>Understanding</li>\n<li>Rewrite</li>\n</ul>\n</li>\n<li>相关搜索</li>\n<li>业务相关<ul>\n<li>大促销(11.11)</li>\n</ul>\n</li>\n<li>算法<ul>\n<li>引入KPI因子（客单价，客单数）</li>\n<li>语义分析（送给爸爸的礼物）</li>\n<li>序列识别</li>\n<li>主题聚类（育儿场景）</li>\n<li>个性化（性别，类目，历史词）</li>\n</ul>\n</li>\n<li>数据<ul>\n<li>归一化（命名实体识别）</li>\n<li>数据清洗（反作弊）</li>\n<li>违禁词（菜刀）</li>\n<li>不完整词（阿迪达）</li>\n<li>季节词（茶叶，季节词提权重）</li>\n<li>纠错</li>\n</ul>\n</li>\n<li>基础 <ul>\n<li>ABTest分流</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"基础数据\"><a href=\"#基础数据\" class=\"headerlink\" title=\"基础数据\"></a>基础数据</h4><ul>\n<li><h4 id=\"QueryParse\"><a href=\"#QueryParse\" class=\"headerlink\" title=\"QueryParse\"></a>QueryParse</h4></li>\n<li>QP Server<br>*</li>\n</ul>\n<h4 id=\"下拉-amp-相关搜索\"><a href=\"#下拉-amp-相关搜索\" class=\"headerlink\" title=\"下拉&amp;相关搜索\"></a>下拉&amp;相关搜索</h4><p>8 </p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"引流产品\"><a href=\"#引流产品\" class=\"headerlink\" title=\"引流产品\"></a>引流产品</h4><ul>\n<li>暗文</li>\n<li>热词</li>\n<li>下拉词</li>\n<li>QueryParse<ul>\n<li>Understanding</li>\n<li>Rewrite</li>\n</ul>\n</li>\n<li>相关搜索</li>\n<li>业务相关<ul>\n<li>大促销(11.11)</li>\n</ul>\n</li>\n<li>算法<ul>\n<li>引入KPI因子（客单价，客单数）</li>\n<li>语义分析（送给爸爸的礼物）</li>\n<li>序列识别</li>\n<li>主题聚类（育儿场景）</li>\n<li>个性化（性别，类目，历史词）</li>\n</ul>\n</li>\n<li>数据<ul>\n<li>归一化（命名实体识别）</li>\n<li>数据清洗（反作弊）</li>\n<li>违禁词（菜刀）</li>\n<li>不完整词（阿迪达）</li>\n<li>季节词（茶叶，季节词提权重）</li>\n<li>纠错</li>\n</ul>\n</li>\n<li>基础 <ul>\n<li>ABTest分流</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"基础数据\"><a href=\"#基础数据\" class=\"headerlink\" title=\"基础数据\"></a>基础数据</h4><ul>\n<li><h4 id=\"QueryParse\"><a href=\"#QueryParse\" class=\"headerlink\" title=\"QueryParse\"></a>QueryParse</h4></li>\n<li>QP Server<br>*</li>\n</ul>\n<h4 id=\"下拉-amp-相关搜索\"><a href=\"#下拉-amp-相关搜索\" class=\"headerlink\" title=\"下拉&amp;相关搜索\"></a>下拉&amp;相关搜索</h4><p>8 </p>\n"},{"title":"word2vec","date":"2018-06-13T14:34:28.000Z","_content":"\n##### 起源和工具包\nword2vec是google在2013年的模型，可以有效的将词映射到某个向量空间，经典的demo是：\nvector('Paris') - vector('France') + vector('Italy') = vector('Rome')\n其开源地址是：https://code.google.com/archive/p/word2vec/\n此工具简单好用，在很多项目上起到了很好的效果\n\n##### word embedding的各种姿势\n* Topic Model：词袋模型，不分先后顺序，代表是LDA模型\n* Word2vec：等价于三层神经网络，能捕捉时序信息\n* FM：通过FM的weight矩阵将原始向量做变换\n* DSSM：通过点击数据把user和item的分别作Deep模型，取出来最顶层的做word representation\n\n另外NLP领域最新的模型是LSTM和Attention，能否利用其模型做embedding还需调研下\n* LSTM：能更好的捕捉时序信息，解决long dependence的问题\n* Attention：通过注意力机制\n\n##### word2vec的各种包\n* google word2vec：效果非常好，单机\n* gensim：经典的自然语言处理包，用的人很多，单机\n* spark mllib：不确定效果如何，分布式\n* tensorflow：未尝试，分布式\n（因为各种姿势都可以通过tensorflow来实现，包括FM，Word2Vec，以及各种矩阵分解，因此tf是很值得学习的工具。）\n\n##### Why word embedding？\n* 降维：原始ID维度的特征太大了，需要过多的训练数据，word embedding之后不需要那么多数据，而且防止过拟合\n\n##### embedding的两类方法\n* count-based：lsa系列，通过矩阵分解来做embedding\n* predictive-based：通过预测词可能出现的词来做embedding，如word2vec\n\n##### gensim建模+tensorboard可视化\n* 生成gensim训练数据，每行一个doc，空格分隔\n```\nselect list\nfrom\n(\n    select user_id, concat_ws(' ', collect_set(item_id)) as list\n    from dm_music_prd.t_7d_imusic_iting_user_item_action\n    where ds>=20180520 and action=1 and item_id != -1 and extend>1000*90\n    group by user_id\n) t1\njoin\n(\n    select user_id\n    from\n    (\n        select user_id, count(distinct item_id) as cnt\n        from dm_music_prd.t_7d_imusic_iting_user_item_action\n        where ds>=20180520 and action=1 and item_id != -1 and extend>1000*90\n        group by user_id\n    ) f\n    where cnt >= 5 and cnt <= 50\n) t2 on t1.user_id=t2.user_id\n```\n\n* gensim训练word2vec模型\n```\n/root/xpguo/anaconda3/bin/python w2v_v2.py user_songlist.v2 ./model_v2/w2v20180625 song_vector_v2\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport logging\nimport os\nimport sys\nimport multiprocessing\n\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\n\nif __name__ == '__main__':\n    program = os.path.basename(sys.argv[0])\n    logger = logging.getLogger(program)\n\n    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n    logging.root.setLevel(level=logging.INFO)\n    logger.info(\"running %s\" % ' '.join(sys.argv))\n\n    # check and process input arguments\n    if len(sys.argv) < 4:\n        print(globals()['__doc__'] % locals())\n        sys.exit(1)\n    inp, outp1, outp2 = sys.argv[1:4]\n\n    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5,\n                     workers=multiprocessing.cpu_count())\n\n    # trim unneeded model memory = use(much) less RAM\n    # model.init_sims(replace=True)\n    # w2v model\n    model.save(outp1)\n    # word vectors\n    model.wv.save_word2vec_format(outp2, binary=False)\n```\n\n* Tensorboard 可视化\n```\n1.\t拉取id2name\nmysql -h10.20.125.43 -umyshuju_r -p3KAjvBHaDB{gLE9H -e \"select third_id, name from music.t_song_info where third_id is not null and name is not null and length(third_id)>0 and length(name)>0\" > t_song_info\n\n2.\t写tensorboard模型\nimport sys, os\nfrom gensim.models import Word2Vec\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\nid2name = {}\n\n## 可视化函数\n## model: gensim模型地址\n## output_path: tf board转换模型地址\ndef visualize(model, output_path):\n    ## tf board转换模型名称，自定义\n    meta_file = \"w2x_metadata.tsv\"\n\n    ## 词向量个数*词向量维数\n    placeholder = np.zeros((len(model.wv.index2word), 400))\n\n    ## 读取ID2name文件\n    inFp = open(\"./t_song_info\", 'r')\n    while True:\n        line = inFp.readline()\n        if not line:\n            break\n        items = line.strip().split('\\t')\n        if len(items) != 2:\n            continue\n        id2name[items[0]] = items[1]\n    inFp.close()\n\n    ## 地址+名称拼接\n    with open(os.path.join(output_path,meta_file), 'wb') as file_metadata:\n\n        ## 对于每个词向量，写文件\n        for i, word in enumerate(model.wv.index2word):\n            placeholder[i] = model[word]\n            # temporary solution for https://github.com/tensorflow/tensorflow/issues/9094\n            if word == '':\n                print(\"Emply Line, should replecaed by any thing else, or will cause a bug of tensorboard\")\n                file_metadata.write(\"{0}\".format('<Empty Line>').encode('utf-8') + b'\\n')\n            else:\n#                file_metadata.write(\"{0}\".format(word).encode('utf-8') + b'\\n')\n                file_metadata.write(\"{0}\".format(id2name.get(word, 'null')).encode('utf-8') + b'\\n')\n\n\n    # define the model without training\n    sess = tf.InteractiveSession()\n\n    embedding = tf.Variable(placeholder, trainable = False, name = 'w2x_metadata')\n    tf.global_variables_initializer().run()\n\n    saver = tf.train.Saver()\n    writer = tf.summary.FileWriter(output_path, sess.graph)\n\n    # adding into projector\n    config = projector.ProjectorConfig()\n    embed = config.embeddings.add()\n    embed.tensor_name = 'w2x_metadata'\n    embed.metadata_path = meta_file\n\n    # Specify the width and height of a single thumbnail.\n    projector.visualize_embeddings(writer, config)\n    saver.save(sess, os.path.join(output_path,'w2x_metadata.ckpt'))\n    print('Run `tensorboard --logdir={0}` to run visualize result on tensorboard'.format(output_path))\n\nif __name__ == \"__main__\":\n    model = Word2Vec.load(\"/home/xpguo/gensim/word2vec/song_vector_v1/a\")\nvisualize(model,\"/home/xpguo/gensim/word2vec/song_vector_v1_tf_board\")\n\n3.\ttensorboard可视化\ntensorboard --logdir=/home/xpguo/gensim/word2vec/song_vector_v2_tf_board --port=6607\n```\n\n* Tensorboard 可视化效果\n{% asset_img \"tfboard.png\" [tf-board] %}\n\n##### word2vec源码\n*　cbow-框架图\n{% asset_img \"cbow-1.png\" [cbow-1] %}\n\n* cbow-Hierarchical softmax\n{% asset_img \"cbow-hs.png\" [cbow-hs] %}\n\n* cbow-hs-hand\n{% asset_img \"cbow-hs-hand.png\" [cbow-hs-hand.png] %}\n\n##### word2vec源码细节\n* 预计算sigmoid(x)，将[-6, 6]的区间划分成1000份来计算。\n```\n#define EXP_TABLE_SIZE 1000\n#define MAX_EXP 6\nexpTable = (real *)malloc((EXP_TABLE_SIZE + 1) * sizeof(real));\nfor (i = 0; i < EXP_TABLE_SIZE; i++) {\n  expTable[i] = exp((i / (real)EXP_TABLE_SIZE * 2 - 1) * MAX_EXP); // Precompute the exp() table\n  expTable[i] = expTable[i] / (expTable[i] + 1);                   // Precompute f(x) = x / (x + 1)\n}\n```\n* hash函数，每个char+原hash值*257，很常见的做法\n```\n// Returns hash value of a word\nint GetWordHash(char *word) {\n  unsigned long long a, hash = 0;\n  for (a = 0; a < strlen(word); a++) hash = hash * 257 + word[a];\n  hash = hash % vocab_hash_size;\n  return hash;\n}\n```\n* 线性探测法，hash后线性地去找词，要么找到返回词的位置，要么找到-1\n```\n// Returns position of a word in the vocabulary; if the word is not found, returns -1\nint SearchVocab(char *word) {\n  unsigned int hash = GetWordHash(word);\n  while (1) {\n    if (vocab_hash[hash] == -1) return -1;\n    if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash];\n    hash = (hash + 1) % vocab_hash_size;\n  }\n  return -1;\n}\n```\n* 将词添加到词典中， 有两个地方要加，第一是hash映射表，第二是词对应的count表\n```\n// Adds a word to the vocabulary\nint AddWordToVocab(char *word) {\n  unsigned int hash, length = strlen(word) + 1;\n  if (length > MAX_STRING) length = MAX_STRING;\n  vocab[vocab_size].word = (char *)calloc(length, sizeof(char));\n  strcpy(vocab[vocab_size].word, word);\n  vocab[vocab_size].cn = 0;\n  vocab_size++;\n  // Reallocate memory if needed\n  if (vocab_size + 2 >= vocab_max_size) {\n    vocab_max_size += 1000;\n    vocab = (struct vocab_word *)realloc(vocab, vocab_max_size * sizeof(struct vocab_word));\n  }\n  hash = GetWordHash(word);\n  while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;\n  vocab_hash[hash] = vocab_size - 1;\n  return vocab_size - 1;\n}\n```\n* 将词典排序，目的是为了建立哈弗曼树，有几个地方要注意：一是出现频率比较低的词，会被过滤掉；二是过滤掉之后需要讲hashtable重新hash，因为去掉了一些词，table不能用了。感慨一下，c语言要自己实现hash_dict，真心麻烦。\n```\n// Sorts the vocabulary by frequency using word counts\nvoid SortVocab() {\n  int a, size;\n  unsigned int hash;\n  // Sort the vocabulary and keep </s> at the first position\n  qsort(&vocab[1], vocab_size - 1, sizeof(struct vocab_word), VocabCompare);\n  for (a = 0; a < vocab_hash_size; a++) vocab_hash[a] = -1;\n  size = vocab_size;\n  train_words = 0;\n  for (a = 0; a < size; a++) {\n    // Words occuring less than min_count times will be discarded from the vocab\n    if ((vocab[a].cn < min_count) && (a != 0)) {\n      vocab_size--;\n      free(vocab[a].word);\n    } else {\n      // Hash will be re-computed, as after the sorting it is not actual\n      hash=GetWordHash(vocab[a].word);\n      while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;\n      vocab_hash[hash] = a;\n      train_words += vocab[a].cn;\n    }\n  }\n  vocab = (struct vocab_word *)realloc(vocab, (vocab_size + 1) * sizeof(struct vocab_word));\n  // Allocate memory for the binary tree construction\n  for (a = 0; a < vocab_size; a++) {\n    vocab[a].code = (char *)calloc(MAX_CODE_LENGTH, sizeof(char));\n    vocab[a].point = (int *)calloc(MAX_CODE_LENGTH, sizeof(int));\n  }\n}\n```\n* 初始化网络参数，首先要搞清楚网络是什么样子的：\n    * syn0: e(w), 向量映射表，最终看的结果\n    * syn1: theta(w, j), 存放hs的每个节点的参数\n    * syn1neg: theta for negtive sampling, 存放theta(u)\n    * vocab_size:词典大小。\n    * layer1_size:第一层大小，即向量映射的维度\n    * 末尾还会创建一棵哈弗曼树\n \n```\nvoid InitNet() {\n  long long a, b;\n  unsigned long long next_random = 1;\n  a = posix_memalign((void **)&syn0, 128, (long long)vocab_size * layer1_size * sizeof(real));\n  if (syn0 == NULL) {printf(\"Memory allocation failed\\n\"); exit(1);}\n  if (hs) {\n    a = posix_memalign((void **)&syn1, 128, (long long)vocab_size * layer1_size * sizeof(real));\n    if (syn1 == NULL) {printf(\"Memory allocation failed\\n\"); exit(1);}\n    for (a = 0; a < vocab_size; a++) for (b = 0; b < layer1_size; b++)\n     syn1[a * layer1_size + b] = 0;\n  }\n  if (negative>0) {\n    a = posix_memalign((void **)&syn1neg, 128, (long long)vocab_size * layer1_size * sizeof(real));\n    if (syn1neg == NULL) {printf(\"Memory allocation failed\\n\"); exit(1);}\n    for (a = 0; a < vocab_size; a++) for (b = 0; b < layer1_size; b++)\n     syn1neg[a * layer1_size + b] = 0;\n  }\n  for (a = 0; a < vocab_size; a++) for (b = 0; b < layer1_size; b++) {\n    next_random = next_random * (unsigned long long)25214903917 + 11;\n    syn0[a * layer1_size + b] = (((next_random & 0xFFFF) / (real)65536) - 0.5) / layer1_size;\n  }\n  CreateBinaryTree();\n}\n```\n* 初始化采样表，注意不是直接用次数，而是用pow(次数, 0.75)来做，降低一下高频词的采到的概率。\n```\nvoid InitUnigramTable() {\n  int a, i;\n  double train_words_pow = 0;\n  double d1, power = 0.75;\n  table = (int *)malloc(table_size * sizeof(int));\n  for (a = 0; a < vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);\n  i = 0;\n  d1 = pow(vocab[i].cn, power) / train_words_pow;\n  for (a = 0; a < table_size; a++) {\n    table[a] = i;\n    if (a / (double)table_size > d1) {\n      i++;\n      d1 += pow(vocab[i].cn, power) / train_words_pow;\n    }\n    if (i >= vocab_size) i = vocab_size - 1;\n  }\n}\n```\n* 构建哈弗曼树，自底向上构建，code指的是huffman tree的编码，point指向对应词在词典中的位置\n```\n// Create binary Huffman tree using the word counts\n// Frequent words will have short uniqe binary codes\nvoid CreateBinaryTree() {\n  long long a, b, i, min1i, min2i, pos1, pos2, point[MAX_CODE_LENGTH];\n  char code[MAX_CODE_LENGTH];\n  long long *count = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));\n  long long *binary = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));\n  long long *parent_node = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));\n  for (a = 0; a < vocab_size; a++) count[a] = vocab[a].cn;\n  for (a = vocab_size; a < vocab_size * 2; a++) count[a] = 1e15;\n  pos1 = vocab_size - 1;\n  pos2 = vocab_size;\n  // Following algorithm constructs the Huffman tree by adding one node at a time\n  for (a = 0; a < vocab_size - 1; a++) {\n    // First, find two smallest nodes 'min1, min2'\n    if (pos1 >= 0) {\n      if (count[pos1] < count[pos2]) {\n        min1i = pos1;\n        pos1--;\n      } else {\n        min1i = pos2;\n        pos2++;\n      }\n    } else {\n      min1i = pos2;\n      pos2++;\n    }\n    if (pos1 >= 0) {\n      if (count[pos1] < count[pos2]) {\n        min2i = pos1;\n        pos1--;\n      } else {\n        min2i = pos2;\n        pos2++;\n      }\n    } else {\n      min2i = pos2;\n      pos2++;\n    }\n    count[vocab_size + a] = count[min1i] + count[min2i];\n    parent_node[min1i] = vocab_size + a;\n    parent_node[min2i] = vocab_size + a;\n    binary[min2i] = 1;\n  }\n  // Now assign binary code to each vocabulary word\n  for (a = 0; a < vocab_size; a++) {\n    b = a;\n    i = 0;\n    while (1) {\n      code[i] = binary[b];\n      point[i] = b;\n      i++;\n      b = parent_node[b];\n      if (b == vocab_size * 2 - 2) break;\n    }\n    vocab[a].codelen = i;\n    vocab[a].point[0] = vocab_size - 2;\n    for (b = 0; b < i; b++) {\n      vocab[a].code[i - b - 1] = code[b];\n      vocab[a].point[i - b] = point[b] - vocab_size;\n    }\n  }\n  free(count);\n  free(binary);\n  free(parent_node);\n}\n```\n* 训练模型\n{% asset_img \"cbow-hs-2.png\" [cbow-hs-2] %}\n\n```\nif (cw) {\n        for (c = 0; c < layer1_size; c++) neu1[c] /= cw;\n        // 如果分层softmax, theta(j: 2->l, w)作为tree的路径参数\n        if (hs) for (d = 0; d < vocab[word].codelen; d++) {\n          f = 0;\n          l2 = vocab[word].point[d] * layer1_size;\n          // Propagate hidden -> output\n          // 3.1 查表计算sigma(x(w)*theta)\n          for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1[c + l2];\n          if (f <= -MAX_EXP) continue;\n          else if (f >= MAX_EXP) continue;\n          else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];\n          // 'g' is the gradient multiplied by the learning rate: 3.2 g = (1-dj-q) * eta\n          g = (1 - vocab[word].code[d] - f) * alpha;\n          // Propagate errors output -> hidden : 3.3 v = v + g*theta\n          for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];\n          // Learn weights hidden -> output: 3.4 theta = theta + g*x(w)\n          for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * neu1[c];\n        }\n```","source":"_posts/word2vec.md","raw":"---\ntitle: word2vec\ndate: 2018-06-13 22:34:28\ntags:\n---\n\n##### 起源和工具包\nword2vec是google在2013年的模型，可以有效的将词映射到某个向量空间，经典的demo是：\nvector('Paris') - vector('France') + vector('Italy') = vector('Rome')\n其开源地址是：https://code.google.com/archive/p/word2vec/\n此工具简单好用，在很多项目上起到了很好的效果\n\n##### word embedding的各种姿势\n* Topic Model：词袋模型，不分先后顺序，代表是LDA模型\n* Word2vec：等价于三层神经网络，能捕捉时序信息\n* FM：通过FM的weight矩阵将原始向量做变换\n* DSSM：通过点击数据把user和item的分别作Deep模型，取出来最顶层的做word representation\n\n另外NLP领域最新的模型是LSTM和Attention，能否利用其模型做embedding还需调研下\n* LSTM：能更好的捕捉时序信息，解决long dependence的问题\n* Attention：通过注意力机制\n\n##### word2vec的各种包\n* google word2vec：效果非常好，单机\n* gensim：经典的自然语言处理包，用的人很多，单机\n* spark mllib：不确定效果如何，分布式\n* tensorflow：未尝试，分布式\n（因为各种姿势都可以通过tensorflow来实现，包括FM，Word2Vec，以及各种矩阵分解，因此tf是很值得学习的工具。）\n\n##### Why word embedding？\n* 降维：原始ID维度的特征太大了，需要过多的训练数据，word embedding之后不需要那么多数据，而且防止过拟合\n\n##### embedding的两类方法\n* count-based：lsa系列，通过矩阵分解来做embedding\n* predictive-based：通过预测词可能出现的词来做embedding，如word2vec\n\n##### gensim建模+tensorboard可视化\n* 生成gensim训练数据，每行一个doc，空格分隔\n```\nselect list\nfrom\n(\n    select user_id, concat_ws(' ', collect_set(item_id)) as list\n    from dm_music_prd.t_7d_imusic_iting_user_item_action\n    where ds>=20180520 and action=1 and item_id != -1 and extend>1000*90\n    group by user_id\n) t1\njoin\n(\n    select user_id\n    from\n    (\n        select user_id, count(distinct item_id) as cnt\n        from dm_music_prd.t_7d_imusic_iting_user_item_action\n        where ds>=20180520 and action=1 and item_id != -1 and extend>1000*90\n        group by user_id\n    ) f\n    where cnt >= 5 and cnt <= 50\n) t2 on t1.user_id=t2.user_id\n```\n\n* gensim训练word2vec模型\n```\n/root/xpguo/anaconda3/bin/python w2v_v2.py user_songlist.v2 ./model_v2/w2v20180625 song_vector_v2\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport logging\nimport os\nimport sys\nimport multiprocessing\n\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\n\nif __name__ == '__main__':\n    program = os.path.basename(sys.argv[0])\n    logger = logging.getLogger(program)\n\n    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n    logging.root.setLevel(level=logging.INFO)\n    logger.info(\"running %s\" % ' '.join(sys.argv))\n\n    # check and process input arguments\n    if len(sys.argv) < 4:\n        print(globals()['__doc__'] % locals())\n        sys.exit(1)\n    inp, outp1, outp2 = sys.argv[1:4]\n\n    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5,\n                     workers=multiprocessing.cpu_count())\n\n    # trim unneeded model memory = use(much) less RAM\n    # model.init_sims(replace=True)\n    # w2v model\n    model.save(outp1)\n    # word vectors\n    model.wv.save_word2vec_format(outp2, binary=False)\n```\n\n* Tensorboard 可视化\n```\n1.\t拉取id2name\nmysql -h10.20.125.43 -umyshuju_r -p3KAjvBHaDB{gLE9H -e \"select third_id, name from music.t_song_info where third_id is not null and name is not null and length(third_id)>0 and length(name)>0\" > t_song_info\n\n2.\t写tensorboard模型\nimport sys, os\nfrom gensim.models import Word2Vec\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\nid2name = {}\n\n## 可视化函数\n## model: gensim模型地址\n## output_path: tf board转换模型地址\ndef visualize(model, output_path):\n    ## tf board转换模型名称，自定义\n    meta_file = \"w2x_metadata.tsv\"\n\n    ## 词向量个数*词向量维数\n    placeholder = np.zeros((len(model.wv.index2word), 400))\n\n    ## 读取ID2name文件\n    inFp = open(\"./t_song_info\", 'r')\n    while True:\n        line = inFp.readline()\n        if not line:\n            break\n        items = line.strip().split('\\t')\n        if len(items) != 2:\n            continue\n        id2name[items[0]] = items[1]\n    inFp.close()\n\n    ## 地址+名称拼接\n    with open(os.path.join(output_path,meta_file), 'wb') as file_metadata:\n\n        ## 对于每个词向量，写文件\n        for i, word in enumerate(model.wv.index2word):\n            placeholder[i] = model[word]\n            # temporary solution for https://github.com/tensorflow/tensorflow/issues/9094\n            if word == '':\n                print(\"Emply Line, should replecaed by any thing else, or will cause a bug of tensorboard\")\n                file_metadata.write(\"{0}\".format('<Empty Line>').encode('utf-8') + b'\\n')\n            else:\n#                file_metadata.write(\"{0}\".format(word).encode('utf-8') + b'\\n')\n                file_metadata.write(\"{0}\".format(id2name.get(word, 'null')).encode('utf-8') + b'\\n')\n\n\n    # define the model without training\n    sess = tf.InteractiveSession()\n\n    embedding = tf.Variable(placeholder, trainable = False, name = 'w2x_metadata')\n    tf.global_variables_initializer().run()\n\n    saver = tf.train.Saver()\n    writer = tf.summary.FileWriter(output_path, sess.graph)\n\n    # adding into projector\n    config = projector.ProjectorConfig()\n    embed = config.embeddings.add()\n    embed.tensor_name = 'w2x_metadata'\n    embed.metadata_path = meta_file\n\n    # Specify the width and height of a single thumbnail.\n    projector.visualize_embeddings(writer, config)\n    saver.save(sess, os.path.join(output_path,'w2x_metadata.ckpt'))\n    print('Run `tensorboard --logdir={0}` to run visualize result on tensorboard'.format(output_path))\n\nif __name__ == \"__main__\":\n    model = Word2Vec.load(\"/home/xpguo/gensim/word2vec/song_vector_v1/a\")\nvisualize(model,\"/home/xpguo/gensim/word2vec/song_vector_v1_tf_board\")\n\n3.\ttensorboard可视化\ntensorboard --logdir=/home/xpguo/gensim/word2vec/song_vector_v2_tf_board --port=6607\n```\n\n* Tensorboard 可视化效果\n{% asset_img \"tfboard.png\" [tf-board] %}\n\n##### word2vec源码\n*　cbow-框架图\n{% asset_img \"cbow-1.png\" [cbow-1] %}\n\n* cbow-Hierarchical softmax\n{% asset_img \"cbow-hs.png\" [cbow-hs] %}\n\n* cbow-hs-hand\n{% asset_img \"cbow-hs-hand.png\" [cbow-hs-hand.png] %}\n\n##### word2vec源码细节\n* 预计算sigmoid(x)，将[-6, 6]的区间划分成1000份来计算。\n```\n#define EXP_TABLE_SIZE 1000\n#define MAX_EXP 6\nexpTable = (real *)malloc((EXP_TABLE_SIZE + 1) * sizeof(real));\nfor (i = 0; i < EXP_TABLE_SIZE; i++) {\n  expTable[i] = exp((i / (real)EXP_TABLE_SIZE * 2 - 1) * MAX_EXP); // Precompute the exp() table\n  expTable[i] = expTable[i] / (expTable[i] + 1);                   // Precompute f(x) = x / (x + 1)\n}\n```\n* hash函数，每个char+原hash值*257，很常见的做法\n```\n// Returns hash value of a word\nint GetWordHash(char *word) {\n  unsigned long long a, hash = 0;\n  for (a = 0; a < strlen(word); a++) hash = hash * 257 + word[a];\n  hash = hash % vocab_hash_size;\n  return hash;\n}\n```\n* 线性探测法，hash后线性地去找词，要么找到返回词的位置，要么找到-1\n```\n// Returns position of a word in the vocabulary; if the word is not found, returns -1\nint SearchVocab(char *word) {\n  unsigned int hash = GetWordHash(word);\n  while (1) {\n    if (vocab_hash[hash] == -1) return -1;\n    if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash];\n    hash = (hash + 1) % vocab_hash_size;\n  }\n  return -1;\n}\n```\n* 将词添加到词典中， 有两个地方要加，第一是hash映射表，第二是词对应的count表\n```\n// Adds a word to the vocabulary\nint AddWordToVocab(char *word) {\n  unsigned int hash, length = strlen(word) + 1;\n  if (length > MAX_STRING) length = MAX_STRING;\n  vocab[vocab_size].word = (char *)calloc(length, sizeof(char));\n  strcpy(vocab[vocab_size].word, word);\n  vocab[vocab_size].cn = 0;\n  vocab_size++;\n  // Reallocate memory if needed\n  if (vocab_size + 2 >= vocab_max_size) {\n    vocab_max_size += 1000;\n    vocab = (struct vocab_word *)realloc(vocab, vocab_max_size * sizeof(struct vocab_word));\n  }\n  hash = GetWordHash(word);\n  while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;\n  vocab_hash[hash] = vocab_size - 1;\n  return vocab_size - 1;\n}\n```\n* 将词典排序，目的是为了建立哈弗曼树，有几个地方要注意：一是出现频率比较低的词，会被过滤掉；二是过滤掉之后需要讲hashtable重新hash，因为去掉了一些词，table不能用了。感慨一下，c语言要自己实现hash_dict，真心麻烦。\n```\n// Sorts the vocabulary by frequency using word counts\nvoid SortVocab() {\n  int a, size;\n  unsigned int hash;\n  // Sort the vocabulary and keep </s> at the first position\n  qsort(&vocab[1], vocab_size - 1, sizeof(struct vocab_word), VocabCompare);\n  for (a = 0; a < vocab_hash_size; a++) vocab_hash[a] = -1;\n  size = vocab_size;\n  train_words = 0;\n  for (a = 0; a < size; a++) {\n    // Words occuring less than min_count times will be discarded from the vocab\n    if ((vocab[a].cn < min_count) && (a != 0)) {\n      vocab_size--;\n      free(vocab[a].word);\n    } else {\n      // Hash will be re-computed, as after the sorting it is not actual\n      hash=GetWordHash(vocab[a].word);\n      while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;\n      vocab_hash[hash] = a;\n      train_words += vocab[a].cn;\n    }\n  }\n  vocab = (struct vocab_word *)realloc(vocab, (vocab_size + 1) * sizeof(struct vocab_word));\n  // Allocate memory for the binary tree construction\n  for (a = 0; a < vocab_size; a++) {\n    vocab[a].code = (char *)calloc(MAX_CODE_LENGTH, sizeof(char));\n    vocab[a].point = (int *)calloc(MAX_CODE_LENGTH, sizeof(int));\n  }\n}\n```\n* 初始化网络参数，首先要搞清楚网络是什么样子的：\n    * syn0: e(w), 向量映射表，最终看的结果\n    * syn1: theta(w, j), 存放hs的每个节点的参数\n    * syn1neg: theta for negtive sampling, 存放theta(u)\n    * vocab_size:词典大小。\n    * layer1_size:第一层大小，即向量映射的维度\n    * 末尾还会创建一棵哈弗曼树\n \n```\nvoid InitNet() {\n  long long a, b;\n  unsigned long long next_random = 1;\n  a = posix_memalign((void **)&syn0, 128, (long long)vocab_size * layer1_size * sizeof(real));\n  if (syn0 == NULL) {printf(\"Memory allocation failed\\n\"); exit(1);}\n  if (hs) {\n    a = posix_memalign((void **)&syn1, 128, (long long)vocab_size * layer1_size * sizeof(real));\n    if (syn1 == NULL) {printf(\"Memory allocation failed\\n\"); exit(1);}\n    for (a = 0; a < vocab_size; a++) for (b = 0; b < layer1_size; b++)\n     syn1[a * layer1_size + b] = 0;\n  }\n  if (negative>0) {\n    a = posix_memalign((void **)&syn1neg, 128, (long long)vocab_size * layer1_size * sizeof(real));\n    if (syn1neg == NULL) {printf(\"Memory allocation failed\\n\"); exit(1);}\n    for (a = 0; a < vocab_size; a++) for (b = 0; b < layer1_size; b++)\n     syn1neg[a * layer1_size + b] = 0;\n  }\n  for (a = 0; a < vocab_size; a++) for (b = 0; b < layer1_size; b++) {\n    next_random = next_random * (unsigned long long)25214903917 + 11;\n    syn0[a * layer1_size + b] = (((next_random & 0xFFFF) / (real)65536) - 0.5) / layer1_size;\n  }\n  CreateBinaryTree();\n}\n```\n* 初始化采样表，注意不是直接用次数，而是用pow(次数, 0.75)来做，降低一下高频词的采到的概率。\n```\nvoid InitUnigramTable() {\n  int a, i;\n  double train_words_pow = 0;\n  double d1, power = 0.75;\n  table = (int *)malloc(table_size * sizeof(int));\n  for (a = 0; a < vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);\n  i = 0;\n  d1 = pow(vocab[i].cn, power) / train_words_pow;\n  for (a = 0; a < table_size; a++) {\n    table[a] = i;\n    if (a / (double)table_size > d1) {\n      i++;\n      d1 += pow(vocab[i].cn, power) / train_words_pow;\n    }\n    if (i >= vocab_size) i = vocab_size - 1;\n  }\n}\n```\n* 构建哈弗曼树，自底向上构建，code指的是huffman tree的编码，point指向对应词在词典中的位置\n```\n// Create binary Huffman tree using the word counts\n// Frequent words will have short uniqe binary codes\nvoid CreateBinaryTree() {\n  long long a, b, i, min1i, min2i, pos1, pos2, point[MAX_CODE_LENGTH];\n  char code[MAX_CODE_LENGTH];\n  long long *count = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));\n  long long *binary = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));\n  long long *parent_node = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));\n  for (a = 0; a < vocab_size; a++) count[a] = vocab[a].cn;\n  for (a = vocab_size; a < vocab_size * 2; a++) count[a] = 1e15;\n  pos1 = vocab_size - 1;\n  pos2 = vocab_size;\n  // Following algorithm constructs the Huffman tree by adding one node at a time\n  for (a = 0; a < vocab_size - 1; a++) {\n    // First, find two smallest nodes 'min1, min2'\n    if (pos1 >= 0) {\n      if (count[pos1] < count[pos2]) {\n        min1i = pos1;\n        pos1--;\n      } else {\n        min1i = pos2;\n        pos2++;\n      }\n    } else {\n      min1i = pos2;\n      pos2++;\n    }\n    if (pos1 >= 0) {\n      if (count[pos1] < count[pos2]) {\n        min2i = pos1;\n        pos1--;\n      } else {\n        min2i = pos2;\n        pos2++;\n      }\n    } else {\n      min2i = pos2;\n      pos2++;\n    }\n    count[vocab_size + a] = count[min1i] + count[min2i];\n    parent_node[min1i] = vocab_size + a;\n    parent_node[min2i] = vocab_size + a;\n    binary[min2i] = 1;\n  }\n  // Now assign binary code to each vocabulary word\n  for (a = 0; a < vocab_size; a++) {\n    b = a;\n    i = 0;\n    while (1) {\n      code[i] = binary[b];\n      point[i] = b;\n      i++;\n      b = parent_node[b];\n      if (b == vocab_size * 2 - 2) break;\n    }\n    vocab[a].codelen = i;\n    vocab[a].point[0] = vocab_size - 2;\n    for (b = 0; b < i; b++) {\n      vocab[a].code[i - b - 1] = code[b];\n      vocab[a].point[i - b] = point[b] - vocab_size;\n    }\n  }\n  free(count);\n  free(binary);\n  free(parent_node);\n}\n```\n* 训练模型\n{% asset_img \"cbow-hs-2.png\" [cbow-hs-2] %}\n\n```\nif (cw) {\n        for (c = 0; c < layer1_size; c++) neu1[c] /= cw;\n        // 如果分层softmax, theta(j: 2->l, w)作为tree的路径参数\n        if (hs) for (d = 0; d < vocab[word].codelen; d++) {\n          f = 0;\n          l2 = vocab[word].point[d] * layer1_size;\n          // Propagate hidden -> output\n          // 3.1 查表计算sigma(x(w)*theta)\n          for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1[c + l2];\n          if (f <= -MAX_EXP) continue;\n          else if (f >= MAX_EXP) continue;\n          else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];\n          // 'g' is the gradient multiplied by the learning rate: 3.2 g = (1-dj-q) * eta\n          g = (1 - vocab[word].code[d] - f) * alpha;\n          // Propagate errors output -> hidden : 3.3 v = v + g*theta\n          for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];\n          // Learn weights hidden -> output: 3.4 theta = theta + g*x(w)\n          for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * neu1[c];\n        }\n```","slug":"word2vec","published":1,"updated":"2018-11-05T02:27:05.614Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gbl0019tcin5kl8ci0s","content":"<h5 id=\"起源和工具包\"><a href=\"#起源和工具包\" class=\"headerlink\" title=\"起源和工具包\"></a>起源和工具包</h5><p>word2vec是google在2013年的模型，可以有效的将词映射到某个向量空间，经典的demo是：<br>vector(‘Paris’) - vector(‘France’) + vector(‘Italy’) = vector(‘Rome’)<br>其开源地址是：<a href=\"https://code.google.com/archive/p/word2vec/\" target=\"_blank\" rel=\"external\">https://code.google.com/archive/p/word2vec/</a><br>此工具简单好用，在很多项目上起到了很好的效果</p>\n<h5 id=\"word-embedding的各种姿势\"><a href=\"#word-embedding的各种姿势\" class=\"headerlink\" title=\"word embedding的各种姿势\"></a>word embedding的各种姿势</h5><ul>\n<li>Topic Model：词袋模型，不分先后顺序，代表是LDA模型</li>\n<li>Word2vec：等价于三层神经网络，能捕捉时序信息</li>\n<li>FM：通过FM的weight矩阵将原始向量做变换</li>\n<li>DSSM：通过点击数据把user和item的分别作Deep模型，取出来最顶层的做word representation</li>\n</ul>\n<p>另外NLP领域最新的模型是LSTM和Attention，能否利用其模型做embedding还需调研下</p>\n<ul>\n<li>LSTM：能更好的捕捉时序信息，解决long dependence的问题</li>\n<li>Attention：通过注意力机制</li>\n</ul>\n<h5 id=\"word2vec的各种包\"><a href=\"#word2vec的各种包\" class=\"headerlink\" title=\"word2vec的各种包\"></a>word2vec的各种包</h5><ul>\n<li>google word2vec：效果非常好，单机</li>\n<li>gensim：经典的自然语言处理包，用的人很多，单机</li>\n<li>spark mllib：不确定效果如何，分布式</li>\n<li>tensorflow：未尝试，分布式<br>（因为各种姿势都可以通过tensorflow来实现，包括FM，Word2Vec，以及各种矩阵分解，因此tf是很值得学习的工具。）</li>\n</ul>\n<h5 id=\"Why-word-embedding？\"><a href=\"#Why-word-embedding？\" class=\"headerlink\" title=\"Why word embedding？\"></a>Why word embedding？</h5><ul>\n<li>降维：原始ID维度的特征太大了，需要过多的训练数据，word embedding之后不需要那么多数据，而且防止过拟合</li>\n</ul>\n<h5 id=\"embedding的两类方法\"><a href=\"#embedding的两类方法\" class=\"headerlink\" title=\"embedding的两类方法\"></a>embedding的两类方法</h5><ul>\n<li>count-based：lsa系列，通过矩阵分解来做embedding</li>\n<li>predictive-based：通过预测词可能出现的词来做embedding，如word2vec</li>\n</ul>\n<h5 id=\"gensim建模-tensorboard可视化\"><a href=\"#gensim建模-tensorboard可视化\" class=\"headerlink\" title=\"gensim建模+tensorboard可视化\"></a>gensim建模+tensorboard可视化</h5><ul>\n<li><p>生成gensim训练数据，每行一个doc，空格分隔</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">select list</div><div class=\"line\">from</div><div class=\"line\">(</div><div class=\"line\">    select user_id, concat_ws(&apos; &apos;, collect_set(item_id)) as list</div><div class=\"line\">    from dm_music_prd.t_7d_imusic_iting_user_item_action</div><div class=\"line\">    where ds&gt;=20180520 and action=1 and item_id != -1 and extend&gt;1000*90</div><div class=\"line\">    group by user_id</div><div class=\"line\">) t1</div><div class=\"line\">join</div><div class=\"line\">(</div><div class=\"line\">    select user_id</div><div class=\"line\">    from</div><div class=\"line\">    (</div><div class=\"line\">        select user_id, count(distinct item_id) as cnt</div><div class=\"line\">        from dm_music_prd.t_7d_imusic_iting_user_item_action</div><div class=\"line\">        where ds&gt;=20180520 and action=1 and item_id != -1 and extend&gt;1000*90</div><div class=\"line\">        group by user_id</div><div class=\"line\">    ) f</div><div class=\"line\">    where cnt &gt;= 5 and cnt &lt;= 50</div><div class=\"line\">) t2 on t1.user_id=t2.user_id</div></pre></td></tr></table></figure>\n</li>\n<li><p>gensim训练word2vec模型</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div></pre></td><td class=\"code\"><pre><div class=\"line\">/root/xpguo/anaconda3/bin/python w2v_v2.py user_songlist.v2 ./model_v2/w2v20180625 song_vector_v2</div><div class=\"line\"></div><div class=\"line\">#!/usr/bin/env python</div><div class=\"line\"># -*- coding: utf-8 -*-</div><div class=\"line\"></div><div class=\"line\">import logging</div><div class=\"line\">import os</div><div class=\"line\">import sys</div><div class=\"line\">import multiprocessing</div><div class=\"line\"></div><div class=\"line\">from gensim.models import Word2Vec</div><div class=\"line\">from gensim.models.word2vec import LineSentence</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    program = os.path.basename(sys.argv[0])</div><div class=\"line\">    logger = logging.getLogger(program)</div><div class=\"line\"></div><div class=\"line\">    logging.basicConfig(format=&apos;%(asctime)s: %(levelname)s: %(message)s&apos;)</div><div class=\"line\">    logging.root.setLevel(level=logging.INFO)</div><div class=\"line\">    logger.info(&quot;running %s&quot; % &apos; &apos;.join(sys.argv))</div><div class=\"line\"></div><div class=\"line\">    # check and process input arguments</div><div class=\"line\">    if len(sys.argv) &lt; 4:</div><div class=\"line\">        print(globals()[&apos;__doc__&apos;] % locals())</div><div class=\"line\">        sys.exit(1)</div><div class=\"line\">    inp, outp1, outp2 = sys.argv[1:4]</div><div class=\"line\"></div><div class=\"line\">    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5,</div><div class=\"line\">                     workers=multiprocessing.cpu_count())</div><div class=\"line\"></div><div class=\"line\">    # trim unneeded model memory = use(much) less RAM</div><div class=\"line\">    # model.init_sims(replace=True)</div><div class=\"line\">    # w2v model</div><div class=\"line\">    model.save(outp1)</div><div class=\"line\">    # word vectors</div><div class=\"line\">    model.wv.save_word2vec_format(outp2, binary=False)</div></pre></td></tr></table></figure>\n</li>\n<li><p>Tensorboard 可视化</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div></pre></td><td class=\"code\"><pre><div class=\"line\">1.\t拉取id2name</div><div class=\"line\">mysql -h10.20.125.43 -umyshuju_r -p3KAjvBHaDB&#123;gLE9H -e &quot;select third_id, name from music.t_song_info where third_id is not null and name is not null and length(third_id)&gt;0 and length(name)&gt;0&quot; &gt; t_song_info</div><div class=\"line\"></div><div class=\"line\">2.\t写tensorboard模型</div><div class=\"line\">import sys, os</div><div class=\"line\">from gensim.models import Word2Vec</div><div class=\"line\">import tensorflow as tf</div><div class=\"line\">import numpy as np</div><div class=\"line\">from tensorflow.contrib.tensorboard.plugins import projector</div><div class=\"line\"></div><div class=\"line\">id2name = &#123;&#125;</div><div class=\"line\"></div><div class=\"line\">## 可视化函数</div><div class=\"line\">## model: gensim模型地址</div><div class=\"line\">## output_path: tf board转换模型地址</div><div class=\"line\">def visualize(model, output_path):</div><div class=\"line\">    ## tf board转换模型名称，自定义</div><div class=\"line\">    meta_file = &quot;w2x_metadata.tsv&quot;</div><div class=\"line\"></div><div class=\"line\">    ## 词向量个数*词向量维数</div><div class=\"line\">    placeholder = np.zeros((len(model.wv.index2word), 400))</div><div class=\"line\"></div><div class=\"line\">    ## 读取ID2name文件</div><div class=\"line\">    inFp = open(&quot;./t_song_info&quot;, &apos;r&apos;)</div><div class=\"line\">    while True:</div><div class=\"line\">        line = inFp.readline()</div><div class=\"line\">        if not line:</div><div class=\"line\">            break</div><div class=\"line\">        items = line.strip().split(&apos;\\t&apos;)</div><div class=\"line\">        if len(items) != 2:</div><div class=\"line\">            continue</div><div class=\"line\">        id2name[items[0]] = items[1]</div><div class=\"line\">    inFp.close()</div><div class=\"line\"></div><div class=\"line\">    ## 地址+名称拼接</div><div class=\"line\">    with open(os.path.join(output_path,meta_file), &apos;wb&apos;) as file_metadata:</div><div class=\"line\"></div><div class=\"line\">        ## 对于每个词向量，写文件</div><div class=\"line\">        for i, word in enumerate(model.wv.index2word):</div><div class=\"line\">            placeholder[i] = model[word]</div><div class=\"line\">            # temporary solution for https://github.com/tensorflow/tensorflow/issues/9094</div><div class=\"line\">            if word == &apos;&apos;:</div><div class=\"line\">                print(&quot;Emply Line, should replecaed by any thing else, or will cause a bug of tensorboard&quot;)</div><div class=\"line\">                file_metadata.write(&quot;&#123;0&#125;&quot;.format(&apos;&lt;Empty Line&gt;&apos;).encode(&apos;utf-8&apos;) + b&apos;\\n&apos;)</div><div class=\"line\">            else:</div><div class=\"line\">#                file_metadata.write(&quot;&#123;0&#125;&quot;.format(word).encode(&apos;utf-8&apos;) + b&apos;\\n&apos;)</div><div class=\"line\">                file_metadata.write(&quot;&#123;0&#125;&quot;.format(id2name.get(word, &apos;null&apos;)).encode(&apos;utf-8&apos;) + b&apos;\\n&apos;)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    # define the model without training</div><div class=\"line\">    sess = tf.InteractiveSession()</div><div class=\"line\"></div><div class=\"line\">    embedding = tf.Variable(placeholder, trainable = False, name = &apos;w2x_metadata&apos;)</div><div class=\"line\">    tf.global_variables_initializer().run()</div><div class=\"line\"></div><div class=\"line\">    saver = tf.train.Saver()</div><div class=\"line\">    writer = tf.summary.FileWriter(output_path, sess.graph)</div><div class=\"line\"></div><div class=\"line\">    # adding into projector</div><div class=\"line\">    config = projector.ProjectorConfig()</div><div class=\"line\">    embed = config.embeddings.add()</div><div class=\"line\">    embed.tensor_name = &apos;w2x_metadata&apos;</div><div class=\"line\">    embed.metadata_path = meta_file</div><div class=\"line\"></div><div class=\"line\">    # Specify the width and height of a single thumbnail.</div><div class=\"line\">    projector.visualize_embeddings(writer, config)</div><div class=\"line\">    saver.save(sess, os.path.join(output_path,&apos;w2x_metadata.ckpt&apos;))</div><div class=\"line\">    print(&apos;Run `tensorboard --logdir=&#123;0&#125;` to run visualize result on tensorboard&apos;.format(output_path))</div><div class=\"line\"></div><div class=\"line\">if __name__ == &quot;__main__&quot;:</div><div class=\"line\">    model = Word2Vec.load(&quot;/home/xpguo/gensim/word2vec/song_vector_v1/a&quot;)</div><div class=\"line\">visualize(model,&quot;/home/xpguo/gensim/word2vec/song_vector_v1_tf_board&quot;)</div><div class=\"line\"></div><div class=\"line\">3.\ttensorboard可视化</div><div class=\"line\">tensorboard --logdir=/home/xpguo/gensim/word2vec/song_vector_v2_tf_board --port=6607</div></pre></td></tr></table></figure>\n</li>\n<li><p>Tensorboard 可视化效果</p>\n<img src=\"/2018/06/13/word2vec/tfboard.png\" alt=\"[tf-board]\" title=\"[tf-board]\">\n</li>\n</ul>\n<h5 id=\"word2vec源码\"><a href=\"#word2vec源码\" class=\"headerlink\" title=\"word2vec源码\"></a>word2vec源码</h5><p>*　cbow-框架图<br><img src=\"/2018/06/13/word2vec/cbow-1.png\" alt=\"[cbow-1]\" title=\"[cbow-1]\"></p>\n<ul>\n<li><p>cbow-Hierarchical softmax</p>\n<img src=\"/2018/06/13/word2vec/cbow-hs.png\" alt=\"[cbow-hs]\" title=\"[cbow-hs]\">\n</li>\n<li><p>cbow-hs-hand</p>\n<img src=\"/2018/06/13/word2vec/cbow-hs-hand.png\" alt=\"[cbow-hs-hand.png]\" title=\"[cbow-hs-hand.png]\">\n</li>\n</ul>\n<h5 id=\"word2vec源码细节\"><a href=\"#word2vec源码细节\" class=\"headerlink\" title=\"word2vec源码细节\"></a>word2vec源码细节</h5><ul>\n<li><p>预计算sigmoid(x)，将[-6, 6]的区间划分成1000份来计算。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">#define EXP_TABLE_SIZE 1000</div><div class=\"line\">#define MAX_EXP 6</div><div class=\"line\">expTable = (real *)malloc((EXP_TABLE_SIZE + 1) * sizeof(real));</div><div class=\"line\">for (i = 0; i &lt; EXP_TABLE_SIZE; i++) &#123;</div><div class=\"line\">  expTable[i] = exp((i / (real)EXP_TABLE_SIZE * 2 - 1) * MAX_EXP); // Precompute the exp() table</div><div class=\"line\">  expTable[i] = expTable[i] / (expTable[i] + 1);                   // Precompute f(x) = x / (x + 1)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>hash函数，每个char+原hash值*257，很常见的做法</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Returns hash value of a word</div><div class=\"line\">int GetWordHash(char *word) &#123;</div><div class=\"line\">  unsigned long long a, hash = 0;</div><div class=\"line\">  for (a = 0; a &lt; strlen(word); a++) hash = hash * 257 + word[a];</div><div class=\"line\">  hash = hash % vocab_hash_size;</div><div class=\"line\">  return hash;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>线性探测法，hash后线性地去找词，要么找到返回词的位置，要么找到-1</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Returns position of a word in the vocabulary; if the word is not found, returns -1</div><div class=\"line\">int SearchVocab(char *word) &#123;</div><div class=\"line\">  unsigned int hash = GetWordHash(word);</div><div class=\"line\">  while (1) &#123;</div><div class=\"line\">    if (vocab_hash[hash] == -1) return -1;</div><div class=\"line\">    if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash];</div><div class=\"line\">    hash = (hash + 1) % vocab_hash_size;</div><div class=\"line\">  &#125;</div><div class=\"line\">  return -1;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>将词添加到词典中， 有两个地方要加，第一是hash映射表，第二是词对应的count表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Adds a word to the vocabulary</div><div class=\"line\">int AddWordToVocab(char *word) &#123;</div><div class=\"line\">  unsigned int hash, length = strlen(word) + 1;</div><div class=\"line\">  if (length &gt; MAX_STRING) length = MAX_STRING;</div><div class=\"line\">  vocab[vocab_size].word = (char *)calloc(length, sizeof(char));</div><div class=\"line\">  strcpy(vocab[vocab_size].word, word);</div><div class=\"line\">  vocab[vocab_size].cn = 0;</div><div class=\"line\">  vocab_size++;</div><div class=\"line\">  // Reallocate memory if needed</div><div class=\"line\">  if (vocab_size + 2 &gt;= vocab_max_size) &#123;</div><div class=\"line\">    vocab_max_size += 1000;</div><div class=\"line\">    vocab = (struct vocab_word *)realloc(vocab, vocab_max_size * sizeof(struct vocab_word));</div><div class=\"line\">  &#125;</div><div class=\"line\">  hash = GetWordHash(word);</div><div class=\"line\">  while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;</div><div class=\"line\">  vocab_hash[hash] = vocab_size - 1;</div><div class=\"line\">  return vocab_size - 1;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>将词典排序，目的是为了建立哈弗曼树，有几个地方要注意：一是出现频率比较低的词，会被过滤掉；二是过滤掉之后需要讲hashtable重新hash，因为去掉了一些词，table不能用了。感慨一下，c语言要自己实现hash_dict，真心麻烦。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Sorts the vocabulary by frequency using word counts</div><div class=\"line\">void SortVocab() &#123;</div><div class=\"line\">  int a, size;</div><div class=\"line\">  unsigned int hash;</div><div class=\"line\">  // Sort the vocabulary and keep &lt;/s&gt; at the first position</div><div class=\"line\">  qsort(&amp;vocab[1], vocab_size - 1, sizeof(struct vocab_word), VocabCompare);</div><div class=\"line\">  for (a = 0; a &lt; vocab_hash_size; a++) vocab_hash[a] = -1;</div><div class=\"line\">  size = vocab_size;</div><div class=\"line\">  train_words = 0;</div><div class=\"line\">  for (a = 0; a &lt; size; a++) &#123;</div><div class=\"line\">    // Words occuring less than min_count times will be discarded from the vocab</div><div class=\"line\">    if ((vocab[a].cn &lt; min_count) &amp;&amp; (a != 0)) &#123;</div><div class=\"line\">      vocab_size--;</div><div class=\"line\">      free(vocab[a].word);</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      // Hash will be re-computed, as after the sorting it is not actual</div><div class=\"line\">      hash=GetWordHash(vocab[a].word);</div><div class=\"line\">      while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;</div><div class=\"line\">      vocab_hash[hash] = a;</div><div class=\"line\">      train_words += vocab[a].cn;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  vocab = (struct vocab_word *)realloc(vocab, (vocab_size + 1) * sizeof(struct vocab_word));</div><div class=\"line\">  // Allocate memory for the binary tree construction</div><div class=\"line\">  for (a = 0; a &lt; vocab_size; a++) &#123;</div><div class=\"line\">    vocab[a].code = (char *)calloc(MAX_CODE_LENGTH, sizeof(char));</div><div class=\"line\">    vocab[a].point = (int *)calloc(MAX_CODE_LENGTH, sizeof(int));</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>初始化网络参数，首先要搞清楚网络是什么样子的：</p>\n<ul>\n<li>syn0: e(w), 向量映射表，最终看的结果</li>\n<li>syn1: theta(w, j), 存放hs的每个节点的参数</li>\n<li>syn1neg: theta for negtive sampling, 存放theta(u)</li>\n<li>vocab_size:词典大小。</li>\n<li>layer1_size:第一层大小，即向量映射的维度</li>\n<li>末尾还会创建一棵哈弗曼树</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">void InitNet() &#123;</div><div class=\"line\">  long long a, b;</div><div class=\"line\">  unsigned long long next_random = 1;</div><div class=\"line\">  a = posix_memalign((void **)&amp;syn0, 128, (long long)vocab_size * layer1_size * sizeof(real));</div><div class=\"line\">  if (syn0 == NULL) &#123;printf(&quot;Memory allocation failed\\n&quot;); exit(1);&#125;</div><div class=\"line\">  if (hs) &#123;</div><div class=\"line\">    a = posix_memalign((void **)&amp;syn1, 128, (long long)vocab_size * layer1_size * sizeof(real));</div><div class=\"line\">    if (syn1 == NULL) &#123;printf(&quot;Memory allocation failed\\n&quot;); exit(1);&#125;</div><div class=\"line\">    for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++)</div><div class=\"line\">     syn1[a * layer1_size + b] = 0;</div><div class=\"line\">  &#125;</div><div class=\"line\">  if (negative&gt;0) &#123;</div><div class=\"line\">    a = posix_memalign((void **)&amp;syn1neg, 128, (long long)vocab_size * layer1_size * sizeof(real));</div><div class=\"line\">    if (syn1neg == NULL) &#123;printf(&quot;Memory allocation failed\\n&quot;); exit(1);&#125;</div><div class=\"line\">    for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++)</div><div class=\"line\">     syn1neg[a * layer1_size + b] = 0;</div><div class=\"line\">  &#125;</div><div class=\"line\">  for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) &#123;</div><div class=\"line\">    next_random = next_random * (unsigned long long)25214903917 + 11;</div><div class=\"line\">    syn0[a * layer1_size + b] = (((next_random &amp; 0xFFFF) / (real)65536) - 0.5) / layer1_size;</div><div class=\"line\">  &#125;</div><div class=\"line\">  CreateBinaryTree();</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li><p>初始化采样表，注意不是直接用次数，而是用pow(次数, 0.75)来做，降低一下高频词的采到的概率。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">void InitUnigramTable() &#123;</div><div class=\"line\">  int a, i;</div><div class=\"line\">  double train_words_pow = 0;</div><div class=\"line\">  double d1, power = 0.75;</div><div class=\"line\">  table = (int *)malloc(table_size * sizeof(int));</div><div class=\"line\">  for (a = 0; a &lt; vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);</div><div class=\"line\">  i = 0;</div><div class=\"line\">  d1 = pow(vocab[i].cn, power) / train_words_pow;</div><div class=\"line\">  for (a = 0; a &lt; table_size; a++) &#123;</div><div class=\"line\">    table[a] = i;</div><div class=\"line\">    if (a / (double)table_size &gt; d1) &#123;</div><div class=\"line\">      i++;</div><div class=\"line\">      d1 += pow(vocab[i].cn, power) / train_words_pow;</div><div class=\"line\">    &#125;</div><div class=\"line\">    if (i &gt;= vocab_size) i = vocab_size - 1;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>构建哈弗曼树，自底向上构建，code指的是huffman tree的编码，point指向对应词在词典中的位置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Create binary Huffman tree using the word counts</div><div class=\"line\">// Frequent words will have short uniqe binary codes</div><div class=\"line\">void CreateBinaryTree() &#123;</div><div class=\"line\">  long long a, b, i, min1i, min2i, pos1, pos2, point[MAX_CODE_LENGTH];</div><div class=\"line\">  char code[MAX_CODE_LENGTH];</div><div class=\"line\">  long long *count = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</div><div class=\"line\">  long long *binary = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</div><div class=\"line\">  long long *parent_node = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</div><div class=\"line\">  for (a = 0; a &lt; vocab_size; a++) count[a] = vocab[a].cn;</div><div class=\"line\">  for (a = vocab_size; a &lt; vocab_size * 2; a++) count[a] = 1e15;</div><div class=\"line\">  pos1 = vocab_size - 1;</div><div class=\"line\">  pos2 = vocab_size;</div><div class=\"line\">  // Following algorithm constructs the Huffman tree by adding one node at a time</div><div class=\"line\">  for (a = 0; a &lt; vocab_size - 1; a++) &#123;</div><div class=\"line\">    // First, find two smallest nodes &apos;min1, min2&apos;</div><div class=\"line\">    if (pos1 &gt;= 0) &#123;</div><div class=\"line\">      if (count[pos1] &lt; count[pos2]) &#123;</div><div class=\"line\">        min1i = pos1;</div><div class=\"line\">        pos1--;</div><div class=\"line\">      &#125; else &#123;</div><div class=\"line\">        min1i = pos2;</div><div class=\"line\">        pos2++;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      min1i = pos2;</div><div class=\"line\">      pos2++;</div><div class=\"line\">    &#125;</div><div class=\"line\">    if (pos1 &gt;= 0) &#123;</div><div class=\"line\">      if (count[pos1] &lt; count[pos2]) &#123;</div><div class=\"line\">        min2i = pos1;</div><div class=\"line\">        pos1--;</div><div class=\"line\">      &#125; else &#123;</div><div class=\"line\">        min2i = pos2;</div><div class=\"line\">        pos2++;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      min2i = pos2;</div><div class=\"line\">      pos2++;</div><div class=\"line\">    &#125;</div><div class=\"line\">    count[vocab_size + a] = count[min1i] + count[min2i];</div><div class=\"line\">    parent_node[min1i] = vocab_size + a;</div><div class=\"line\">    parent_node[min2i] = vocab_size + a;</div><div class=\"line\">    binary[min2i] = 1;</div><div class=\"line\">  &#125;</div><div class=\"line\">  // Now assign binary code to each vocabulary word</div><div class=\"line\">  for (a = 0; a &lt; vocab_size; a++) &#123;</div><div class=\"line\">    b = a;</div><div class=\"line\">    i = 0;</div><div class=\"line\">    while (1) &#123;</div><div class=\"line\">      code[i] = binary[b];</div><div class=\"line\">      point[i] = b;</div><div class=\"line\">      i++;</div><div class=\"line\">      b = parent_node[b];</div><div class=\"line\">      if (b == vocab_size * 2 - 2) break;</div><div class=\"line\">    &#125;</div><div class=\"line\">    vocab[a].codelen = i;</div><div class=\"line\">    vocab[a].point[0] = vocab_size - 2;</div><div class=\"line\">    for (b = 0; b &lt; i; b++) &#123;</div><div class=\"line\">      vocab[a].code[i - b - 1] = code[b];</div><div class=\"line\">      vocab[a].point[i - b] = point[b] - vocab_size;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  free(count);</div><div class=\"line\">  free(binary);</div><div class=\"line\">  free(parent_node);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>训练模型</p>\n<img src=\"/2018/06/13/word2vec/cbow-hs-2.png\" alt=\"[cbow-hs-2]\" title=\"[cbow-hs-2]\">\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">if (cw) &#123;</div><div class=\"line\">        for (c = 0; c &lt; layer1_size; c++) neu1[c] /= cw;</div><div class=\"line\">        // 如果分层softmax, theta(j: 2-&gt;l, w)作为tree的路径参数</div><div class=\"line\">        if (hs) for (d = 0; d &lt; vocab[word].codelen; d++) &#123;</div><div class=\"line\">          f = 0;</div><div class=\"line\">          l2 = vocab[word].point[d] * layer1_size;</div><div class=\"line\">          // Propagate hidden -&gt; output</div><div class=\"line\">          // 3.1 查表计算sigma(x(w)*theta)</div><div class=\"line\">          for (c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1[c + l2];</div><div class=\"line\">          if (f &lt;= -MAX_EXP) continue;</div><div class=\"line\">          else if (f &gt;= MAX_EXP) continue;</div><div class=\"line\">          else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];</div><div class=\"line\">          // &apos;g&apos; is the gradient multiplied by the learning rate: 3.2 g = (1-dj-q) * eta</div><div class=\"line\">          g = (1 - vocab[word].code[d] - f) * alpha;</div><div class=\"line\">          // Propagate errors output -&gt; hidden : 3.3 v = v + g*theta</div><div class=\"line\">          for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2];</div><div class=\"line\">          // Learn weights hidden -&gt; output: 3.4 theta = theta + g*x(w)</div><div class=\"line\">          for (c = 0; c &lt; layer1_size; c++) syn1[c + l2] += g * neu1[c];</div><div class=\"line\">        &#125;</div></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h5 id=\"起源和工具包\"><a href=\"#起源和工具包\" class=\"headerlink\" title=\"起源和工具包\"></a>起源和工具包</h5><p>word2vec是google在2013年的模型，可以有效的将词映射到某个向量空间，经典的demo是：<br>vector(‘Paris’) - vector(‘France’) + vector(‘Italy’) = vector(‘Rome’)<br>其开源地址是：<a href=\"https://code.google.com/archive/p/word2vec/\" target=\"_blank\" rel=\"external\">https://code.google.com/archive/p/word2vec/</a><br>此工具简单好用，在很多项目上起到了很好的效果</p>\n<h5 id=\"word-embedding的各种姿势\"><a href=\"#word-embedding的各种姿势\" class=\"headerlink\" title=\"word embedding的各种姿势\"></a>word embedding的各种姿势</h5><ul>\n<li>Topic Model：词袋模型，不分先后顺序，代表是LDA模型</li>\n<li>Word2vec：等价于三层神经网络，能捕捉时序信息</li>\n<li>FM：通过FM的weight矩阵将原始向量做变换</li>\n<li>DSSM：通过点击数据把user和item的分别作Deep模型，取出来最顶层的做word representation</li>\n</ul>\n<p>另外NLP领域最新的模型是LSTM和Attention，能否利用其模型做embedding还需调研下</p>\n<ul>\n<li>LSTM：能更好的捕捉时序信息，解决long dependence的问题</li>\n<li>Attention：通过注意力机制</li>\n</ul>\n<h5 id=\"word2vec的各种包\"><a href=\"#word2vec的各种包\" class=\"headerlink\" title=\"word2vec的各种包\"></a>word2vec的各种包</h5><ul>\n<li>google word2vec：效果非常好，单机</li>\n<li>gensim：经典的自然语言处理包，用的人很多，单机</li>\n<li>spark mllib：不确定效果如何，分布式</li>\n<li>tensorflow：未尝试，分布式<br>（因为各种姿势都可以通过tensorflow来实现，包括FM，Word2Vec，以及各种矩阵分解，因此tf是很值得学习的工具。）</li>\n</ul>\n<h5 id=\"Why-word-embedding？\"><a href=\"#Why-word-embedding？\" class=\"headerlink\" title=\"Why word embedding？\"></a>Why word embedding？</h5><ul>\n<li>降维：原始ID维度的特征太大了，需要过多的训练数据，word embedding之后不需要那么多数据，而且防止过拟合</li>\n</ul>\n<h5 id=\"embedding的两类方法\"><a href=\"#embedding的两类方法\" class=\"headerlink\" title=\"embedding的两类方法\"></a>embedding的两类方法</h5><ul>\n<li>count-based：lsa系列，通过矩阵分解来做embedding</li>\n<li>predictive-based：通过预测词可能出现的词来做embedding，如word2vec</li>\n</ul>\n<h5 id=\"gensim建模-tensorboard可视化\"><a href=\"#gensim建模-tensorboard可视化\" class=\"headerlink\" title=\"gensim建模+tensorboard可视化\"></a>gensim建模+tensorboard可视化</h5><ul>\n<li><p>生成gensim训练数据，每行一个doc，空格分隔</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">select list</div><div class=\"line\">from</div><div class=\"line\">(</div><div class=\"line\">    select user_id, concat_ws(&apos; &apos;, collect_set(item_id)) as list</div><div class=\"line\">    from dm_music_prd.t_7d_imusic_iting_user_item_action</div><div class=\"line\">    where ds&gt;=20180520 and action=1 and item_id != -1 and extend&gt;1000*90</div><div class=\"line\">    group by user_id</div><div class=\"line\">) t1</div><div class=\"line\">join</div><div class=\"line\">(</div><div class=\"line\">    select user_id</div><div class=\"line\">    from</div><div class=\"line\">    (</div><div class=\"line\">        select user_id, count(distinct item_id) as cnt</div><div class=\"line\">        from dm_music_prd.t_7d_imusic_iting_user_item_action</div><div class=\"line\">        where ds&gt;=20180520 and action=1 and item_id != -1 and extend&gt;1000*90</div><div class=\"line\">        group by user_id</div><div class=\"line\">    ) f</div><div class=\"line\">    where cnt &gt;= 5 and cnt &lt;= 50</div><div class=\"line\">) t2 on t1.user_id=t2.user_id</div></pre></td></tr></table></figure>\n</li>\n<li><p>gensim训练word2vec模型</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div></pre></td><td class=\"code\"><pre><div class=\"line\">/root/xpguo/anaconda3/bin/python w2v_v2.py user_songlist.v2 ./model_v2/w2v20180625 song_vector_v2</div><div class=\"line\"></div><div class=\"line\">#!/usr/bin/env python</div><div class=\"line\"># -*- coding: utf-8 -*-</div><div class=\"line\"></div><div class=\"line\">import logging</div><div class=\"line\">import os</div><div class=\"line\">import sys</div><div class=\"line\">import multiprocessing</div><div class=\"line\"></div><div class=\"line\">from gensim.models import Word2Vec</div><div class=\"line\">from gensim.models.word2vec import LineSentence</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    program = os.path.basename(sys.argv[0])</div><div class=\"line\">    logger = logging.getLogger(program)</div><div class=\"line\"></div><div class=\"line\">    logging.basicConfig(format=&apos;%(asctime)s: %(levelname)s: %(message)s&apos;)</div><div class=\"line\">    logging.root.setLevel(level=logging.INFO)</div><div class=\"line\">    logger.info(&quot;running %s&quot; % &apos; &apos;.join(sys.argv))</div><div class=\"line\"></div><div class=\"line\">    # check and process input arguments</div><div class=\"line\">    if len(sys.argv) &lt; 4:</div><div class=\"line\">        print(globals()[&apos;__doc__&apos;] % locals())</div><div class=\"line\">        sys.exit(1)</div><div class=\"line\">    inp, outp1, outp2 = sys.argv[1:4]</div><div class=\"line\"></div><div class=\"line\">    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5,</div><div class=\"line\">                     workers=multiprocessing.cpu_count())</div><div class=\"line\"></div><div class=\"line\">    # trim unneeded model memory = use(much) less RAM</div><div class=\"line\">    # model.init_sims(replace=True)</div><div class=\"line\">    # w2v model</div><div class=\"line\">    model.save(outp1)</div><div class=\"line\">    # word vectors</div><div class=\"line\">    model.wv.save_word2vec_format(outp2, binary=False)</div></pre></td></tr></table></figure>\n</li>\n<li><p>Tensorboard 可视化</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div></pre></td><td class=\"code\"><pre><div class=\"line\">1.\t拉取id2name</div><div class=\"line\">mysql -h10.20.125.43 -umyshuju_r -p3KAjvBHaDB&#123;gLE9H -e &quot;select third_id, name from music.t_song_info where third_id is not null and name is not null and length(third_id)&gt;0 and length(name)&gt;0&quot; &gt; t_song_info</div><div class=\"line\"></div><div class=\"line\">2.\t写tensorboard模型</div><div class=\"line\">import sys, os</div><div class=\"line\">from gensim.models import Word2Vec</div><div class=\"line\">import tensorflow as tf</div><div class=\"line\">import numpy as np</div><div class=\"line\">from tensorflow.contrib.tensorboard.plugins import projector</div><div class=\"line\"></div><div class=\"line\">id2name = &#123;&#125;</div><div class=\"line\"></div><div class=\"line\">## 可视化函数</div><div class=\"line\">## model: gensim模型地址</div><div class=\"line\">## output_path: tf board转换模型地址</div><div class=\"line\">def visualize(model, output_path):</div><div class=\"line\">    ## tf board转换模型名称，自定义</div><div class=\"line\">    meta_file = &quot;w2x_metadata.tsv&quot;</div><div class=\"line\"></div><div class=\"line\">    ## 词向量个数*词向量维数</div><div class=\"line\">    placeholder = np.zeros((len(model.wv.index2word), 400))</div><div class=\"line\"></div><div class=\"line\">    ## 读取ID2name文件</div><div class=\"line\">    inFp = open(&quot;./t_song_info&quot;, &apos;r&apos;)</div><div class=\"line\">    while True:</div><div class=\"line\">        line = inFp.readline()</div><div class=\"line\">        if not line:</div><div class=\"line\">            break</div><div class=\"line\">        items = line.strip().split(&apos;\\t&apos;)</div><div class=\"line\">        if len(items) != 2:</div><div class=\"line\">            continue</div><div class=\"line\">        id2name[items[0]] = items[1]</div><div class=\"line\">    inFp.close()</div><div class=\"line\"></div><div class=\"line\">    ## 地址+名称拼接</div><div class=\"line\">    with open(os.path.join(output_path,meta_file), &apos;wb&apos;) as file_metadata:</div><div class=\"line\"></div><div class=\"line\">        ## 对于每个词向量，写文件</div><div class=\"line\">        for i, word in enumerate(model.wv.index2word):</div><div class=\"line\">            placeholder[i] = model[word]</div><div class=\"line\">            # temporary solution for https://github.com/tensorflow/tensorflow/issues/9094</div><div class=\"line\">            if word == &apos;&apos;:</div><div class=\"line\">                print(&quot;Emply Line, should replecaed by any thing else, or will cause a bug of tensorboard&quot;)</div><div class=\"line\">                file_metadata.write(&quot;&#123;0&#125;&quot;.format(&apos;&lt;Empty Line&gt;&apos;).encode(&apos;utf-8&apos;) + b&apos;\\n&apos;)</div><div class=\"line\">            else:</div><div class=\"line\">#                file_metadata.write(&quot;&#123;0&#125;&quot;.format(word).encode(&apos;utf-8&apos;) + b&apos;\\n&apos;)</div><div class=\"line\">                file_metadata.write(&quot;&#123;0&#125;&quot;.format(id2name.get(word, &apos;null&apos;)).encode(&apos;utf-8&apos;) + b&apos;\\n&apos;)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    # define the model without training</div><div class=\"line\">    sess = tf.InteractiveSession()</div><div class=\"line\"></div><div class=\"line\">    embedding = tf.Variable(placeholder, trainable = False, name = &apos;w2x_metadata&apos;)</div><div class=\"line\">    tf.global_variables_initializer().run()</div><div class=\"line\"></div><div class=\"line\">    saver = tf.train.Saver()</div><div class=\"line\">    writer = tf.summary.FileWriter(output_path, sess.graph)</div><div class=\"line\"></div><div class=\"line\">    # adding into projector</div><div class=\"line\">    config = projector.ProjectorConfig()</div><div class=\"line\">    embed = config.embeddings.add()</div><div class=\"line\">    embed.tensor_name = &apos;w2x_metadata&apos;</div><div class=\"line\">    embed.metadata_path = meta_file</div><div class=\"line\"></div><div class=\"line\">    # Specify the width and height of a single thumbnail.</div><div class=\"line\">    projector.visualize_embeddings(writer, config)</div><div class=\"line\">    saver.save(sess, os.path.join(output_path,&apos;w2x_metadata.ckpt&apos;))</div><div class=\"line\">    print(&apos;Run `tensorboard --logdir=&#123;0&#125;` to run visualize result on tensorboard&apos;.format(output_path))</div><div class=\"line\"></div><div class=\"line\">if __name__ == &quot;__main__&quot;:</div><div class=\"line\">    model = Word2Vec.load(&quot;/home/xpguo/gensim/word2vec/song_vector_v1/a&quot;)</div><div class=\"line\">visualize(model,&quot;/home/xpguo/gensim/word2vec/song_vector_v1_tf_board&quot;)</div><div class=\"line\"></div><div class=\"line\">3.\ttensorboard可视化</div><div class=\"line\">tensorboard --logdir=/home/xpguo/gensim/word2vec/song_vector_v2_tf_board --port=6607</div></pre></td></tr></table></figure>\n</li>\n<li><p>Tensorboard 可视化效果</p>\n<img src=\"/2018/06/13/word2vec/tfboard.png\" alt=\"[tf-board]\" title=\"[tf-board]\">\n</li>\n</ul>\n<h5 id=\"word2vec源码\"><a href=\"#word2vec源码\" class=\"headerlink\" title=\"word2vec源码\"></a>word2vec源码</h5><p>*　cbow-框架图<br><img src=\"/2018/06/13/word2vec/cbow-1.png\" alt=\"[cbow-1]\" title=\"[cbow-1]\"></p>\n<ul>\n<li><p>cbow-Hierarchical softmax</p>\n<img src=\"/2018/06/13/word2vec/cbow-hs.png\" alt=\"[cbow-hs]\" title=\"[cbow-hs]\">\n</li>\n<li><p>cbow-hs-hand</p>\n<img src=\"/2018/06/13/word2vec/cbow-hs-hand.png\" alt=\"[cbow-hs-hand.png]\" title=\"[cbow-hs-hand.png]\">\n</li>\n</ul>\n<h5 id=\"word2vec源码细节\"><a href=\"#word2vec源码细节\" class=\"headerlink\" title=\"word2vec源码细节\"></a>word2vec源码细节</h5><ul>\n<li><p>预计算sigmoid(x)，将[-6, 6]的区间划分成1000份来计算。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">#define EXP_TABLE_SIZE 1000</div><div class=\"line\">#define MAX_EXP 6</div><div class=\"line\">expTable = (real *)malloc((EXP_TABLE_SIZE + 1) * sizeof(real));</div><div class=\"line\">for (i = 0; i &lt; EXP_TABLE_SIZE; i++) &#123;</div><div class=\"line\">  expTable[i] = exp((i / (real)EXP_TABLE_SIZE * 2 - 1) * MAX_EXP); // Precompute the exp() table</div><div class=\"line\">  expTable[i] = expTable[i] / (expTable[i] + 1);                   // Precompute f(x) = x / (x + 1)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>hash函数，每个char+原hash值*257，很常见的做法</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Returns hash value of a word</div><div class=\"line\">int GetWordHash(char *word) &#123;</div><div class=\"line\">  unsigned long long a, hash = 0;</div><div class=\"line\">  for (a = 0; a &lt; strlen(word); a++) hash = hash * 257 + word[a];</div><div class=\"line\">  hash = hash % vocab_hash_size;</div><div class=\"line\">  return hash;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>线性探测法，hash后线性地去找词，要么找到返回词的位置，要么找到-1</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Returns position of a word in the vocabulary; if the word is not found, returns -1</div><div class=\"line\">int SearchVocab(char *word) &#123;</div><div class=\"line\">  unsigned int hash = GetWordHash(word);</div><div class=\"line\">  while (1) &#123;</div><div class=\"line\">    if (vocab_hash[hash] == -1) return -1;</div><div class=\"line\">    if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash];</div><div class=\"line\">    hash = (hash + 1) % vocab_hash_size;</div><div class=\"line\">  &#125;</div><div class=\"line\">  return -1;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>将词添加到词典中， 有两个地方要加，第一是hash映射表，第二是词对应的count表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Adds a word to the vocabulary</div><div class=\"line\">int AddWordToVocab(char *word) &#123;</div><div class=\"line\">  unsigned int hash, length = strlen(word) + 1;</div><div class=\"line\">  if (length &gt; MAX_STRING) length = MAX_STRING;</div><div class=\"line\">  vocab[vocab_size].word = (char *)calloc(length, sizeof(char));</div><div class=\"line\">  strcpy(vocab[vocab_size].word, word);</div><div class=\"line\">  vocab[vocab_size].cn = 0;</div><div class=\"line\">  vocab_size++;</div><div class=\"line\">  // Reallocate memory if needed</div><div class=\"line\">  if (vocab_size + 2 &gt;= vocab_max_size) &#123;</div><div class=\"line\">    vocab_max_size += 1000;</div><div class=\"line\">    vocab = (struct vocab_word *)realloc(vocab, vocab_max_size * sizeof(struct vocab_word));</div><div class=\"line\">  &#125;</div><div class=\"line\">  hash = GetWordHash(word);</div><div class=\"line\">  while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;</div><div class=\"line\">  vocab_hash[hash] = vocab_size - 1;</div><div class=\"line\">  return vocab_size - 1;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>将词典排序，目的是为了建立哈弗曼树，有几个地方要注意：一是出现频率比较低的词，会被过滤掉；二是过滤掉之后需要讲hashtable重新hash，因为去掉了一些词，table不能用了。感慨一下，c语言要自己实现hash_dict，真心麻烦。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Sorts the vocabulary by frequency using word counts</div><div class=\"line\">void SortVocab() &#123;</div><div class=\"line\">  int a, size;</div><div class=\"line\">  unsigned int hash;</div><div class=\"line\">  // Sort the vocabulary and keep &lt;/s&gt; at the first position</div><div class=\"line\">  qsort(&amp;vocab[1], vocab_size - 1, sizeof(struct vocab_word), VocabCompare);</div><div class=\"line\">  for (a = 0; a &lt; vocab_hash_size; a++) vocab_hash[a] = -1;</div><div class=\"line\">  size = vocab_size;</div><div class=\"line\">  train_words = 0;</div><div class=\"line\">  for (a = 0; a &lt; size; a++) &#123;</div><div class=\"line\">    // Words occuring less than min_count times will be discarded from the vocab</div><div class=\"line\">    if ((vocab[a].cn &lt; min_count) &amp;&amp; (a != 0)) &#123;</div><div class=\"line\">      vocab_size--;</div><div class=\"line\">      free(vocab[a].word);</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      // Hash will be re-computed, as after the sorting it is not actual</div><div class=\"line\">      hash=GetWordHash(vocab[a].word);</div><div class=\"line\">      while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;</div><div class=\"line\">      vocab_hash[hash] = a;</div><div class=\"line\">      train_words += vocab[a].cn;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  vocab = (struct vocab_word *)realloc(vocab, (vocab_size + 1) * sizeof(struct vocab_word));</div><div class=\"line\">  // Allocate memory for the binary tree construction</div><div class=\"line\">  for (a = 0; a &lt; vocab_size; a++) &#123;</div><div class=\"line\">    vocab[a].code = (char *)calloc(MAX_CODE_LENGTH, sizeof(char));</div><div class=\"line\">    vocab[a].point = (int *)calloc(MAX_CODE_LENGTH, sizeof(int));</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>初始化网络参数，首先要搞清楚网络是什么样子的：</p>\n<ul>\n<li>syn0: e(w), 向量映射表，最终看的结果</li>\n<li>syn1: theta(w, j), 存放hs的每个节点的参数</li>\n<li>syn1neg: theta for negtive sampling, 存放theta(u)</li>\n<li>vocab_size:词典大小。</li>\n<li>layer1_size:第一层大小，即向量映射的维度</li>\n<li>末尾还会创建一棵哈弗曼树</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">void InitNet() &#123;</div><div class=\"line\">  long long a, b;</div><div class=\"line\">  unsigned long long next_random = 1;</div><div class=\"line\">  a = posix_memalign((void **)&amp;syn0, 128, (long long)vocab_size * layer1_size * sizeof(real));</div><div class=\"line\">  if (syn0 == NULL) &#123;printf(&quot;Memory allocation failed\\n&quot;); exit(1);&#125;</div><div class=\"line\">  if (hs) &#123;</div><div class=\"line\">    a = posix_memalign((void **)&amp;syn1, 128, (long long)vocab_size * layer1_size * sizeof(real));</div><div class=\"line\">    if (syn1 == NULL) &#123;printf(&quot;Memory allocation failed\\n&quot;); exit(1);&#125;</div><div class=\"line\">    for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++)</div><div class=\"line\">     syn1[a * layer1_size + b] = 0;</div><div class=\"line\">  &#125;</div><div class=\"line\">  if (negative&gt;0) &#123;</div><div class=\"line\">    a = posix_memalign((void **)&amp;syn1neg, 128, (long long)vocab_size * layer1_size * sizeof(real));</div><div class=\"line\">    if (syn1neg == NULL) &#123;printf(&quot;Memory allocation failed\\n&quot;); exit(1);&#125;</div><div class=\"line\">    for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++)</div><div class=\"line\">     syn1neg[a * layer1_size + b] = 0;</div><div class=\"line\">  &#125;</div><div class=\"line\">  for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) &#123;</div><div class=\"line\">    next_random = next_random * (unsigned long long)25214903917 + 11;</div><div class=\"line\">    syn0[a * layer1_size + b] = (((next_random &amp; 0xFFFF) / (real)65536) - 0.5) / layer1_size;</div><div class=\"line\">  &#125;</div><div class=\"line\">  CreateBinaryTree();</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ul>\n<li><p>初始化采样表，注意不是直接用次数，而是用pow(次数, 0.75)来做，降低一下高频词的采到的概率。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">void InitUnigramTable() &#123;</div><div class=\"line\">  int a, i;</div><div class=\"line\">  double train_words_pow = 0;</div><div class=\"line\">  double d1, power = 0.75;</div><div class=\"line\">  table = (int *)malloc(table_size * sizeof(int));</div><div class=\"line\">  for (a = 0; a &lt; vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);</div><div class=\"line\">  i = 0;</div><div class=\"line\">  d1 = pow(vocab[i].cn, power) / train_words_pow;</div><div class=\"line\">  for (a = 0; a &lt; table_size; a++) &#123;</div><div class=\"line\">    table[a] = i;</div><div class=\"line\">    if (a / (double)table_size &gt; d1) &#123;</div><div class=\"line\">      i++;</div><div class=\"line\">      d1 += pow(vocab[i].cn, power) / train_words_pow;</div><div class=\"line\">    &#125;</div><div class=\"line\">    if (i &gt;= vocab_size) i = vocab_size - 1;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>构建哈弗曼树，自底向上构建，code指的是huffman tree的编码，point指向对应词在词典中的位置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Create binary Huffman tree using the word counts</div><div class=\"line\">// Frequent words will have short uniqe binary codes</div><div class=\"line\">void CreateBinaryTree() &#123;</div><div class=\"line\">  long long a, b, i, min1i, min2i, pos1, pos2, point[MAX_CODE_LENGTH];</div><div class=\"line\">  char code[MAX_CODE_LENGTH];</div><div class=\"line\">  long long *count = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</div><div class=\"line\">  long long *binary = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</div><div class=\"line\">  long long *parent_node = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</div><div class=\"line\">  for (a = 0; a &lt; vocab_size; a++) count[a] = vocab[a].cn;</div><div class=\"line\">  for (a = vocab_size; a &lt; vocab_size * 2; a++) count[a] = 1e15;</div><div class=\"line\">  pos1 = vocab_size - 1;</div><div class=\"line\">  pos2 = vocab_size;</div><div class=\"line\">  // Following algorithm constructs the Huffman tree by adding one node at a time</div><div class=\"line\">  for (a = 0; a &lt; vocab_size - 1; a++) &#123;</div><div class=\"line\">    // First, find two smallest nodes &apos;min1, min2&apos;</div><div class=\"line\">    if (pos1 &gt;= 0) &#123;</div><div class=\"line\">      if (count[pos1] &lt; count[pos2]) &#123;</div><div class=\"line\">        min1i = pos1;</div><div class=\"line\">        pos1--;</div><div class=\"line\">      &#125; else &#123;</div><div class=\"line\">        min1i = pos2;</div><div class=\"line\">        pos2++;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      min1i = pos2;</div><div class=\"line\">      pos2++;</div><div class=\"line\">    &#125;</div><div class=\"line\">    if (pos1 &gt;= 0) &#123;</div><div class=\"line\">      if (count[pos1] &lt; count[pos2]) &#123;</div><div class=\"line\">        min2i = pos1;</div><div class=\"line\">        pos1--;</div><div class=\"line\">      &#125; else &#123;</div><div class=\"line\">        min2i = pos2;</div><div class=\"line\">        pos2++;</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      min2i = pos2;</div><div class=\"line\">      pos2++;</div><div class=\"line\">    &#125;</div><div class=\"line\">    count[vocab_size + a] = count[min1i] + count[min2i];</div><div class=\"line\">    parent_node[min1i] = vocab_size + a;</div><div class=\"line\">    parent_node[min2i] = vocab_size + a;</div><div class=\"line\">    binary[min2i] = 1;</div><div class=\"line\">  &#125;</div><div class=\"line\">  // Now assign binary code to each vocabulary word</div><div class=\"line\">  for (a = 0; a &lt; vocab_size; a++) &#123;</div><div class=\"line\">    b = a;</div><div class=\"line\">    i = 0;</div><div class=\"line\">    while (1) &#123;</div><div class=\"line\">      code[i] = binary[b];</div><div class=\"line\">      point[i] = b;</div><div class=\"line\">      i++;</div><div class=\"line\">      b = parent_node[b];</div><div class=\"line\">      if (b == vocab_size * 2 - 2) break;</div><div class=\"line\">    &#125;</div><div class=\"line\">    vocab[a].codelen = i;</div><div class=\"line\">    vocab[a].point[0] = vocab_size - 2;</div><div class=\"line\">    for (b = 0; b &lt; i; b++) &#123;</div><div class=\"line\">      vocab[a].code[i - b - 1] = code[b];</div><div class=\"line\">      vocab[a].point[i - b] = point[b] - vocab_size;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  free(count);</div><div class=\"line\">  free(binary);</div><div class=\"line\">  free(parent_node);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>训练模型</p>\n<img src=\"/2018/06/13/word2vec/cbow-hs-2.png\" alt=\"[cbow-hs-2]\" title=\"[cbow-hs-2]\">\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">if (cw) &#123;</div><div class=\"line\">        for (c = 0; c &lt; layer1_size; c++) neu1[c] /= cw;</div><div class=\"line\">        // 如果分层softmax, theta(j: 2-&gt;l, w)作为tree的路径参数</div><div class=\"line\">        if (hs) for (d = 0; d &lt; vocab[word].codelen; d++) &#123;</div><div class=\"line\">          f = 0;</div><div class=\"line\">          l2 = vocab[word].point[d] * layer1_size;</div><div class=\"line\">          // Propagate hidden -&gt; output</div><div class=\"line\">          // 3.1 查表计算sigma(x(w)*theta)</div><div class=\"line\">          for (c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1[c + l2];</div><div class=\"line\">          if (f &lt;= -MAX_EXP) continue;</div><div class=\"line\">          else if (f &gt;= MAX_EXP) continue;</div><div class=\"line\">          else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];</div><div class=\"line\">          // &apos;g&apos; is the gradient multiplied by the learning rate: 3.2 g = (1-dj-q) * eta</div><div class=\"line\">          g = (1 - vocab[word].code[d] - f) * alpha;</div><div class=\"line\">          // Propagate errors output -&gt; hidden : 3.3 v = v + g*theta</div><div class=\"line\">          for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2];</div><div class=\"line\">          // Learn weights hidden -&gt; output: 3.4 theta = theta + g*x(w)</div><div class=\"line\">          for (c = 0; c &lt; layer1_size; c++) syn1[c + l2] += g * neu1[c];</div><div class=\"line\">        &#125;</div></pre></td></tr></table></figure>"},{"title":"tf-serving","date":"2018-09-30T09:27:27.000Z","_content":"\n* ","source":"_posts/tf-serving.md","raw":"---\ntitle: tf-serving\ndate: 2018-09-30 17:27:27\ntags:\n---\n\n* ","slug":"tf-serving","published":1,"updated":"2018-09-30T09:28:00.245Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gbm001atcinoh2mjyj2","content":"<p>* </p>\n","site":{"data":{}},"excerpt":"","more":"<p>* </p>\n"},{"title":"search","date":"2018-01-08T08:05:05.000Z","_content":"\n### 垂直领域搜索\nAPP搜索引擎，属于垂直领域的搜索引擎，相对于泛需求的搜索引擎会简单很多。\n泛搜索引擎 -> 意图识别（下APP，听歌，找wiki） -> 该意图垂直领域引擎 \n即在垂直领域，不需要挂载意图识别模块。\n\n<!-- more -->\n\n### 相关性(relevance)和重要性(importance)\n相关性：用户搜索“美”，京东和美团对比，美团的“相关性”更高，所以美团比京东排序高。\n重要性：用户搜索“美”，美团和美丽说对比，美团的“重要性”更高（美团用户量更大，美丽说小众），所以美团比美丽说的排序高。\n\n相关性和重要性，是搜索里需要tradeoff的两个指标，这里的tradeoff，往往是建立排序模型。\n即：相关性召回+重要性召回 -> 排序 \n\n### 相关性 \n#### 文本相似性\n* 标题精准匹配（Exact Match）\n用户搜索“美团”，大概率是对“APP名称的搜索”，因此按照名称精准匹配，最简单最有效。\n* 标题模糊匹配（Fuzzy Match）\n用户会打错字，用户会query打不全，因此模糊匹配非常重要。\n    * 拼音化\n用户搜索“meituan”，也能够出来“美团”的结果，是因为将app名称进行拼音化再匹配。\n    * query纠错/改写\n用户搜索“没团”，也能够出来“美团”的结果，是将query纠错后再匹配。\n    * 编辑距离\n用户搜索“美”，也能够出来“美团”的结果，是因为“美”和“美团”的编辑距离为1，比较小。\n【编辑距离ref】：https://zh.wikipedia.org/wiki/%E7%B7%A8%E8%BC%AF%E8%B7%9D%E9%9B%A2\n* 内容TF-IDF \n    * TF(Term Frequency)\nquery分词后，词语在内容（描述，评论）中出现的次数，可以使0-1值，可以是次数，可以是log（次数）等等任意变种。\n    * IDF(Inverse Document Frequency)\nquery分词后，词语在query中的重要性，可以是0-1值，可以是log(N/Nt)，可以是log(1+N/Nt)等等任意变种。\n    * 组合\nIDF将query分词后设置词的权重，TF将词去和文档匹配，TF-IDF就是加权的词和文档的匹配。\n【TF-IDF ref】https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n* 内容BM25\nBM25是一种TF-IDF-like retrieval functions，即原理相通。BM25假设有多个词，即sum(每个词的TF-IDF)。\n公式为：\nscore(D, Q) = sum(TF-IDF) D stands for doc, Q stands for query.\n\nTF: {% asset_img \"TF.png\" [TF] %}\n* f(qi, D) is qi's term frequency in the document D\n* |D| is the length of the document D in words\n* avgdl is the average document length in the text collection from which documents are drawn.\n* k1 and b are free parameters, usually chosen, in absence of an advanced optimization, as k1 in [1.2,2.0] and b = 0.75.\n\nIDF: {% asset_img \"IDF.png\" [IDF] %}\n* N is the total number of documents in the collection\n* n(qi) is the number of documents containing qi.\n\n【BM25 ref】https://en.wikipedia.org/wiki/Okapi_BM25\n\n#### 语义相似性\n* Topic Model\n    乔布斯的苹果和水果店的苹果，识别语义。建议采用”维基预料训练“，而非query来训练，因为LDA对短文本效果不好。\n* 类型相关\n    识别query的类型，比如搜索”聊天“，那就从”聊天软件“类目中召回APP。\n\n### 重要性\n#### 属性重要性 \n* 星级/评论挖掘\n* 曝光量/点击量/下载量/安装量/使用量/注册量/付费量/留存量（率）\n* 商业化价值\n\n#### 图重要性\n* Page Rank值\n\n#### 其他召回\n* x%用户下载召回\n* 新品召回\n* 运营召回\n* 个性化召回\n\n### 排序 \n* 点击熵\n    识别是精准需求（文本相关）还是泛需求（语义相关）的指标，可以用来确定召回比例，也可以用来作为排序特征（此时需要交叉两个召回来源）。\n* 条件概率 p(ctr|query, user, scene)\n给某人（对人的个性化），场景（对场景的把控，在机场推荐航班相关，晚饭时间推荐美食相关），query（用户对意图的主动描述）的最优化问题。\n\n### 评估（IR evaluation）\n* MAP: Mean Average Precision，不再赘述。\n* nDCG: Normalized Discounted cumulative gain，按照位置加权。\n\n【IR metrics ref】http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.htmll\n【MAP vs NDCG ref】https://www.youtube.com/watch?v=qm1In7NH8WE\n\n","source":"_posts/search.md","raw":"---\ntitle: search\ndate: 2018-01-08 16:05:05\ntags:\n---\n\n### 垂直领域搜索\nAPP搜索引擎，属于垂直领域的搜索引擎，相对于泛需求的搜索引擎会简单很多。\n泛搜索引擎 -> 意图识别（下APP，听歌，找wiki） -> 该意图垂直领域引擎 \n即在垂直领域，不需要挂载意图识别模块。\n\n<!-- more -->\n\n### 相关性(relevance)和重要性(importance)\n相关性：用户搜索“美”，京东和美团对比，美团的“相关性”更高，所以美团比京东排序高。\n重要性：用户搜索“美”，美团和美丽说对比，美团的“重要性”更高（美团用户量更大，美丽说小众），所以美团比美丽说的排序高。\n\n相关性和重要性，是搜索里需要tradeoff的两个指标，这里的tradeoff，往往是建立排序模型。\n即：相关性召回+重要性召回 -> 排序 \n\n### 相关性 \n#### 文本相似性\n* 标题精准匹配（Exact Match）\n用户搜索“美团”，大概率是对“APP名称的搜索”，因此按照名称精准匹配，最简单最有效。\n* 标题模糊匹配（Fuzzy Match）\n用户会打错字，用户会query打不全，因此模糊匹配非常重要。\n    * 拼音化\n用户搜索“meituan”，也能够出来“美团”的结果，是因为将app名称进行拼音化再匹配。\n    * query纠错/改写\n用户搜索“没团”，也能够出来“美团”的结果，是将query纠错后再匹配。\n    * 编辑距离\n用户搜索“美”，也能够出来“美团”的结果，是因为“美”和“美团”的编辑距离为1，比较小。\n【编辑距离ref】：https://zh.wikipedia.org/wiki/%E7%B7%A8%E8%BC%AF%E8%B7%9D%E9%9B%A2\n* 内容TF-IDF \n    * TF(Term Frequency)\nquery分词后，词语在内容（描述，评论）中出现的次数，可以使0-1值，可以是次数，可以是log（次数）等等任意变种。\n    * IDF(Inverse Document Frequency)\nquery分词后，词语在query中的重要性，可以是0-1值，可以是log(N/Nt)，可以是log(1+N/Nt)等等任意变种。\n    * 组合\nIDF将query分词后设置词的权重，TF将词去和文档匹配，TF-IDF就是加权的词和文档的匹配。\n【TF-IDF ref】https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n* 内容BM25\nBM25是一种TF-IDF-like retrieval functions，即原理相通。BM25假设有多个词，即sum(每个词的TF-IDF)。\n公式为：\nscore(D, Q) = sum(TF-IDF) D stands for doc, Q stands for query.\n\nTF: {% asset_img \"TF.png\" [TF] %}\n* f(qi, D) is qi's term frequency in the document D\n* |D| is the length of the document D in words\n* avgdl is the average document length in the text collection from which documents are drawn.\n* k1 and b are free parameters, usually chosen, in absence of an advanced optimization, as k1 in [1.2,2.0] and b = 0.75.\n\nIDF: {% asset_img \"IDF.png\" [IDF] %}\n* N is the total number of documents in the collection\n* n(qi) is the number of documents containing qi.\n\n【BM25 ref】https://en.wikipedia.org/wiki/Okapi_BM25\n\n#### 语义相似性\n* Topic Model\n    乔布斯的苹果和水果店的苹果，识别语义。建议采用”维基预料训练“，而非query来训练，因为LDA对短文本效果不好。\n* 类型相关\n    识别query的类型，比如搜索”聊天“，那就从”聊天软件“类目中召回APP。\n\n### 重要性\n#### 属性重要性 \n* 星级/评论挖掘\n* 曝光量/点击量/下载量/安装量/使用量/注册量/付费量/留存量（率）\n* 商业化价值\n\n#### 图重要性\n* Page Rank值\n\n#### 其他召回\n* x%用户下载召回\n* 新品召回\n* 运营召回\n* 个性化召回\n\n### 排序 \n* 点击熵\n    识别是精准需求（文本相关）还是泛需求（语义相关）的指标，可以用来确定召回比例，也可以用来作为排序特征（此时需要交叉两个召回来源）。\n* 条件概率 p(ctr|query, user, scene)\n给某人（对人的个性化），场景（对场景的把控，在机场推荐航班相关，晚饭时间推荐美食相关），query（用户对意图的主动描述）的最优化问题。\n\n### 评估（IR evaluation）\n* MAP: Mean Average Precision，不再赘述。\n* nDCG: Normalized Discounted cumulative gain，按照位置加权。\n\n【IR metrics ref】http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.htmll\n【MAP vs NDCG ref】https://www.youtube.com/watch?v=qm1In7NH8WE\n\n","slug":"search","published":1,"updated":"2018-01-08T11:57:41.621Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gbo001btcinj1bzt215","content":"<h3 id=\"垂直领域搜索\"><a href=\"#垂直领域搜索\" class=\"headerlink\" title=\"垂直领域搜索\"></a>垂直领域搜索</h3><p>APP搜索引擎，属于垂直领域的搜索引擎，相对于泛需求的搜索引擎会简单很多。<br>泛搜索引擎 -&gt; 意图识别（下APP，听歌，找wiki） -&gt; 该意图垂直领域引擎<br>即在垂直领域，不需要挂载意图识别模块。</p>\n<a id=\"more\"></a>\n<h3 id=\"相关性-relevance-和重要性-importance\"><a href=\"#相关性-relevance-和重要性-importance\" class=\"headerlink\" title=\"相关性(relevance)和重要性(importance)\"></a>相关性(relevance)和重要性(importance)</h3><p>相关性：用户搜索“美”，京东和美团对比，美团的“相关性”更高，所以美团比京东排序高。<br>重要性：用户搜索“美”，美团和美丽说对比，美团的“重要性”更高（美团用户量更大，美丽说小众），所以美团比美丽说的排序高。</p>\n<p>相关性和重要性，是搜索里需要tradeoff的两个指标，这里的tradeoff，往往是建立排序模型。<br>即：相关性召回+重要性召回 -&gt; 排序 </p>\n<h3 id=\"相关性\"><a href=\"#相关性\" class=\"headerlink\" title=\"相关性\"></a>相关性</h3><h4 id=\"文本相似性\"><a href=\"#文本相似性\" class=\"headerlink\" title=\"文本相似性\"></a>文本相似性</h4><ul>\n<li>标题精准匹配（Exact Match）<br>用户搜索“美团”，大概率是对“APP名称的搜索”，因此按照名称精准匹配，最简单最有效。</li>\n<li>标题模糊匹配（Fuzzy Match）<br>用户会打错字，用户会query打不全，因此模糊匹配非常重要。<ul>\n<li>拼音化<br>用户搜索“meituan”，也能够出来“美团”的结果，是因为将app名称进行拼音化再匹配。</li>\n<li>query纠错/改写<br>用户搜索“没团”，也能够出来“美团”的结果，是将query纠错后再匹配。</li>\n<li>编辑距离<br>用户搜索“美”，也能够出来“美团”的结果，是因为“美”和“美团”的编辑距离为1，比较小。<br>【编辑距离ref】：<a href=\"https://zh.wikipedia.org/wiki/%E7%B7%A8%E8%BC%AF%E8%B7%9D%E9%9B%A2\" target=\"_blank\" rel=\"external\">https://zh.wikipedia.org/wiki/%E7%B7%A8%E8%BC%AF%E8%B7%9D%E9%9B%A2</a></li>\n</ul>\n</li>\n<li>内容TF-IDF <ul>\n<li>TF(Term Frequency)<br>query分词后，词语在内容（描述，评论）中出现的次数，可以使0-1值，可以是次数，可以是log（次数）等等任意变种。</li>\n<li>IDF(Inverse Document Frequency)<br>query分词后，词语在query中的重要性，可以是0-1值，可以是log(N/Nt)，可以是log(1+N/Nt)等等任意变种。</li>\n<li>组合<br>IDF将query分词后设置词的权重，TF将词去和文档匹配，TF-IDF就是加权的词和文档的匹配。<br>【TF-IDF ref】<a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a></li>\n</ul>\n</li>\n<li>内容BM25<br>BM25是一种TF-IDF-like retrieval functions，即原理相通。BM25假设有多个词，即sum(每个词的TF-IDF)。<br>公式为：<br>score(D, Q) = sum(TF-IDF) D stands for doc, Q stands for query.</li>\n</ul>\n<p>TF: <img src=\"/2018/01/08/search/TF.png\" alt=\"[TF]\" title=\"[TF]\"></p>\n<ul>\n<li>f(qi, D) is qi’s term frequency in the document D</li>\n<li>|D| is the length of the document D in words</li>\n<li>avgdl is the average document length in the text collection from which documents are drawn.</li>\n<li>k1 and b are free parameters, usually chosen, in absence of an advanced optimization, as k1 in [1.2,2.0] and b = 0.75.</li>\n</ul>\n<p>IDF: <img src=\"/2018/01/08/search/IDF.png\" alt=\"[IDF]\" title=\"[IDF]\"></p>\n<ul>\n<li>N is the total number of documents in the collection</li>\n<li>n(qi) is the number of documents containing qi.</li>\n</ul>\n<p>【BM25 ref】<a href=\"https://en.wikipedia.org/wiki/Okapi_BM25\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Okapi_BM25</a></p>\n<h4 id=\"语义相似性\"><a href=\"#语义相似性\" class=\"headerlink\" title=\"语义相似性\"></a>语义相似性</h4><ul>\n<li>Topic Model<br>  乔布斯的苹果和水果店的苹果，识别语义。建议采用”维基预料训练“，而非query来训练，因为LDA对短文本效果不好。</li>\n<li>类型相关<br>  识别query的类型，比如搜索”聊天“，那就从”聊天软件“类目中召回APP。</li>\n</ul>\n<h3 id=\"重要性\"><a href=\"#重要性\" class=\"headerlink\" title=\"重要性\"></a>重要性</h3><h4 id=\"属性重要性\"><a href=\"#属性重要性\" class=\"headerlink\" title=\"属性重要性\"></a>属性重要性</h4><ul>\n<li>星级/评论挖掘</li>\n<li>曝光量/点击量/下载量/安装量/使用量/注册量/付费量/留存量（率）</li>\n<li>商业化价值</li>\n</ul>\n<h4 id=\"图重要性\"><a href=\"#图重要性\" class=\"headerlink\" title=\"图重要性\"></a>图重要性</h4><ul>\n<li>Page Rank值</li>\n</ul>\n<h4 id=\"其他召回\"><a href=\"#其他召回\" class=\"headerlink\" title=\"其他召回\"></a>其他召回</h4><ul>\n<li>x%用户下载召回</li>\n<li>新品召回</li>\n<li>运营召回</li>\n<li>个性化召回</li>\n</ul>\n<h3 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h3><ul>\n<li>点击熵<br>  识别是精准需求（文本相关）还是泛需求（语义相关）的指标，可以用来确定召回比例，也可以用来作为排序特征（此时需要交叉两个召回来源）。</li>\n<li>条件概率 p(ctr|query, user, scene)<br>给某人（对人的个性化），场景（对场景的把控，在机场推荐航班相关，晚饭时间推荐美食相关），query（用户对意图的主动描述）的最优化问题。</li>\n</ul>\n<h3 id=\"评估（IR-evaluation）\"><a href=\"#评估（IR-evaluation）\" class=\"headerlink\" title=\"评估（IR evaluation）\"></a>评估（IR evaluation）</h3><ul>\n<li>MAP: Mean Average Precision，不再赘述。</li>\n<li>nDCG: Normalized Discounted cumulative gain，按照位置加权。</li>\n</ul>\n<p>【IR metrics ref】<a href=\"http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.htmll\" target=\"_blank\" rel=\"external\">http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.htmll</a><br>【MAP vs NDCG ref】<a href=\"https://www.youtube.com/watch?v=qm1In7NH8WE\" target=\"_blank\" rel=\"external\">https://www.youtube.com/watch?v=qm1In7NH8WE</a></p>\n","site":{"data":{}},"excerpt":"<h3 id=\"垂直领域搜索\"><a href=\"#垂直领域搜索\" class=\"headerlink\" title=\"垂直领域搜索\"></a>垂直领域搜索</h3><p>APP搜索引擎，属于垂直领域的搜索引擎，相对于泛需求的搜索引擎会简单很多。<br>泛搜索引擎 -&gt; 意图识别（下APP，听歌，找wiki） -&gt; 该意图垂直领域引擎<br>即在垂直领域，不需要挂载意图识别模块。</p>","more":"<h3 id=\"相关性-relevance-和重要性-importance\"><a href=\"#相关性-relevance-和重要性-importance\" class=\"headerlink\" title=\"相关性(relevance)和重要性(importance)\"></a>相关性(relevance)和重要性(importance)</h3><p>相关性：用户搜索“美”，京东和美团对比，美团的“相关性”更高，所以美团比京东排序高。<br>重要性：用户搜索“美”，美团和美丽说对比，美团的“重要性”更高（美团用户量更大，美丽说小众），所以美团比美丽说的排序高。</p>\n<p>相关性和重要性，是搜索里需要tradeoff的两个指标，这里的tradeoff，往往是建立排序模型。<br>即：相关性召回+重要性召回 -&gt; 排序 </p>\n<h3 id=\"相关性\"><a href=\"#相关性\" class=\"headerlink\" title=\"相关性\"></a>相关性</h3><h4 id=\"文本相似性\"><a href=\"#文本相似性\" class=\"headerlink\" title=\"文本相似性\"></a>文本相似性</h4><ul>\n<li>标题精准匹配（Exact Match）<br>用户搜索“美团”，大概率是对“APP名称的搜索”，因此按照名称精准匹配，最简单最有效。</li>\n<li>标题模糊匹配（Fuzzy Match）<br>用户会打错字，用户会query打不全，因此模糊匹配非常重要。<ul>\n<li>拼音化<br>用户搜索“meituan”，也能够出来“美团”的结果，是因为将app名称进行拼音化再匹配。</li>\n<li>query纠错/改写<br>用户搜索“没团”，也能够出来“美团”的结果，是将query纠错后再匹配。</li>\n<li>编辑距离<br>用户搜索“美”，也能够出来“美团”的结果，是因为“美”和“美团”的编辑距离为1，比较小。<br>【编辑距离ref】：<a href=\"https://zh.wikipedia.org/wiki/%E7%B7%A8%E8%BC%AF%E8%B7%9D%E9%9B%A2\" target=\"_blank\" rel=\"external\">https://zh.wikipedia.org/wiki/%E7%B7%A8%E8%BC%AF%E8%B7%9D%E9%9B%A2</a></li>\n</ul>\n</li>\n<li>内容TF-IDF <ul>\n<li>TF(Term Frequency)<br>query分词后，词语在内容（描述，评论）中出现的次数，可以使0-1值，可以是次数，可以是log（次数）等等任意变种。</li>\n<li>IDF(Inverse Document Frequency)<br>query分词后，词语在query中的重要性，可以是0-1值，可以是log(N/Nt)，可以是log(1+N/Nt)等等任意变种。</li>\n<li>组合<br>IDF将query分词后设置词的权重，TF将词去和文档匹配，TF-IDF就是加权的词和文档的匹配。<br>【TF-IDF ref】<a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a></li>\n</ul>\n</li>\n<li>内容BM25<br>BM25是一种TF-IDF-like retrieval functions，即原理相通。BM25假设有多个词，即sum(每个词的TF-IDF)。<br>公式为：<br>score(D, Q) = sum(TF-IDF) D stands for doc, Q stands for query.</li>\n</ul>\n<p>TF: <img src=\"/2018/01/08/search/TF.png\" alt=\"[TF]\" title=\"[TF]\"></p>\n<ul>\n<li>f(qi, D) is qi’s term frequency in the document D</li>\n<li>|D| is the length of the document D in words</li>\n<li>avgdl is the average document length in the text collection from which documents are drawn.</li>\n<li>k1 and b are free parameters, usually chosen, in absence of an advanced optimization, as k1 in [1.2,2.0] and b = 0.75.</li>\n</ul>\n<p>IDF: <img src=\"/2018/01/08/search/IDF.png\" alt=\"[IDF]\" title=\"[IDF]\"></p>\n<ul>\n<li>N is the total number of documents in the collection</li>\n<li>n(qi) is the number of documents containing qi.</li>\n</ul>\n<p>【BM25 ref】<a href=\"https://en.wikipedia.org/wiki/Okapi_BM25\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Okapi_BM25</a></p>\n<h4 id=\"语义相似性\"><a href=\"#语义相似性\" class=\"headerlink\" title=\"语义相似性\"></a>语义相似性</h4><ul>\n<li>Topic Model<br>  乔布斯的苹果和水果店的苹果，识别语义。建议采用”维基预料训练“，而非query来训练，因为LDA对短文本效果不好。</li>\n<li>类型相关<br>  识别query的类型，比如搜索”聊天“，那就从”聊天软件“类目中召回APP。</li>\n</ul>\n<h3 id=\"重要性\"><a href=\"#重要性\" class=\"headerlink\" title=\"重要性\"></a>重要性</h3><h4 id=\"属性重要性\"><a href=\"#属性重要性\" class=\"headerlink\" title=\"属性重要性\"></a>属性重要性</h4><ul>\n<li>星级/评论挖掘</li>\n<li>曝光量/点击量/下载量/安装量/使用量/注册量/付费量/留存量（率）</li>\n<li>商业化价值</li>\n</ul>\n<h4 id=\"图重要性\"><a href=\"#图重要性\" class=\"headerlink\" title=\"图重要性\"></a>图重要性</h4><ul>\n<li>Page Rank值</li>\n</ul>\n<h4 id=\"其他召回\"><a href=\"#其他召回\" class=\"headerlink\" title=\"其他召回\"></a>其他召回</h4><ul>\n<li>x%用户下载召回</li>\n<li>新品召回</li>\n<li>运营召回</li>\n<li>个性化召回</li>\n</ul>\n<h3 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h3><ul>\n<li>点击熵<br>  识别是精准需求（文本相关）还是泛需求（语义相关）的指标，可以用来确定召回比例，也可以用来作为排序特征（此时需要交叉两个召回来源）。</li>\n<li>条件概率 p(ctr|query, user, scene)<br>给某人（对人的个性化），场景（对场景的把控，在机场推荐航班相关，晚饭时间推荐美食相关），query（用户对意图的主动描述）的最优化问题。</li>\n</ul>\n<h3 id=\"评估（IR-evaluation）\"><a href=\"#评估（IR-evaluation）\" class=\"headerlink\" title=\"评估（IR evaluation）\"></a>评估（IR evaluation）</h3><ul>\n<li>MAP: Mean Average Precision，不再赘述。</li>\n<li>nDCG: Normalized Discounted cumulative gain，按照位置加权。</li>\n</ul>\n<p>【IR metrics ref】<a href=\"http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.htmll\" target=\"_blank\" rel=\"external\">http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.htmll</a><br>【MAP vs NDCG ref】<a href=\"https://www.youtube.com/watch?v=qm1In7NH8WE\" target=\"_blank\" rel=\"external\">https://www.youtube.com/watch?v=qm1In7NH8WE</a></p>"},{"title":"做地铁和囚徒困境","date":"2017-10-26T02:06:09.000Z","_content":"早上做地铁，很挤，发现进地铁门，是个囚徒困境。\n* 环境\n    * 进地铁门，可以从左边上，可以从右边上\n    * 出地铁门，可以从左边下，右边下（标志是中间下，但门不大，不可能同时两边上中间下 ，尴尬）\n* 困境\n    * 两边同时上：因车上人要下来，所以谁都上不去\n    * 一边上一遍下：上去了的开心，下去了的开心，上不去的一边尴尬\n    * 两边先都不上：车上人迅速下完，开开心心上车\n* 结论\n    * 社会在惩罚遵守规则者\n    * 建议地铁门做大点儿","source":"_posts/subway.md","raw":"---\ntitle: 做地铁和囚徒困境\ndate: 2017-10-26 10:06:09\ntags:\n---\n早上做地铁，很挤，发现进地铁门，是个囚徒困境。\n* 环境\n    * 进地铁门，可以从左边上，可以从右边上\n    * 出地铁门，可以从左边下，右边下（标志是中间下，但门不大，不可能同时两边上中间下 ，尴尬）\n* 困境\n    * 两边同时上：因车上人要下来，所以谁都上不去\n    * 一边上一遍下：上去了的开心，下去了的开心，上不去的一边尴尬\n    * 两边先都不上：车上人迅速下完，开开心心上车\n* 结论\n    * 社会在惩罚遵守规则者\n    * 建议地铁门做大点儿","slug":"subway","published":1,"updated":"2017-10-26T02:07:10.211Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gbq001dtcinjbimuhvn","content":"<p>早上做地铁，很挤，发现进地铁门，是个囚徒困境。</p>\n<ul>\n<li>环境<ul>\n<li>进地铁门，可以从左边上，可以从右边上</li>\n<li>出地铁门，可以从左边下，右边下（标志是中间下，但门不大，不可能同时两边上中间下 ，尴尬）</li>\n</ul>\n</li>\n<li>困境<ul>\n<li>两边同时上：因车上人要下来，所以谁都上不去</li>\n<li>一边上一遍下：上去了的开心，下去了的开心，上不去的一边尴尬</li>\n<li>两边先都不上：车上人迅速下完，开开心心上车</li>\n</ul>\n</li>\n<li>结论<ul>\n<li>社会在惩罚遵守规则者</li>\n<li>建议地铁门做大点儿</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>早上做地铁，很挤，发现进地铁门，是个囚徒困境。</p>\n<ul>\n<li>环境<ul>\n<li>进地铁门，可以从左边上，可以从右边上</li>\n<li>出地铁门，可以从左边下，右边下（标志是中间下，但门不大，不可能同时两边上中间下 ，尴尬）</li>\n</ul>\n</li>\n<li>困境<ul>\n<li>两边同时上：因车上人要下来，所以谁都上不去</li>\n<li>一边上一遍下：上去了的开心，下去了的开心，上不去的一边尴尬</li>\n<li>两边先都不上：车上人迅速下完，开开心心上车</li>\n</ul>\n</li>\n<li>结论<ul>\n<li>社会在惩罚遵守规则者</li>\n<li>建议地铁门做大点儿</li>\n</ul>\n</li>\n</ul>\n"},{"title":"xgboost-n-spark","date":"2017-10-28T06:30:04.000Z","_content":"\n### xgboost\n* model: GBDT/GBRT/GBM\n* language: Python, R, Java, Scala, C++\n* integrity: single machine, Hadoop, Spark, Flink, DataFlow\n\n### xgboost和gbdt的区别\n>The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms.\nWhich is the reason why many people use xgboost. \nFor model, it might be more suitable to be called as regularized gradient boosting.\n\n### xgboost","source":"_posts/xgboost-n-spark.md","raw":"---\ntitle: xgboost-n-spark\ndate: 2017-10-28 14:30:04\ntags:\n---\n\n### xgboost\n* model: GBDT/GBRT/GBM\n* language: Python, R, Java, Scala, C++\n* integrity: single machine, Hadoop, Spark, Flink, DataFlow\n\n### xgboost和gbdt的区别\n>The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms.\nWhich is the reason why many people use xgboost. \nFor model, it might be more suitable to be called as regularized gradient boosting.\n\n### xgboost","slug":"xgboost-n-spark","published":1,"updated":"2017-10-28T06:34:38.485Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gbs001etcin22fnrbfr","content":"<h3 id=\"xgboost\"><a href=\"#xgboost\" class=\"headerlink\" title=\"xgboost\"></a>xgboost</h3><ul>\n<li>model: GBDT/GBRT/GBM</li>\n<li>language: Python, R, Java, Scala, C++</li>\n<li>integrity: single machine, Hadoop, Spark, Flink, DataFlow</li>\n</ul>\n<h3 id=\"xgboost和gbdt的区别\"><a href=\"#xgboost和gbdt的区别\" class=\"headerlink\" title=\"xgboost和gbdt的区别\"></a>xgboost和gbdt的区别</h3><blockquote>\n<p>The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms.<br>Which is the reason why many people use xgboost.<br>For model, it might be more suitable to be called as regularized gradient boosting.</p>\n</blockquote>\n<h3 id=\"xgboost-1\"><a href=\"#xgboost-1\" class=\"headerlink\" title=\"xgboost\"></a>xgboost</h3>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"xgboost\"><a href=\"#xgboost\" class=\"headerlink\" title=\"xgboost\"></a>xgboost</h3><ul>\n<li>model: GBDT/GBRT/GBM</li>\n<li>language: Python, R, Java, Scala, C++</li>\n<li>integrity: single machine, Hadoop, Spark, Flink, DataFlow</li>\n</ul>\n<h3 id=\"xgboost和gbdt的区别\"><a href=\"#xgboost和gbdt的区别\" class=\"headerlink\" title=\"xgboost和gbdt的区别\"></a>xgboost和gbdt的区别</h3><blockquote>\n<p>The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms.<br>Which is the reason why many people use xgboost.<br>For model, it might be more suitable to be called as regularized gradient boosting.</p>\n</blockquote>\n<h3 id=\"xgboost-1\"><a href=\"#xgboost-1\" class=\"headerlink\" title=\"xgboost\"></a>xgboost</h3>"},{"title":"tf_wnd","date":"2018-01-05T03:46:39.000Z","_content":"\n#### wide_n_deep code\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport logging\nimport json\nimport math\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\n# 读取文件\nreader = tf.TextLineReader(skip_header_lines = 0)\n\n# 文件列表\ntrain_input_files = [\"/root/xpguo/wnd/1.txt\", \"/root/xpguo/wnd/2.txt\"]\n\ninput_file_list = []\nfor input_file in train_input_files:\n    if len(input_file) > 0:\n        input_file_list.append(tf.train.match_filenames_once(input_file))\n\nfilename_queue = tf.train.string_input_producer(\n                tf.concat(input_file_list, axis = 0),\n                num_epochs = 10,     # strings are repeated num_epochs\n                shuffle = True,     # strings are randomly shuffled within each epoch\n                capacity = 512)\n\nbatch_size = 3\n\n(_, records) = reader.read_up_to(filename_queue, num_records = batch_size)\nsamples = tf.decode_csv(records, record_defaults = column_defaults, field_delim = ',')\nlabel = tf.cast(samples[self.column_dict[\"label\"]], dtype = tf.int32)\nfeature_dict = {}\nfor (key, value) in self.column_dict.items():\n    if key == \"label\" or value < 0 or value >= len(samples):\n        continue\n    if key in [\"user_features\", \"ads_features\"]:\n        feature_dict[key] = tf.string_split(samples[value], delimiter = ';')\n    if key in [\"user_weights\", \"ads_weights\"]:\n        feature_dict[key] = self.string_to_number(\n                tf.string_split(samples[value], delimiter = ';'),\n                dtype = tf.float32)\nreturn feature_dict, label\n```\n\n```\n\n```","source":"_posts/tf-wnd.md","raw":"---\ntitle: tf_wnd\ndate: 2018-01-05 11:46:39\ntags:\n---\n\n#### wide_n_deep code\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport logging\nimport json\nimport math\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\n# 读取文件\nreader = tf.TextLineReader(skip_header_lines = 0)\n\n# 文件列表\ntrain_input_files = [\"/root/xpguo/wnd/1.txt\", \"/root/xpguo/wnd/2.txt\"]\n\ninput_file_list = []\nfor input_file in train_input_files:\n    if len(input_file) > 0:\n        input_file_list.append(tf.train.match_filenames_once(input_file))\n\nfilename_queue = tf.train.string_input_producer(\n                tf.concat(input_file_list, axis = 0),\n                num_epochs = 10,     # strings are repeated num_epochs\n                shuffle = True,     # strings are randomly shuffled within each epoch\n                capacity = 512)\n\nbatch_size = 3\n\n(_, records) = reader.read_up_to(filename_queue, num_records = batch_size)\nsamples = tf.decode_csv(records, record_defaults = column_defaults, field_delim = ',')\nlabel = tf.cast(samples[self.column_dict[\"label\"]], dtype = tf.int32)\nfeature_dict = {}\nfor (key, value) in self.column_dict.items():\n    if key == \"label\" or value < 0 or value >= len(samples):\n        continue\n    if key in [\"user_features\", \"ads_features\"]:\n        feature_dict[key] = tf.string_split(samples[value], delimiter = ';')\n    if key in [\"user_weights\", \"ads_weights\"]:\n        feature_dict[key] = self.string_to_number(\n                tf.string_split(samples[value], delimiter = ';'),\n                dtype = tf.float32)\nreturn feature_dict, label\n```\n\n```\n\n```","slug":"tf-wnd","published":1,"updated":"2018-01-08T14:07:53.756Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjogm8gbt001ftcinp9nwa8be","content":"<h4 id=\"wide-n-deep-code\"><a href=\"#wide-n-deep-code\" class=\"headerlink\" title=\"wide_n_deep code\"></a>wide_n_deep code</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div></pre></td><td class=\"code\"><pre><div class=\"line\">from __future__ import absolute_import</div><div class=\"line\">from __future__ import division</div><div class=\"line\">from __future__ import print_function</div><div class=\"line\"></div><div class=\"line\">import sys</div><div class=\"line\">import os</div><div class=\"line\">import logging</div><div class=\"line\">import json</div><div class=\"line\">import math</div><div class=\"line\"></div><div class=\"line\">from six.moves import urllib</div><div class=\"line\">import tensorflow as tf</div><div class=\"line\"></div><div class=\"line\"># 读取文件</div><div class=\"line\">reader = tf.TextLineReader(skip_header_lines = 0)</div><div class=\"line\"></div><div class=\"line\"># 文件列表</div><div class=\"line\">train_input_files = [&quot;/root/xpguo/wnd/1.txt&quot;, &quot;/root/xpguo/wnd/2.txt&quot;]</div><div class=\"line\"></div><div class=\"line\">input_file_list = []</div><div class=\"line\">for input_file in train_input_files:</div><div class=\"line\">    if len(input_file) &gt; 0:</div><div class=\"line\">        input_file_list.append(tf.train.match_filenames_once(input_file))</div><div class=\"line\"></div><div class=\"line\">filename_queue = tf.train.string_input_producer(</div><div class=\"line\">                tf.concat(input_file_list, axis = 0),</div><div class=\"line\">                num_epochs = 10,     # strings are repeated num_epochs</div><div class=\"line\">                shuffle = True,     # strings are randomly shuffled within each epoch</div><div class=\"line\">                capacity = 512)</div><div class=\"line\"></div><div class=\"line\">batch_size = 3</div><div class=\"line\"></div><div class=\"line\">(_, records) = reader.read_up_to(filename_queue, num_records = batch_size)</div><div class=\"line\">samples = tf.decode_csv(records, record_defaults = column_defaults, field_delim = &apos;,&apos;)</div><div class=\"line\">label = tf.cast(samples[self.column_dict[&quot;label&quot;]], dtype = tf.int32)</div><div class=\"line\">feature_dict = &#123;&#125;</div><div class=\"line\">for (key, value) in self.column_dict.items():</div><div class=\"line\">    if key == &quot;label&quot; or value &lt; 0 or value &gt;= len(samples):</div><div class=\"line\">        continue</div><div class=\"line\">    if key in [&quot;user_features&quot;, &quot;ads_features&quot;]:</div><div class=\"line\">        feature_dict[key] = tf.string_split(samples[value], delimiter = &apos;;&apos;)</div><div class=\"line\">    if key in [&quot;user_weights&quot;, &quot;ads_weights&quot;]:</div><div class=\"line\">        feature_dict[key] = self.string_to_number(</div><div class=\"line\">                tf.string_split(samples[value], delimiter = &apos;;&apos;),</div><div class=\"line\">                dtype = tf.float32)</div><div class=\"line\">return feature_dict, label</div></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"></div></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h4 id=\"wide-n-deep-code\"><a href=\"#wide-n-deep-code\" class=\"headerlink\" title=\"wide_n_deep code\"></a>wide_n_deep code</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div></pre></td><td class=\"code\"><pre><div class=\"line\">from __future__ import absolute_import</div><div class=\"line\">from __future__ import division</div><div class=\"line\">from __future__ import print_function</div><div class=\"line\"></div><div class=\"line\">import sys</div><div class=\"line\">import os</div><div class=\"line\">import logging</div><div class=\"line\">import json</div><div class=\"line\">import math</div><div class=\"line\"></div><div class=\"line\">from six.moves import urllib</div><div class=\"line\">import tensorflow as tf</div><div class=\"line\"></div><div class=\"line\"># 读取文件</div><div class=\"line\">reader = tf.TextLineReader(skip_header_lines = 0)</div><div class=\"line\"></div><div class=\"line\"># 文件列表</div><div class=\"line\">train_input_files = [&quot;/root/xpguo/wnd/1.txt&quot;, &quot;/root/xpguo/wnd/2.txt&quot;]</div><div class=\"line\"></div><div class=\"line\">input_file_list = []</div><div class=\"line\">for input_file in train_input_files:</div><div class=\"line\">    if len(input_file) &gt; 0:</div><div class=\"line\">        input_file_list.append(tf.train.match_filenames_once(input_file))</div><div class=\"line\"></div><div class=\"line\">filename_queue = tf.train.string_input_producer(</div><div class=\"line\">                tf.concat(input_file_list, axis = 0),</div><div class=\"line\">                num_epochs = 10,     # strings are repeated num_epochs</div><div class=\"line\">                shuffle = True,     # strings are randomly shuffled within each epoch</div><div class=\"line\">                capacity = 512)</div><div class=\"line\"></div><div class=\"line\">batch_size = 3</div><div class=\"line\"></div><div class=\"line\">(_, records) = reader.read_up_to(filename_queue, num_records = batch_size)</div><div class=\"line\">samples = tf.decode_csv(records, record_defaults = column_defaults, field_delim = &apos;,&apos;)</div><div class=\"line\">label = tf.cast(samples[self.column_dict[&quot;label&quot;]], dtype = tf.int32)</div><div class=\"line\">feature_dict = &#123;&#125;</div><div class=\"line\">for (key, value) in self.column_dict.items():</div><div class=\"line\">    if key == &quot;label&quot; or value &lt; 0 or value &gt;= len(samples):</div><div class=\"line\">        continue</div><div class=\"line\">    if key in [&quot;user_features&quot;, &quot;ads_features&quot;]:</div><div class=\"line\">        feature_dict[key] = tf.string_split(samples[value], delimiter = &apos;;&apos;)</div><div class=\"line\">    if key in [&quot;user_weights&quot;, &quot;ads_weights&quot;]:</div><div class=\"line\">        feature_dict[key] = self.string_to_number(</div><div class=\"line\">                tf.string_split(samples[value], delimiter = &apos;;&apos;),</div><div class=\"line\">                dtype = tf.float32)</div><div class=\"line\">return feature_dict, label</div></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"></div></pre></td></tr></table></figure>"}],"PostAsset":[{"_id":"source/_posts/auc-n-logloss/3.png","slug":"3.png","post":"cjogm8g900005tcins45dq19i","modified":1,"renderable":0},{"_id":"source/_posts/lda/9.png","slug":"9.png","post":"cjogm8gbe0014tcinf77og2ku","modified":1,"renderable":0},{"_id":"source/_posts/auc-n-logloss/1.gif","slug":"1.gif","post":"cjogm8g900005tcins45dq19i","modified":1,"renderable":0},{"_id":"source/_posts/cnn/alexNet.png","slug":"alexNet.png","post":"cjogm8gam000stcing5cwgvfq","modified":1,"renderable":0},{"_id":"source/_posts/lda/302.png","slug":"302.png","post":"cjogm8gbe0014tcinf77og2ku","modified":1,"renderable":0},{"_id":"source/_posts/feature-engineer/զ%A6ڥ%E4%FE%F6+%B5%AB%C1.png","post":"cjogm8gaz000vtcinyyg5ys4i","slug":"զ%A6ڥ%E4%FE%F6+%B5%AB%C1.png","modified":1,"renderable":1},{"_id":"source/_posts/hivemall/hivemall_1.PNG","post":"cjogm8gb4000xtcinuadzbh8v","slug":"hivemall_1.PNG","modified":1,"renderable":1},{"_id":"source/_posts/ee-n-dqn/1.png","post":"cjogm8gaj000qtcinj59u112i","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_posts/ee-n-dqn/2.png","post":"cjogm8gaj000qtcinj59u112i","slug":"2.png","modified":1,"renderable":1},{"_id":"source/_posts/logit-n-probit/logistic.png","post":"cjogm8gba0012tcinal0kduj2","slug":"logistic.png","modified":1,"renderable":1},{"_id":"source/_posts/logit-n-probit/logit-logistic-relation.jpg","post":"cjogm8gba0012tcinal0kduj2","slug":"logit-logistic-relation.jpg","modified":1,"renderable":1},{"_id":"source/_posts/logit-n-probit/logit.png","post":"cjogm8gba0012tcinal0kduj2","slug":"logit.png","modified":1,"renderable":1},{"_id":"source/_posts/auc-n-logloss/2.png","post":"cjogm8g900005tcins45dq19i","slug":"2.png","modified":1,"renderable":1},{"_id":"source/_posts/auc-n-logloss/4.png","post":"cjogm8g900005tcins45dq19i","slug":"4.png","modified":1,"renderable":1},{"_id":"source/_posts/auc-n-logloss/6.png","post":"cjogm8g900005tcins45dq19i","slug":"6.png","modified":1,"renderable":1},{"_id":"source/_posts/auc-n-logloss/7.png","post":"cjogm8g900005tcins45dq19i","slug":"7.png","modified":1,"renderable":1},{"_id":"source/_posts/auc-n-logloss/8.png","post":"cjogm8g900005tcins45dq19i","slug":"8.png","modified":1,"renderable":1},{"_id":"source/_posts/auc-n-logloss/9.png","post":"cjogm8g900005tcins45dq19i","slug":"9.png","modified":1,"renderable":1},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A51.1.png","slug":"%D5%F8%A51.1.png","post":"cjogm8g8o0002tcin1u7onrsn","modified":1,"renderable":0},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A52.2.png","post":"cjogm8g8o0002tcin1u7onrsn","slug":"%D5%F8%A52.2.png","modified":1,"renderable":1},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A53.3.jpg","slug":"%D5%F8%A53.3.jpg","post":"cjogm8g8o0002tcin1u7onrsn","modified":1,"renderable":0},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A53.3.png","slug":"%D5%F8%A53.3.png","post":"cjogm8g8o0002tcin1u7onrsn","modified":1,"renderable":0},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A54.4.png","post":"cjogm8g8o0002tcin1u7onrsn","slug":"%D5%F8%A54.4.png","modified":1,"renderable":1},{"_id":"source/_posts/ctr-recalibration/图1.1.png","slug":"图1.1.png","post":"cjogm8g8o0002tcin1u7onrsn","modified":1,"renderable":0},{"_id":"source/_posts/ctr-recalibration/图2.2.png","post":"cjogm8g8o0002tcin1u7onrsn","slug":"图2.2.png","modified":1,"renderable":1},{"_id":"source/_posts/ctr-recalibration/图3.3.jpg","slug":"图3.3.jpg","post":"cjogm8g8o0002tcin1u7onrsn","modified":1,"renderable":0},{"_id":"source/_posts/ctr-recalibration/图3.3.png","slug":"图3.3.png","post":"cjogm8g8o0002tcin1u7onrsn","modified":1,"renderable":0},{"_id":"source/_posts/ctr-recalibration/图4.4.png","post":"cjogm8g8o0002tcin1u7onrsn","slug":"图4.4.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/1.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/100.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"100.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/101.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"101.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/102.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"102.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/103.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"103.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/104.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"104.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/105.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"105.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/106.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"106.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/2.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"2.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/201.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"201.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/3.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"3.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/301.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"301.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/4.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"4.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/401.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"401.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/402.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"402.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/5.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"5.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/501.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"501.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/6.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"6.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/601.jpg","post":"cjogm8gbe0014tcinf77og2ku","slug":"601.jpg","modified":1,"renderable":1},{"_id":"source/_posts/lda/7.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"7.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/8.png","post":"cjogm8gbe0014tcinf77og2ku","slug":"8.png","modified":1,"renderable":1},{"_id":"source/_posts/word2vec/cbow-hs-hand.png","slug":"cbow-hs-hand.png","post":"cjogm8gbl0019tcin5kl8ci0s","modified":1,"renderable":0},{"_id":"source/_posts/search/IDF.png","post":"cjogm8gbo001btcinj1bzt215","slug":"IDF.png","modified":1,"renderable":1},{"_id":"source/_posts/search/TF.png","post":"cjogm8gbo001btcinj1bzt215","slug":"TF.png","modified":1,"renderable":1},{"_id":"source/_posts/cnn/cnn002.png","post":"cjogm8gam000stcing5cwgvfq","slug":"cnn002.png","modified":1,"renderable":1},{"_id":"source/_posts/cnn/cnn003.png","slug":"cnn003.png","post":"cjogm8gam000stcing5cwgvfq","modified":1,"renderable":0},{"_id":"source/_posts/cnn/cnn004.png","post":"cjogm8gam000stcing5cwgvfq","slug":"cnn004.png","modified":1,"renderable":1},{"_id":"source/_posts/cnn/lenet.png","post":"cjogm8gam000stcing5cwgvfq","slug":"lenet.png","modified":1,"renderable":1},{"_id":"source/_posts/cnn/logistic.png","post":"cjogm8gam000stcing5cwgvfq","slug":"logistic.png","modified":1,"renderable":1},{"_id":"source/_posts/word2vec/cbow-1.png","post":"cjogm8gbl0019tcin5kl8ci0s","slug":"cbow-1.png","modified":1,"renderable":1},{"_id":"source/_posts/word2vec/cbow-hs-2.png","post":"cjogm8gbl0019tcin5kl8ci0s","slug":"cbow-hs-2.png","modified":1,"renderable":1},{"_id":"source/_posts/word2vec/cbow-hs-code.png","post":"cjogm8gbl0019tcin5kl8ci0s","slug":"cbow-hs-code.png","modified":1,"renderable":1},{"_id":"source/_posts/word2vec/cbow-hs-hand.jpg","post":"cjogm8gbl0019tcin5kl8ci0s","slug":"cbow-hs-hand.jpg","modified":1,"renderable":1},{"_id":"source/_posts/word2vec/cbow-hs.png","post":"cjogm8gbl0019tcin5kl8ci0s","slug":"cbow-hs.png","modified":1,"renderable":1},{"_id":"source/_posts/word2vec/tfboard.png","slug":"tfboard.png","post":"cjogm8gbl0019tcin5kl8ci0s","modified":1,"renderable":0}],"PostCategory":[{"post_id":"cjogm8g900005tcins45dq19i","category_id":"cjogm8g9o0008tcin6khce5j5","_id":"cjogm8ga8000htcin4s98jugn"},{"post_id":"cjogm8gaz000vtcinyyg5ys4i","category_id":"cjogm8g9o0008tcin6khce5j5","_id":"cjogm8gb70010tcin3foeh54l"},{"post_id":"cjogm8gbe0014tcinf77og2ku","category_id":"cjogm8gbi0016tcinjyft21cx","_id":"cjogm8gbq001ctcinhx8vrjmf"}],"PostTag":[{"post_id":"cjogm8g8a0000tcinywk3w626","tag_id":"cjogm8g8u0004tcinfgzgs7yn","_id":"cjogm8g9v000btcin0bf6j59a"},{"post_id":"cjogm8g8o0002tcin1u7onrsn","tag_id":"cjogm8g9s000atcinzm7o02f4","_id":"cjogm8ga6000ftcint9r5lih8"},{"post_id":"cjogm8ga8000itcinhb1ldvqg","tag_id":"cjogm8gac000ktcinn5fbyscv","_id":"cjogm8gaj000ptcinb1w92tyd"},{"post_id":"cjogm8gaf000mtcinyuvaiod8","tag_id":"cjogm8gaj000otcin8fgdjv9v","_id":"cjogm8gaz000utcinc5ele074"},{"post_id":"cjogm8gah000ntcinowujpj7i","tag_id":"cjogm8gay000ttcin9idkl7a8","_id":"cjogm8gb5000ytcinenfbud86"}],"Tag":[{"name":"机器学习","_id":"cjogm8g8u0004tcinfgzgs7yn"},{"name":"ML","_id":"cjogm8g9s000atcinzm7o02f4"},{"name":"数据挖掘","_id":"cjogm8gac000ktcinn5fbyscv"},{"name":"DNN","_id":"cjogm8gaj000otcin8fgdjv9v"},{"name":"Search","_id":"cjogm8gay000ttcin9idkl7a8"}]}}