{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon.png","path":"images/apple-touch-icon.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16.png","path":"images/favicon-16x16.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32.png","path":"images/favicon-32x32.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1521125091000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1521125091000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1521125091000},{"_id":"themes/next/.gitignore","hash":"32ea93f21d8693d5d8fa4eef1c51a21ad0670047","modified":1521125091000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1521125091000},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1521125091000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1521125091000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1521125091000},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1521125091000},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1521125091000},{"_id":"themes/next/README.cn.md","hash":"6d9177e7dad87e6129760e4b559bd3f7a15429d7","modified":1521125091000},{"_id":"themes/next/README.md","hash":"aba736f1b934f2b169035ccf94d2771a270ec21d","modified":1521125091000},{"_id":"themes/next/bower.json","hash":"6d6ae7531cf3fedc97c58cdad664f5793eb3cc88","modified":1521125091000},{"_id":"themes/next/_config.yml","hash":"9f08a25d730c241f8dad5ec287c93d7046cdc4d7","modified":1521125091000},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1521125091000},{"_id":"themes/next/package.json","hash":"2174d61cbc9fa276a82b02649426842f088825ab","modified":1521125091000},{"_id":"source/about/index.md","hash":"042801186911e92958bdac7db710ef766de40ad5","modified":1521125091000},{"_id":"source/about/tmp.md","hash":"c5f947754ed8b9d17e4f21e757b023815fc6464e","modified":1521125091000},{"_id":"source/_posts/.DS_Store","hash":"ff11f73ef665561d7cb9c5b1b01e160e1e3bf2a8","modified":1521131908000},{"_id":"source/_posts/MachineLearning-ZhouZhihua.md","hash":"f5cd25ab0d5604e733fabd54bf9bf446c62f4b3c","modified":1521125091000},{"_id":"source/_posts/auc-n-logloss.md","hash":"d1e53e66b357200aacc9e3a4f86c8e5956381bf2","modified":1521125091000},{"_id":"source/_posts/bayes.md","hash":"da5a01b1061d0a113c52a9a770efe39a4635652f","modified":1521125091000},{"_id":"source/_posts/cnn.md","hash":"b8df761ed0413246cb51a426040e958d1640633c","modified":1521391678000},{"_id":"source/_posts/ctr-recalibration.md","hash":"f554b003a1536f70058f7bb54dded5ee9559b272","modified":1521125091000},{"_id":"source/_posts/ctr-smooth.md","hash":"5144129afe1f2305ebbf3fc3fb268a0469662a01","modified":1521125091000},{"_id":"source/_posts/distributed-tf.md","hash":"eb6acc7fa256955562047dedeb4cde1ba9b36e89","modified":1521125091000},{"_id":"source/_posts/dt.md","hash":"2476ee0ab18ab3c62dc7e15d2b1313b86dcc9eb8","modified":1521125091000},{"_id":"source/_posts/edge-rank.md","hash":"d9aae783490f4a28d46c85db00f169abe1c3a961","modified":1521125091000},{"_id":"source/_posts/ee-n-dqn.md","hash":"3e3e530ecc8a221e15f1ee33747dc0a9130bb44f","modified":1521125091000},{"_id":"source/_posts/exploding-n-vanishing.md","hash":"334feb9c1c7d8d54de7ac666bad3915a701527c0","modified":1521125091000},{"_id":"source/_posts/feature-engineer.md","hash":"87442b2d6974e50539fe528ae5f79197258efbfe","modified":1521125091000},{"_id":"source/_posts/kafka.md","hash":"2eb2e7ba8f784e99d73a1482d6086c8f7ce33b17","modified":1521125091000},{"_id":"source/_posts/lda.md","hash":"11f3542771d9fde54391356c0c685bf227798e1c","modified":1521125091000},{"_id":"source/_posts/logit-n-probit.md","hash":"f0f7b68a197f40a407f6796254ba969caa6fcec9","modified":1521125091000},{"_id":"source/_posts/pr.md","hash":"95df36fad445213b9764759ef8bdcc6ba097e2c1","modified":1521125091000},{"_id":"source/_posts/search.md","hash":"4533900f80529c6da50cabfe6c58493a2acd9123","modified":1521125091000},{"_id":"source/_posts/spark-streaming.md","hash":"6df77a59da7947b18d3d15ee565a13abee849b11","modified":1521125091000},{"_id":"source/_posts/subway.md","hash":"e4203e9934ba7fa5c1e3eb8bde625f22799d5345","modified":1521125091000},{"_id":"source/_posts/tf-wnd.md","hash":"8d02a0ac02b5f9e86fd898059dd42b77dee710af","modified":1521125091000},{"_id":"source/_posts/xgboost-n-spark.md","hash":"0e6f3e5c62acb7174e4553e8fa784eeca7c86d65","modified":1521125091000},{"_id":"source/categories/index.md","hash":"b6ca09b85da4e8566672b6dc618aa8a752670e28","modified":1521125091000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1521125091000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"352093a1b210c72136687fd2eee649244cee402c","modified":1521125091000},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1521125091000},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1521125091000},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1521125091000},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1521125091000},{"_id":"themes/next/languages/en.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1521125091000},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1521125091000},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1521125091000},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1521125091000},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1521125091000},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1521125091000},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1521125091000},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1521125091000},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1521125091000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"b342544b715da19d982349017bd56c5aaca11f71","modified":1521125091000},{"_id":"themes/next/languages/zh-hk.yml","hash":"2ef272bcb1f325480f59f1e2ab95584de3c6b8da","modified":1521125091000},{"_id":"themes/next/languages/zh-tw.yml","hash":"c53941a2eaac8e3a2f8dacc73ed555d3c6c5bd59","modified":1521125091000},{"_id":"themes/next/layout/_layout.swig","hash":"72a1a2612f7c14cc9af51c55c8dfac39d6c0a2bf","modified":1521125091000},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1521125091000},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1521125091000},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1521125091000},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1521125091000},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1521125091000},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1521125091000},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1521125091000},{"_id":"themes/next/scripts/merge-configs.js","hash":"cb617ddf692f56e6b6129564d52e302f50b28243","modified":1521125091000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1521125091000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1521125091000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1521125091000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1521125091000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1521125091000},{"_id":"source/_posts/auc-n-logloss/2.png","hash":"4f007082dc559888d479918e5419eff48391a830","modified":1521125091000},{"_id":"source/_posts/auc-n-logloss/7.png","hash":"80956992425cd24224413be93dd711b23b33d5ff","modified":1521125091000},{"_id":"source/_posts/auc-n-logloss/6.png","hash":"bf400d2df199beb6cd11f0724934b37ec6a9e5e4","modified":1521125091000},{"_id":"source/_posts/auc-n-logloss/8.png","hash":"2b160d7859c0433086fb14404165ee6db04ea68c","modified":1521125091000},{"_id":"source/_posts/auc-n-logloss/9.png","hash":"8bdd4379bb480099a83fa956b65c4bd30ce223f5","modified":1521125091000},{"_id":"source/_posts/cnn/logistic.png","hash":"d14da93bd72cd7c5880caf24323fb0af3b863162","modified":1521125091000},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A54.4.png","hash":"816d988a281cf7d1dac9a856919abc4e7ea8ea16","modified":1521125091000},{"_id":"source/_posts/ctr-recalibration/图4.4.png","hash":"816d988a281cf7d1dac9a856919abc4e7ea8ea16","modified":1521391678000},{"_id":"source/_posts/ee-n-dqn/2.png","hash":"6515750b1ed88b7f6bf60910052c0b64820f7d82","modified":1521125091000},{"_id":"source/_posts/ee-n-dqn/1.png","hash":"d0ba7aa13143ebf0d5987f1cb2487a7944ea36cf","modified":1521125091000},{"_id":"source/_posts/feature-engineer/զ%A6ڥ%E4%FE%F6+%B5%AB%C1.png","hash":"2141bdfa44e5cf86009924a72dca0946f5ca9601","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E71.png","hash":"18b316e12d4591fa543bcb5db5ea8dd8fb089ed8","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7100.png","hash":"9b36fbe342f6fe0c7ebd224c6bc4024c77509e2f","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7101.png","hash":"e8a736596eaab0a2863ebaf7cfc2806c05cf2b48","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7103.png","hash":"51596b033c3ef8457f11590c7eb35c9be0f08c30","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7104.png","hash":"dd5fe476184597c8871fd0a581ae15fd5528f862","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7106.png","hash":"e24199f39e09a9034f4c37b9dd6d56060dd71518","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E72.png","hash":"4efddf70286df1d8b3ae4f69597eeb9f159ce682","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7105.png","hash":"a46d624d3e25bfdfe15dc68dc2d7e17ab2bda16f","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E73.png","hash":"63dc46e8d0254975359f8c02d560a417bcc7686c","modified":1521125091000},{"_id":"source/_posts/logit-n-probit/logistic.png","hash":"d14da93bd72cd7c5880caf24323fb0af3b863162","modified":1521125091000},{"_id":"source/_posts/logit-n-probit/logit-logistic-relation.jpg","hash":"4bc9396bf4c63a5e6e03efad1f0d08a3226c5127","modified":1521125091000},{"_id":"source/_posts/logit-n-probit/logit.png","hash":"318241cd887a667d301d8bd068d6791da4dba7ab","modified":1521125091000},{"_id":"source/_posts/search/IDF.png","hash":"a9d19b122aaad81b2ebdf7ac47382bda3a9a6d30","modified":1521125091000},{"_id":"source/_posts/search/TF.png","hash":"a1e40176a572fccb513815595fc6288543bd0a1c","modified":1521125091000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1521125091000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1521125091000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1521125091000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1521125091000},{"_id":"themes/next/layout/_macro/post.swig","hash":"767e1d5503ecce85f577c8fb673a3503b65484ce","modified":1521125091000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1521125091000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"9efc455894921a66bbc074055d3b39c8a34a48a4","modified":1521125091000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1521125091000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"0fbeb56e9c4d5193c6a181d45c4b1b7a44a0e027","modified":1521125091000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"c4d6181f5d3db5365e622f78714af8cc58d7a45e","modified":1521125091000},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1521125091000},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1521125091000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1521125091000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1521125091000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1521125091000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1521125091000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1521125091000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9baf90f7c40b3b10f288e9268c3191e895890cea","modified":1521125091000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1521125091000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1521125091000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1521125091000},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1521125091000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1521125091000},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1521125091000},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1521125091000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1521125091000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1521125091000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1521125091000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1521125091000},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1521125091000},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1521125091000},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1521125091000},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1521125091000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1521125091000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1521125091000},{"_id":"themes/next/source/images/apple-touch-icon.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1521125091000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1521125091000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1521125091000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1521125091000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1521125091000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1521125091000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1521125091000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1521125091000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1521125091000},{"_id":"themes/next/source/images/favicon-16x16.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1521125091000},{"_id":"themes/next/source/images/favicon-32x32.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1521125091000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1521125091000},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1521125091000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1521125091000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1521125091000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1521125091000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1521125091000},{"_id":"source/_posts/auc-n-logloss/4.png","hash":"768177ead93284112e04805f9738479f3ab2fbf3","modified":1521125091000},{"_id":"source/_posts/cnn/cnn002.png","hash":"085da484b2f03ebab31484861b2a47a4c660bec9","modified":1521125091000},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A52.2.png","hash":"ab179418ff1eeb992aaec1db8a7299c180cbd8b9","modified":1521125091000},{"_id":"source/_posts/ctr-recalibration/图2.2.png","hash":"ab179418ff1eeb992aaec1db8a7299c180cbd8b9","modified":1521391677000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7102.png","hash":"10e75aed2f2f3cdee5972f3c4779a2fe34388505","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7201.png","hash":"1b600e59fec7e58cccbf38dced4a4d8953e3a5b8","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E74.png","hash":"ec1cc3151a7a8db835a6150dac1b3109898f1134","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E75.png","hash":"f59c3d0cea4445366028261080d4e1945b8b1a78","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E76.png","hash":"d2f2c684ef148b4aab12d54f9e1b14829bc0bee9","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E77.png","hash":"48fc762aa53ae47d05caf6937ded51e9fcf9dded","modified":1521125091000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E78.png","hash":"be892636e7f7a78f8d86b2898f87ca31295007db","modified":1521125091000},{"_id":"source/_posts/lda/301.png","hash":"493c179e66444d0e3f92ffc70fa12a2914c36fcd","modified":1521125091000},{"_id":"source/_posts/lda/401.png","hash":"d9b18be14991ff22d5beb80e85bbc56fc5e599b3","modified":1521125091000},{"_id":"source/_posts/lda/501.png","hash":"d671c983e48fc27e61c2690f8d2de05cb39635b1","modified":1521125091000},{"_id":"source/_posts/lda/601.jpg","hash":"1011ca8d03ebc9849b5ab510f6a10ee0366fdb47","modified":1521125091000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1521125091000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1521125091000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1521125091000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1521125091000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1521125091000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1521125091000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1521125091000},{"_id":"source/_posts/auc-n-logloss/3.png","hash":"e60e44b56a987bef83090ab1ac4a1d50b788e7ac","modified":1521125091000},{"_id":"source/_posts/cnn/lenet.png","hash":"1a63ac6a9fce5a59b243adc71965e8b55286d654","modified":1521133476000},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E79.png","hash":"17570e2afd53b60ed2b6558c1ed88fa3b6742a4f","modified":1521125091000},{"_id":"source/_posts/lda/402.png","hash":"fb0d940ff0c339a33ce2056c6089c1d1e2544aeb","modified":1521125091000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1521125091000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1521125091000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1521125091000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1521125091000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1521125091000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1521125091000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1521125091000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1521125091000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1521125091000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1521125091000},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1521125091000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1521125091000},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1521125091000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1521125091000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1521125091000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1521125091000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1521125091000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1521125091000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1521125091000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1521125091000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1521125091000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1521125091000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1521125091000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1521125091000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1521125091000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1521125091000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1521125091000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1521125091000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"576e716893153a855eaf6d136fad7cb6d4065e09","modified":1521125091000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1521125091000},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"9f4ed36c73e890909b8ebbe601fb60e13d048288","modified":1521125091000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1521125091000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1521125091000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1521125091000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1521125091000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1521125091000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1521125091000},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1521125091000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1521125091000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1521125091000},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1521125091000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1521125091000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"ec79c23f1956bade7bcaa7189d97b7463b8f9f75","modified":1521125091000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"4df88b33f4bd31b872b5c842405267256024f75a","modified":1521125091000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1521125091000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1521125091000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1521125091000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1521125091000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1521125091000},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1521125091000},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1521125091000},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1521125091000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1521125091000},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1521125091000},{"_id":"themes/next/source/js/src/utils.js","hash":"6b0eeeb9dda4a7c94c1c4f6fafd2c801da6e8f96","modified":1521125091000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1521125091000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1521125091000},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1521125091000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1521125091000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1521125091000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1521125091000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1521125091000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1521125091000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1521125091000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1521125091000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1521125091000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1521125091000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1521125091000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1521125091000},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1521125091000},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1521125091000},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1521125091000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1521125091000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1521125091000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1521125091000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1521125091000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1521125091000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1521125091000},{"_id":"source/_posts/auc-n-logloss/1.gif","hash":"6aa137ccf577a9689570505ad4fa5eac52a784c0","modified":1521125091000},{"_id":"source/_posts/cnn/alexNet.png","hash":"2abd32ba6a560379605ac5d554cfa0b10bb3253b","modified":1521133700000},{"_id":"source/_posts/lda/302.png","hash":"119cb4b9ba91acfb35ec5338011d1a196b296a72","modified":1521125091000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1521125091000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1521125091000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1521125091000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1521125091000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1521125091000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1521125091000},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1521125091000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1521125091000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1521125091000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"5e6c9f8a730b78c7ce5572d327c2a7311c3609b9","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"f3991aeca25d0814f5cea800b58f25d0222cd246","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1521125091000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1521125091000},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1521125091000},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1521125091000},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1521125091000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1521125091000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1521125091000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1521125091000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1521125091000},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1521125091000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1521125091000},{"_id":"source/_posts/cnn/cnn003.png","hash":"5f925b3cabb4d20d3b212c546aaeeab664d54555","modified":1521133073000},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A51.1.png","hash":"8fc18abcde73fb0194e8eab4e1da7070fe0e0d5b","modified":1521125091000},{"_id":"source/_posts/ctr-recalibration/图1.1.png","hash":"8fc18abcde73fb0194e8eab4e1da7070fe0e0d5b","modified":1521391677000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"f958da71d211f592ca64d0cf1328d801fffd3179","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"08a500b2984f109b751f3697ca33172d1340591a","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"fb0d3ae0f0c26393199de8f81fb3658d86fbbfaf","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"77c92a449ce84d558d26d052681f2e0dd77c70c9","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"45df0cf4c97b47e05573bcd41028ee50f3fdf432","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1521125091000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"aeff0e6e23725e8baea27c890ccbbf466024f767","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1521125091000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1521125091000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1521125091000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1521125091000},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1521125091000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1521125091000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1521125091000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1521125091000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1521125091000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1521125091000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1521125091000},{"_id":"source/_posts/ctr-recalibration/图3.3.jpg","hash":"0d0d141f6967b62ec615766d43911993eea99b9a","modified":1521391677000},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A53.3.jpg","hash":"0d0d141f6967b62ec615766d43911993eea99b9a","modified":1521125091000},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A53.3.png","hash":"21b3b0eb1a89a2c3723673e632e463b17a0428b3","modified":1521125091000},{"_id":"source/_posts/ctr-recalibration/图3.3.png","hash":"21b3b0eb1a89a2c3723673e632e463b17a0428b3","modified":1521391678000},{"_id":"public/about/index.html","hash":"cda4121f994f838f6729b12bdea6cdb246ae5ccc","modified":1521392059445},{"_id":"public/about/tmp.html","hash":"94f62b413e047a4387e67043c79c9fc832991183","modified":1521392059445},{"_id":"public/categories/index.html","hash":"b4faaeb52dc39c040aac001c78c874b930fe3c95","modified":1521392059446},{"_id":"public/2018/01/16/bayes/index.html","hash":"13fe59fac9aafe97ec36035f99df500753a852c1","modified":1521392059446},{"_id":"public/2018/01/15/edge-rank/index.html","hash":"ab439eb46789349682559521eab722f6eb4d3bcb","modified":1521392059446},{"_id":"public/2017/12/15/kafka/index.html","hash":"35661732bebc91c81a593d3361b8b4d8629ecc19","modified":1521392059446},{"_id":"public/2017/12/14/distributed-tf/index.html","hash":"9dc674a77c52dc8ef6335b31035b7179262ad112","modified":1521392059446},{"_id":"public/2017/12/13/dt/index.html","hash":"4d7d927e14c495961efe5750bb1f152980789d43","modified":1521392059446},{"_id":"public/2017/11/23/exploding-n-vanishing/index.html","hash":"80234431a8b80e1cae76b1665b34d3b536608763","modified":1521392059446},{"_id":"public/2017/11/17/spark-streaming/index.html","hash":"5fc2d0d491c263c395947888e75f45782c510e85","modified":1521392059446},{"_id":"public/2017/10/28/xgboost-n-spark/index.html","hash":"9756f629d8172625fb9cceb53f99f945b904148e","modified":1521392059447},{"_id":"public/2017/10/26/subway/index.html","hash":"0bc60b9036a3f7f886824d2329489231ecf9ebe4","modified":1521392059447},{"_id":"public/2017/10/15/pr/index.html","hash":"5663d233521bda9b963e86744433131616ab14ca","modified":1521392059447},{"_id":"public/2017/10/10/logit-n-probit/index.html","hash":"33e968aac7ce381d60b6b80b0707b16505c7f69e","modified":1521392059447},{"_id":"public/archives/page/3/index.html","hash":"79a717a32d7fa7c8fbf2f4f722ca39946fe05045","modified":1521392059447},{"_id":"public/archives/2017/page/2/index.html","hash":"4ec34cb8732de1e3c2182caac7ba7f7966a69232","modified":1521392059447},{"_id":"public/archives/2017/10/index.html","hash":"4f0c5cb88d895c9d7d393eef5994f02d503456a4","modified":1521392059448},{"_id":"public/archives/2017/11/index.html","hash":"4f1c518ef2de4f19432df2a10cdd7e10d0269148","modified":1521392059448},{"_id":"public/archives/2017/12/index.html","hash":"3f5f012d46933977e3b511a79aa35dc5ddb10a15","modified":1521392059448},{"_id":"public/archives/2018/index.html","hash":"5191a364102635e703f7fbe7fa149d2d00c78b9e","modified":1521392059448},{"_id":"public/archives/2018/01/index.html","hash":"37f6e75aff0fcad0d16b6bf6e9b8b2d2ebdb5797","modified":1521392059448},{"_id":"public/archives/2018/03/index.html","hash":"8039b84a8f360c4f893b16602a6f2dfba694affe","modified":1521392059448},{"_id":"public/categories/机器学习/index.html","hash":"08f4525eb1089e34207964300ea6c78025116981","modified":1521392059448},{"_id":"public/categories/数据挖掘/index.html","hash":"93b92a95affe11214b0979f3f19984a2f1d22843","modified":1521392059448},{"_id":"public/page/3/index.html","hash":"823d862f6b9529b9fc65ef6f64f12789916b5ba6","modified":1521392059448},{"_id":"public/tags/ML/index.html","hash":"453f8e05155d9b70c086118ec4e699b697817380","modified":1521392059448},{"_id":"public/tags/DNN/index.html","hash":"cf2cf1eb55920c0996ff1f71890ee022a2d1f4c4","modified":1521392059448},{"_id":"public/2018/03/14/cnn/index.html","hash":"0fd7074110d3603ddf2ac1ff864ffe6b3488e4ac","modified":1521392059448},{"_id":"public/2018/01/08/search/index.html","hash":"10258f67d010f4c984087dddd7f0be76a6bbc145","modified":1521392059448},{"_id":"public/2018/01/05/tf-wnd/index.html","hash":"a63456f488a6162a69f4c2df9783fa6ab3232bec","modified":1521392059448},{"_id":"public/2017/12/07/ctr-smooth/index.html","hash":"837a35152ec8ec32fcb39aa4fbf9c66cabcc8ad5","modified":1521392059448},{"_id":"public/2017/11/27/MachineLearning-ZhouZhihua/index.html","hash":"069741004ed6847815081ac4fe519ada8e62f1d3","modified":1521392059448},{"_id":"public/2017/11/23/ctr-recalibration/index.html","hash":"81dfaf3bc78703ac463d10f0db9ba4ba4d5b17aa","modified":1521392059448},{"_id":"public/2017/10/24/ee-n-dqn/index.html","hash":"c8f1334db8335a775aff2f13c0fdb35182ac7ba7","modified":1521392059448},{"_id":"public/2017/10/06/auc-n-logloss/index.html","hash":"0858ffdac5d82fb001b454c95b4a7752450e7fb2","modified":1521392059448},{"_id":"public/2017/10/05/lda/index.html","hash":"214b2d2f54e35a7dfacda1df06b93e62476b1662","modified":1521392059448},{"_id":"public/2017/10/02/feature-engineer/index.html","hash":"5f516167ac4d7120aee364888f8d4b00561e0f17","modified":1521392059449},{"_id":"public/archives/index.html","hash":"c6fa9c9c79dc1493f4ac8deae52f706cf9240fd8","modified":1521392059449},{"_id":"public/archives/page/2/index.html","hash":"0ae9443e4d68f6367d51808ead287e7c7b9a96ec","modified":1521392059449},{"_id":"public/archives/2017/index.html","hash":"b6f716b40201782ccb4fafd79bd66b37879b6d33","modified":1521392059449},{"_id":"public/index.html","hash":"837fa3fb3f3feb76c488a3f4ab28f4393d199445","modified":1521392059449},{"_id":"public/page/2/index.html","hash":"6f3d42d5a83cc213fae1063d168eaef955177830","modified":1521392059449},{"_id":"public/images/apple-touch-icon.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1521392059470},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1521392059470},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1521392059470},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1521392059470},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1521392059470},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1521392059470},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1521392059470},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1521392059470},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1521392059471},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1521392059471},{"_id":"public/images/favicon-16x16.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1521392059471},{"_id":"public/images/favicon-32x32.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1521392059471},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1521392059471},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1521392059471},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1521392059471},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1521392059471},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1521392059471},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1521392059471},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1521392059471},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1521392059471},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1521392059471},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1521392059471},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1521392059471},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1521392059471},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1521392059471},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1521392059471},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1521392059471},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1521392059472},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1521392059472},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1521392059472},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1521392059472},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1521392059472},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1521392059472},{"_id":"public/2017/10/02/feature-engineer/զ%A6ڥ%E4%FE%F6+%B5%AB%C1.png","hash":"2141bdfa44e5cf86009924a72dca0946f5ca9601","modified":1521392059472},{"_id":"public/2017/10/24/ee-n-dqn/2.png","hash":"6515750b1ed88b7f6bf60910052c0b64820f7d82","modified":1521392059472},{"_id":"public/2018/01/08/search/IDF.png","hash":"a9d19b122aaad81b2ebdf7ac47382bda3a9a6d30","modified":1521392059472},{"_id":"public/2018/01/08/search/TF.png","hash":"a1e40176a572fccb513815595fc6288543bd0a1c","modified":1521392059472},{"_id":"public/2017/10/10/logit-n-probit/logistic.png","hash":"d14da93bd72cd7c5880caf24323fb0af3b863162","modified":1521392059472},{"_id":"public/2017/10/10/logit-n-probit/logit.png","hash":"318241cd887a667d301d8bd068d6791da4dba7ab","modified":1521392059472},{"_id":"public/2017/10/10/logit-n-probit/logit-logistic-relation.jpg","hash":"4bc9396bf4c63a5e6e03efad1f0d08a3226c5127","modified":1521392059472},{"_id":"public/2017/10/24/ee-n-dqn/1.png","hash":"d0ba7aa13143ebf0d5987f1cb2487a7944ea36cf","modified":1521392059472},{"_id":"public/2018/03/14/cnn/logistic.png","hash":"d14da93bd72cd7c5880caf24323fb0af3b863162","modified":1521392059472},{"_id":"public/2017/10/06/auc-n-logloss/2.png","hash":"4f007082dc559888d479918e5419eff48391a830","modified":1521392059472},{"_id":"public/2017/10/06/auc-n-logloss/6.png","hash":"bf400d2df199beb6cd11f0724934b37ec6a9e5e4","modified":1521392059472},{"_id":"public/2017/10/06/auc-n-logloss/7.png","hash":"80956992425cd24224413be93dd711b23b33d5ff","modified":1521392059472},{"_id":"public/2017/10/06/auc-n-logloss/8.png","hash":"2b160d7859c0433086fb14404165ee6db04ea68c","modified":1521392059472},{"_id":"public/2017/10/06/auc-n-logloss/9.png","hash":"8bdd4379bb480099a83fa956b65c4bd30ce223f5","modified":1521392059472},{"_id":"public/2017/11/23/ctr-recalibration/%D5%F8%A54.4.png","hash":"816d988a281cf7d1dac9a856919abc4e7ea8ea16","modified":1521392059472},{"_id":"public/2017/11/23/ctr-recalibration/图4.4.png","hash":"816d988a281cf7d1dac9a856919abc4e7ea8ea16","modified":1521392059472},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E71.png","hash":"18b316e12d4591fa543bcb5db5ea8dd8fb089ed8","modified":1521392059472},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E7100.png","hash":"9b36fbe342f6fe0c7ebd224c6bc4024c77509e2f","modified":1521392059472},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E7101.png","hash":"e8a736596eaab0a2863ebaf7cfc2806c05cf2b48","modified":1521392059472},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E7103.png","hash":"51596b033c3ef8457f11590c7eb35c9be0f08c30","modified":1521392059472},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E72.png","hash":"4efddf70286df1d8b3ae4f69597eeb9f159ce682","modified":1521392059472},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E7106.png","hash":"e24199f39e09a9034f4c37b9dd6d56060dd71518","modified":1521392059473},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E7104.png","hash":"dd5fe476184597c8871fd0a581ae15fd5528f862","modified":1521392059473},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E73.png","hash":"63dc46e8d0254975359f8c02d560a417bcc7686c","modified":1521392059473},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E7105.png","hash":"a46d624d3e25bfdfe15dc68dc2d7e17ab2bda16f","modified":1521392059473},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1521392060369},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1521392060388},{"_id":"public/2018/03/14/cnn/cnn002.png","hash":"085da484b2f03ebab31484861b2a47a4c660bec9","modified":1521392060396},{"_id":"public/2017/10/06/auc-n-logloss/4.png","hash":"768177ead93284112e04805f9738479f3ab2fbf3","modified":1521392060396},{"_id":"public/2017/11/23/ctr-recalibration/%D5%F8%A52.2.png","hash":"ab179418ff1eeb992aaec1db8a7299c180cbd8b9","modified":1521392060396},{"_id":"public/2017/11/23/ctr-recalibration/图2.2.png","hash":"ab179418ff1eeb992aaec1db8a7299c180cbd8b9","modified":1521392060396},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E7102.png","hash":"10e75aed2f2f3cdee5972f3c4779a2fe34388505","modified":1521392060397},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E7201.png","hash":"1b600e59fec7e58cccbf38dced4a4d8953e3a5b8","modified":1521392060397},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E74.png","hash":"ec1cc3151a7a8db835a6150dac1b3109898f1134","modified":1521392060397},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E75.png","hash":"f59c3d0cea4445366028261080d4e1945b8b1a78","modified":1521392060398},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E76.png","hash":"d2f2c684ef148b4aab12d54f9e1b14829bc0bee9","modified":1521392060398},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E77.png","hash":"48fc762aa53ae47d05caf6937ded51e9fcf9dded","modified":1521392060399},{"_id":"public/2017/10/05/lda/301.png","hash":"493c179e66444d0e3f92ffc70fa12a2914c36fcd","modified":1521392060399},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E78.png","hash":"be892636e7f7a78f8d86b2898f87ca31295007db","modified":1521392060399},{"_id":"public/2017/10/05/lda/401.png","hash":"d9b18be14991ff22d5beb80e85bbc56fc5e599b3","modified":1521392060399},{"_id":"public/2017/10/05/lda/501.png","hash":"d671c983e48fc27e61c2690f8d2de05cb39635b1","modified":1521392060399},{"_id":"public/2017/10/05/lda/601.jpg","hash":"1011ca8d03ebc9849b5ab510f6a10ee0366fdb47","modified":1521392060400},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1521392060425},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1521392060425},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1521392060425},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1521392060425},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1521392060425},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1521392060425},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1521392060425},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1521392060425},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1521392060425},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1521392060425},{"_id":"public/js/src/utils.js","hash":"6b0eeeb9dda4a7c94c1c4f6fafd2c801da6e8f96","modified":1521392060425},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1521392060425},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1521392060425},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"7fd2f3e2773555392ef40df40cae3bedb884f17a","modified":1521392060426},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1521392060426},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1521392060427},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1521392060431},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1521392060432},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1521392060432},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1521392060432},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1521392060433},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1521392060433},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1521392060433},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1521392060433},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1521392060433},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1521392060433},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1521392060433},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1521392060434},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1521392060434},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1521392060434},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1521392060434},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1521392060434},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1521392060435},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1521392060436},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1521392060436},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1521392060436},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1521392060436},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1521392060437},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1521392060437},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1521392060437},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1521392060437},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1521392060437},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1521392060437},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1521392060437},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1521392060437},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1521392060437},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1521392060437},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1521392060437},{"_id":"public/css/main.css","hash":"d03fa719895c42cde8bbfcb999660d20f8b7ae77","modified":1521392060437},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1521392060437},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1521392060437},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1521392060437},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1521392060437},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1521392060437},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1521392060437},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1521392060437},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1521392060437},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1521392060437},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1521392060438},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1521392060438},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1521392060438},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1521392060438},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1521392060438},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1521392060438},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1521392060438},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1521392060438},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1521392060438},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1521392060438},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1521392060439},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1521392060439},{"_id":"public/2017/10/06/auc-n-logloss/3.png","hash":"e60e44b56a987bef83090ab1ac4a1d50b788e7ac","modified":1521392060439},{"_id":"public/2018/03/14/cnn/lenet.png","hash":"1a63ac6a9fce5a59b243adc71965e8b55286d654","modified":1521392060439},{"_id":"public/2017/10/05/lda/%D5%F8%A5%FE%EB%E79.png","hash":"17570e2afd53b60ed2b6558c1ed88fa3b6742a4f","modified":1521392060441},{"_id":"public/2017/10/05/lda/402.png","hash":"fb0d940ff0c339a33ce2056c6089c1d1e2544aeb","modified":1521392060442},{"_id":"public/2017/10/06/auc-n-logloss/1.gif","hash":"6aa137ccf577a9689570505ad4fa5eac52a784c0","modified":1521392060465},{"_id":"public/2018/03/14/cnn/alexNet.png","hash":"2abd32ba6a560379605ac5d554cfa0b10bb3253b","modified":1521392060465},{"_id":"public/2017/10/05/lda/302.png","hash":"119cb4b9ba91acfb35ec5338011d1a196b296a72","modified":1521392060465},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1521392060532},{"_id":"public/2018/03/14/cnn/cnn003.png","hash":"5f925b3cabb4d20d3b212c546aaeeab664d54555","modified":1521392060532},{"_id":"public/2017/11/23/ctr-recalibration/%D5%F8%A51.1.png","hash":"8fc18abcde73fb0194e8eab4e1da7070fe0e0d5b","modified":1521392060533},{"_id":"public/2017/11/23/ctr-recalibration/图1.1.png","hash":"8fc18abcde73fb0194e8eab4e1da7070fe0e0d5b","modified":1521392060533},{"_id":"public/2017/11/23/ctr-recalibration/%D5%F8%A53.3.jpg","hash":"0d0d141f6967b62ec615766d43911993eea99b9a","modified":1521392060788},{"_id":"public/2017/11/23/ctr-recalibration/图3.3.jpg","hash":"0d0d141f6967b62ec615766d43911993eea99b9a","modified":1521392060788},{"_id":"public/2017/11/23/ctr-recalibration/%D5%F8%A53.3.png","hash":"21b3b0eb1a89a2c3723673e632e463b17a0428b3","modified":1521392060891},{"_id":"public/2017/11/23/ctr-recalibration/图3.3.png","hash":"21b3b0eb1a89a2c3723673e632e463b17a0428b3","modified":1521392060922}],"Category":[{"name":"机器学习","_id":"cjex1jryu0006si9g62ypblhd"},{"name":"数据挖掘","_id":"cjex1jrzf000osi9guqfrfi3p"}],"Data":[],"Page":[{"title":"","date":"2017-10-02T12:29:04.000Z","_content":"\n### 博主简介\n推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。\n\n### Contact me\nsampsonguo302@gmail.com\n","source":"about/index.md","raw":"---\ntitle:\ndate: 2017-10-02 20:29:04\n---\n\n### 博主简介\n推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。\n\n### Contact me\nsampsonguo302@gmail.com\n","updated":"2018-03-15T14:44:51.000Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjex1jrya0000si9gs2hxq33w","content":"<h3 id=\"博主简介\"><a href=\"#博主简介\" class=\"headerlink\" title=\"博主简介\"></a>博主简介</h3><p>推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。</p>\n<h3 id=\"Contact-me\"><a href=\"#Contact-me\" class=\"headerlink\" title=\"Contact me\"></a>Contact me</h3><p>sampsonguo302@gmail.com</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"博主简介\"><a href=\"#博主简介\" class=\"headerlink\" title=\"博主简介\"></a>博主简介</h3><p>推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。</p>\n<h3 id=\"Contact-me\"><a href=\"#Contact-me\" class=\"headerlink\" title=\"Contact me\"></a>Contact me</h3><p>sampsonguo302@gmail.com</p>\n"},{"_content":"推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。","source":"about/tmp.md","raw":"推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。","date":"2018-03-15T16:34:11.000Z","updated":"2018-03-15T14:44:51.000Z","path":"about/tmp.html","title":"","comments":1,"layout":"page","_id":"cjex1jryg0002si9gpy41dnad","content":"<p>推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>推荐系统工程师，专注于推荐，广告，搜索，机器学习和数学科普。</p>\n"},{"title":"categories","date":"2017-10-02T13:07:45.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-10-02 21:07:45\ntype: categories\n---\n","updated":"2018-03-15T14:44:51.000Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjex1jrym0004si9gshvqcoxp","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"MachineLearning_ZhouZhihua","date":"2017-11-27T03:10:36.000Z","_content":"\n### chapter 1\n\n* No Free Lunch Theorem(NFL)\n\n算法应用于问题要具体问题具体分析\n\n### chapture 2: 模型评估和选择\n\n#### Basic Concepts\n* error\n\t* empirical error/training error 经验误差（训练误差）\n\t* generalization error 泛化误差（预测集误差）\n* fitting\n\t* overfitting\n\t* underfitting\n* set\n\t* train / dev / test set\n* cross validation\n\t* k-fold\n* bootstrapping\n\t* 适用小数据集\n\t* 会改变数据分布\n* parameter tuning\n* performance measure\n\t* MSE: mean squared error \n\t* error rate\n\t* accuracy rate\n\t* Precision/Recall/F1/ROC/AUC\n\t\n#### 常见错误\n* 训练集测试集分布不一致(采样？Group By?)\n* 数据穿越\n\n#### 代码敏感错误率与代价曲线\n* 代价曲线与期望总体代价\n* 假设检验\n\n#### 多分类\n* Linear Regression/Logistic Regression/LDA(Linear Discriminant Analysis)\n* OVO, OVR\n\n#### DT\n* entropy/IG/Gini\n* 预减枝/后减枝\n\n#### ANN\n* Perceptron\n\n","source":"_posts/MachineLearning-ZhouZhihua.md","raw":"---\ntitle: MachineLearning_ZhouZhihua\ndate: 2017-11-27 11:10:36\ntags:\n---\n\n### chapter 1\n\n* No Free Lunch Theorem(NFL)\n\n算法应用于问题要具体问题具体分析\n\n### chapture 2: 模型评估和选择\n\n#### Basic Concepts\n* error\n\t* empirical error/training error 经验误差（训练误差）\n\t* generalization error 泛化误差（预测集误差）\n* fitting\n\t* overfitting\n\t* underfitting\n* set\n\t* train / dev / test set\n* cross validation\n\t* k-fold\n* bootstrapping\n\t* 适用小数据集\n\t* 会改变数据分布\n* parameter tuning\n* performance measure\n\t* MSE: mean squared error \n\t* error rate\n\t* accuracy rate\n\t* Precision/Recall/F1/ROC/AUC\n\t\n#### 常见错误\n* 训练集测试集分布不一致(采样？Group By?)\n* 数据穿越\n\n#### 代码敏感错误率与代价曲线\n* 代价曲线与期望总体代价\n* 假设检验\n\n#### 多分类\n* Linear Regression/Logistic Regression/LDA(Linear Discriminant Analysis)\n* OVO, OVR\n\n#### DT\n* entropy/IG/Gini\n* 预减枝/后减枝\n\n#### ANN\n* Perceptron\n\n","slug":"MachineLearning-ZhouZhihua","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jryc0001si9gk2drmoz3","content":"<h3 id=\"chapter-1\"><a href=\"#chapter-1\" class=\"headerlink\" title=\"chapter 1\"></a>chapter 1</h3><ul>\n<li>No Free Lunch Theorem(NFL)</li>\n</ul>\n<p>算法应用于问题要具体问题具体分析</p>\n<h3 id=\"chapture-2-模型评估和选择\"><a href=\"#chapture-2-模型评估和选择\" class=\"headerlink\" title=\"chapture 2: 模型评估和选择\"></a>chapture 2: 模型评估和选择</h3><h4 id=\"Basic-Concepts\"><a href=\"#Basic-Concepts\" class=\"headerlink\" title=\"Basic Concepts\"></a>Basic Concepts</h4><ul>\n<li>error<ul>\n<li>empirical error/training error 经验误差（训练误差）</li>\n<li>generalization error 泛化误差（预测集误差）</li>\n</ul>\n</li>\n<li>fitting<ul>\n<li>overfitting</li>\n<li>underfitting</li>\n</ul>\n</li>\n<li>set<ul>\n<li>train / dev / test set</li>\n</ul>\n</li>\n<li>cross validation<ul>\n<li>k-fold</li>\n</ul>\n</li>\n<li>bootstrapping<ul>\n<li>适用小数据集</li>\n<li>会改变数据分布</li>\n</ul>\n</li>\n<li>parameter tuning</li>\n<li>performance measure<ul>\n<li>MSE: mean squared error </li>\n<li>error rate</li>\n<li>accuracy rate</li>\n<li>Precision/Recall/F1/ROC/AUC</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"常见错误\"><a href=\"#常见错误\" class=\"headerlink\" title=\"常见错误\"></a>常见错误</h4><ul>\n<li>训练集测试集分布不一致(采样？Group By?)</li>\n<li>数据穿越</li>\n</ul>\n<h4 id=\"代码敏感错误率与代价曲线\"><a href=\"#代码敏感错误率与代价曲线\" class=\"headerlink\" title=\"代码敏感错误率与代价曲线\"></a>代码敏感错误率与代价曲线</h4><ul>\n<li>代价曲线与期望总体代价</li>\n<li>假设检验</li>\n</ul>\n<h4 id=\"多分类\"><a href=\"#多分类\" class=\"headerlink\" title=\"多分类\"></a>多分类</h4><ul>\n<li>Linear Regression/Logistic Regression/LDA(Linear Discriminant Analysis)</li>\n<li>OVO, OVR</li>\n</ul>\n<h4 id=\"DT\"><a href=\"#DT\" class=\"headerlink\" title=\"DT\"></a>DT</h4><ul>\n<li>entropy/IG/Gini</li>\n<li>预减枝/后减枝</li>\n</ul>\n<h4 id=\"ANN\"><a href=\"#ANN\" class=\"headerlink\" title=\"ANN\"></a>ANN</h4><ul>\n<li>Perceptron</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"chapter-1\"><a href=\"#chapter-1\" class=\"headerlink\" title=\"chapter 1\"></a>chapter 1</h3><ul>\n<li>No Free Lunch Theorem(NFL)</li>\n</ul>\n<p>算法应用于问题要具体问题具体分析</p>\n<h3 id=\"chapture-2-模型评估和选择\"><a href=\"#chapture-2-模型评估和选择\" class=\"headerlink\" title=\"chapture 2: 模型评估和选择\"></a>chapture 2: 模型评估和选择</h3><h4 id=\"Basic-Concepts\"><a href=\"#Basic-Concepts\" class=\"headerlink\" title=\"Basic Concepts\"></a>Basic Concepts</h4><ul>\n<li>error<ul>\n<li>empirical error/training error 经验误差（训练误差）</li>\n<li>generalization error 泛化误差（预测集误差）</li>\n</ul>\n</li>\n<li>fitting<ul>\n<li>overfitting</li>\n<li>underfitting</li>\n</ul>\n</li>\n<li>set<ul>\n<li>train / dev / test set</li>\n</ul>\n</li>\n<li>cross validation<ul>\n<li>k-fold</li>\n</ul>\n</li>\n<li>bootstrapping<ul>\n<li>适用小数据集</li>\n<li>会改变数据分布</li>\n</ul>\n</li>\n<li>parameter tuning</li>\n<li>performance measure<ul>\n<li>MSE: mean squared error </li>\n<li>error rate</li>\n<li>accuracy rate</li>\n<li>Precision/Recall/F1/ROC/AUC</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"常见错误\"><a href=\"#常见错误\" class=\"headerlink\" title=\"常见错误\"></a>常见错误</h4><ul>\n<li>训练集测试集分布不一致(采样？Group By?)</li>\n<li>数据穿越</li>\n</ul>\n<h4 id=\"代码敏感错误率与代价曲线\"><a href=\"#代码敏感错误率与代价曲线\" class=\"headerlink\" title=\"代码敏感错误率与代价曲线\"></a>代码敏感错误率与代价曲线</h4><ul>\n<li>代价曲线与期望总体代价</li>\n<li>假设检验</li>\n</ul>\n<h4 id=\"多分类\"><a href=\"#多分类\" class=\"headerlink\" title=\"多分类\"></a>多分类</h4><ul>\n<li>Linear Regression/Logistic Regression/LDA(Linear Discriminant Analysis)</li>\n<li>OVO, OVR</li>\n</ul>\n<h4 id=\"DT\"><a href=\"#DT\" class=\"headerlink\" title=\"DT\"></a>DT</h4><ul>\n<li>entropy/IG/Gini</li>\n<li>预减枝/后减枝</li>\n</ul>\n<h4 id=\"ANN\"><a href=\"#ANN\" class=\"headerlink\" title=\"ANN\"></a>ANN</h4><ul>\n<li>Perceptron</li>\n</ul>\n"},{"title":"auc_n_logloss","date":"2017-10-06T07:31:48.000Z","_content":"\n### 评估指标\n评估指标大致分为两种，值评估和序评估。\n\n<!-- more -->\n\n| 值评估 | 序评估 |\n|--- |---|\n| MSE/RMSE | AUC/AUPR |\n| R^2 | P@k/MAP/nDCG |\n| logloss | Precision/Recall |\n| MAE | TP/FP/TN/FN/F1 |\n\n### 序准 VS 值准\n* 指标和目的\n    * 序评估目的是为了序准\n    * 值评估目的是为了值准\n* 应用场景\n    * 序准适用于推荐系统，pCTR相对准确，目的是用户价值最大化\n    * 值准适用于商业化系统，pCTR绝对准确，pCTR*cpc，目的是商业价值最大化\n    * 综合公式 score=pCTR^a * cpc^b，进行调权重\n\n### AUC的由来和计算\nauc的一些基础知识，可以参考维基百科的解释:\n\nhttps://en.wikipedia.org/wiki/Receiver_operating_characteristic\n\n这里需要提到一些常见的错误：\n* 错误1：auc是一条光滑曲线\nauc是一条折线，如下图\n {% asset_img \"1.gif\" [1.gif] %}\n\n* 错误2：auc是和预估值有关系的\nauc只和序有关系，和值无关。\n\n* 错误3：求auc需要画出roc曲线\nauc计算部分，除了画出roc曲线，还可以直接计算：\n {% asset_img \"2.png\" [2.png] %}\n其中,\nM为正类样本的数目，N为负类样本的数目\nrank是用的tiedrank\n\n#### AUC的物理意义\n\n和Wilcoxon-Mann-Witney Test有关，即:\nauc=“测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score”，也即auc的物理意义。\n\n#### AUC的计算\n* spark\n```\n// Compute raw scores on the test set\nval predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n  val prediction = model.predict(features)\n  (prediction, label)\n}\n\n// Instantiate metrics object\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)\n\n// AUROC\nval auROC = metrics.areaUnderROC\nprintln(\"Area under ROC = \" + auROC)\n```\n\n* hivemall\n {% asset_img \"3.png\" [3.png] %}\n\n* C语言\nRef: https://github.com/liuzhiqiangruc/dml/blob/master/regr/auc.c\n```\ndouble auc(int n, double *x, double *y) {\n    if (!y || !x) return 0.0;\n    double *rk = (double*) malloc(sizeof(double) * n);\n    AucP *aucp = (AucP *)malloc(sizeof(AucP) * n);\n    int i, tsum;\n    double rksum, auc;\n    for (i = 0; i < n; ++i) {\n        aucp[i].x = x[i];\n        aucp[i].id = i;\n    }\n    tiedrank(n, aucp, rk);\n    for (rksum = 0., tsum = 0, i = 0; i < n; ++i) {\n        if (y[i] >= 1. - 1e-10) {\n            rksum += rk[i];\n            tsum += 1;\n        }\n    }\n    double mn, pst;\n    mn = (double) (n - tsum);\n    mn *= (double) tsum;\n    pst = (double) tsum;\n    pst *= (double) tsum + 1;\n    auc = (rksum - pst / 2.) / mn;\n    free(rk);\n    free(aucp);\n    return auc;\n}\n```\n\n#### AUC的弊端和AUPR\n {% asset_img \"4.png\" [4.png] %}\n {% asset_img \"6.png\" [6.png] %}\n {% asset_img \"7.png\" [7.png] %}\n\n### logloss的由来和计算\n\n#### logloss由来\nlogloss是根据最大似然推导得到的，可参考：\nhttp://www.csuldw.com/2016/03/26/2016-03-26-loss-function/\n\n有些概念需要区分一下\n* loss function: 样本粒度的函数，如logloss, hingeloss等。\n引用一张名图：\n{% asset_img \"9.png\" [9.png] %}\n\n>Plot of various loss functions. Blue is the 0–1 indicator function. Green is the square loss function. Purple is the hinge loss function. Yellow is the logistic loss function. Note that all surrogates give a loss penalty of 1 for yf(x) = 0\n\n* cost function: 集合粒度的函数，即 sum of loss.\n \n#### logloss计算\nlogloss计算需要避免log0的情况，可以参考kaggle中的计算方式：\n```\nmax(min(p,1−10^−15),10^−15)max(min(p,1−10^−15),10^−15).)\n```\nref: https://www.kaggle.com/wiki/LogLoss\n\n### AUC和logloss何时不一致\n在样本不均衡的情况下，AUC和logloss会出现很大的偏差。\n1. logloss低但是AUC也低\n当负样本过多的时候，人为全部预测为负样本，可以实现低logloss，但是AUC=0.5，并不优秀。\n\n2. AUC高但是logloss也高\n负样本过多，当位置pCTR顺序不变，AUC不变，pCTR统一扩大到接近1时候，导致logloss会变得非常的高。\n\n### 定向模式 VS 推荐模式\n1. PUSH系统：广告为中心，为广告找用户，并push；展示可有可无。\n2. 推荐系统：人为中心，为人找推荐项，并展示；用户来了必须展示。\n\n### 线下AUC和线上不一致\n有三种AUC，很多不一致是因为AUC的描述不同造成的\n假设有user-item-pCTR矩阵，那么可以计算\n* 横向AUC：每用户AUC，适用于推荐系统\n* 纵向AUC：每广告AUC，适用于PUSH系统\n* 全局AUC：统一大模型的AUC\n\n存在很多种情况：\n* 纵向AUC高，横向AUC不一定高\n单广告训练做推荐的典型的问题，举一个例子\n\n|  | Item1 | Item2 | Item3 | Item4 |\n| --- | ---| --- | --- | --- |\n| UserA | 0.7(0) | 0.7(1) | 0.7(1) | 0.7(1) |\n| UserB | 0.6(0) | 0.6(0) | 0.6(1) | 0.6(1) |\n| UserC | 0.5(0) | 0.5(0) | 0.5(0) | 0.5(1) |\n\n从纵向来看，每个单Item模型的AUC=1.0，但是横向的AUC=0.5，因此纵向AUC高，并不代表横向AUC高。即：\n从单广告训练的AUC，集合起来，变成真正用户X过来，对用户X进行广告排序，AUC不一定高。\n这种不一致是由于基于Item的模型并没有发现用户的对比其他人“更”偏好什么。\n\n* 横向AUC高，纵向AUC不一定高\n上图翻转，同理。\n\n### AUC@topk VS AUC人数加权\n为了和线上的情况保持一致，最好的方式是：\n* 用户来了必须展示，因此AUC的计算方式是横向AUC，即每个用户计算AUC，然后加权；\n* 用户往往只看头部，因此只计算AUC@topk，衡量头部排序能力。\n\n\n","source":"_posts/auc-n-logloss.md","raw":"---\ntitle: auc_n_logloss\ndate: 2017-10-06 15:31:48\ntags:\ncategories: 机器学习\n---\n\n### 评估指标\n评估指标大致分为两种，值评估和序评估。\n\n<!-- more -->\n\n| 值评估 | 序评估 |\n|--- |---|\n| MSE/RMSE | AUC/AUPR |\n| R^2 | P@k/MAP/nDCG |\n| logloss | Precision/Recall |\n| MAE | TP/FP/TN/FN/F1 |\n\n### 序准 VS 值准\n* 指标和目的\n    * 序评估目的是为了序准\n    * 值评估目的是为了值准\n* 应用场景\n    * 序准适用于推荐系统，pCTR相对准确，目的是用户价值最大化\n    * 值准适用于商业化系统，pCTR绝对准确，pCTR*cpc，目的是商业价值最大化\n    * 综合公式 score=pCTR^a * cpc^b，进行调权重\n\n### AUC的由来和计算\nauc的一些基础知识，可以参考维基百科的解释:\n\nhttps://en.wikipedia.org/wiki/Receiver_operating_characteristic\n\n这里需要提到一些常见的错误：\n* 错误1：auc是一条光滑曲线\nauc是一条折线，如下图\n {% asset_img \"1.gif\" [1.gif] %}\n\n* 错误2：auc是和预估值有关系的\nauc只和序有关系，和值无关。\n\n* 错误3：求auc需要画出roc曲线\nauc计算部分，除了画出roc曲线，还可以直接计算：\n {% asset_img \"2.png\" [2.png] %}\n其中,\nM为正类样本的数目，N为负类样本的数目\nrank是用的tiedrank\n\n#### AUC的物理意义\n\n和Wilcoxon-Mann-Witney Test有关，即:\nauc=“测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score”，也即auc的物理意义。\n\n#### AUC的计算\n* spark\n```\n// Compute raw scores on the test set\nval predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n  val prediction = model.predict(features)\n  (prediction, label)\n}\n\n// Instantiate metrics object\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)\n\n// AUROC\nval auROC = metrics.areaUnderROC\nprintln(\"Area under ROC = \" + auROC)\n```\n\n* hivemall\n {% asset_img \"3.png\" [3.png] %}\n\n* C语言\nRef: https://github.com/liuzhiqiangruc/dml/blob/master/regr/auc.c\n```\ndouble auc(int n, double *x, double *y) {\n    if (!y || !x) return 0.0;\n    double *rk = (double*) malloc(sizeof(double) * n);\n    AucP *aucp = (AucP *)malloc(sizeof(AucP) * n);\n    int i, tsum;\n    double rksum, auc;\n    for (i = 0; i < n; ++i) {\n        aucp[i].x = x[i];\n        aucp[i].id = i;\n    }\n    tiedrank(n, aucp, rk);\n    for (rksum = 0., tsum = 0, i = 0; i < n; ++i) {\n        if (y[i] >= 1. - 1e-10) {\n            rksum += rk[i];\n            tsum += 1;\n        }\n    }\n    double mn, pst;\n    mn = (double) (n - tsum);\n    mn *= (double) tsum;\n    pst = (double) tsum;\n    pst *= (double) tsum + 1;\n    auc = (rksum - pst / 2.) / mn;\n    free(rk);\n    free(aucp);\n    return auc;\n}\n```\n\n#### AUC的弊端和AUPR\n {% asset_img \"4.png\" [4.png] %}\n {% asset_img \"6.png\" [6.png] %}\n {% asset_img \"7.png\" [7.png] %}\n\n### logloss的由来和计算\n\n#### logloss由来\nlogloss是根据最大似然推导得到的，可参考：\nhttp://www.csuldw.com/2016/03/26/2016-03-26-loss-function/\n\n有些概念需要区分一下\n* loss function: 样本粒度的函数，如logloss, hingeloss等。\n引用一张名图：\n{% asset_img \"9.png\" [9.png] %}\n\n>Plot of various loss functions. Blue is the 0–1 indicator function. Green is the square loss function. Purple is the hinge loss function. Yellow is the logistic loss function. Note that all surrogates give a loss penalty of 1 for yf(x) = 0\n\n* cost function: 集合粒度的函数，即 sum of loss.\n \n#### logloss计算\nlogloss计算需要避免log0的情况，可以参考kaggle中的计算方式：\n```\nmax(min(p,1−10^−15),10^−15)max(min(p,1−10^−15),10^−15).)\n```\nref: https://www.kaggle.com/wiki/LogLoss\n\n### AUC和logloss何时不一致\n在样本不均衡的情况下，AUC和logloss会出现很大的偏差。\n1. logloss低但是AUC也低\n当负样本过多的时候，人为全部预测为负样本，可以实现低logloss，但是AUC=0.5，并不优秀。\n\n2. AUC高但是logloss也高\n负样本过多，当位置pCTR顺序不变，AUC不变，pCTR统一扩大到接近1时候，导致logloss会变得非常的高。\n\n### 定向模式 VS 推荐模式\n1. PUSH系统：广告为中心，为广告找用户，并push；展示可有可无。\n2. 推荐系统：人为中心，为人找推荐项，并展示；用户来了必须展示。\n\n### 线下AUC和线上不一致\n有三种AUC，很多不一致是因为AUC的描述不同造成的\n假设有user-item-pCTR矩阵，那么可以计算\n* 横向AUC：每用户AUC，适用于推荐系统\n* 纵向AUC：每广告AUC，适用于PUSH系统\n* 全局AUC：统一大模型的AUC\n\n存在很多种情况：\n* 纵向AUC高，横向AUC不一定高\n单广告训练做推荐的典型的问题，举一个例子\n\n|  | Item1 | Item2 | Item3 | Item4 |\n| --- | ---| --- | --- | --- |\n| UserA | 0.7(0) | 0.7(1) | 0.7(1) | 0.7(1) |\n| UserB | 0.6(0) | 0.6(0) | 0.6(1) | 0.6(1) |\n| UserC | 0.5(0) | 0.5(0) | 0.5(0) | 0.5(1) |\n\n从纵向来看，每个单Item模型的AUC=1.0，但是横向的AUC=0.5，因此纵向AUC高，并不代表横向AUC高。即：\n从单广告训练的AUC，集合起来，变成真正用户X过来，对用户X进行广告排序，AUC不一定高。\n这种不一致是由于基于Item的模型并没有发现用户的对比其他人“更”偏好什么。\n\n* 横向AUC高，纵向AUC不一定高\n上图翻转，同理。\n\n### AUC@topk VS AUC人数加权\n为了和线上的情况保持一致，最好的方式是：\n* 用户来了必须展示，因此AUC的计算方式是横向AUC，即每个用户计算AUC，然后加权；\n* 用户往往只看头部，因此只计算AUC@topk，衡量头部排序能力。\n\n\n","slug":"auc-n-logloss","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jryh0003si9gibtjyf42","content":"<h3 id=\"评估指标\"><a href=\"#评估指标\" class=\"headerlink\" title=\"评估指标\"></a>评估指标</h3><p>评估指标大致分为两种，值评估和序评估。</p>\n<a id=\"more\"></a>\n<table>\n<thead>\n<tr>\n<th>值评估</th>\n<th>序评估</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MSE/RMSE</td>\n<td>AUC/AUPR</td>\n</tr>\n<tr>\n<td>R^2</td>\n<td>P@k/MAP/nDCG</td>\n</tr>\n<tr>\n<td>logloss</td>\n<td>Precision/Recall</td>\n</tr>\n<tr>\n<td>MAE</td>\n<td>TP/FP/TN/FN/F1</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"序准-VS-值准\"><a href=\"#序准-VS-值准\" class=\"headerlink\" title=\"序准 VS 值准\"></a>序准 VS 值准</h3><ul>\n<li>指标和目的<ul>\n<li>序评估目的是为了序准</li>\n<li>值评估目的是为了值准</li>\n</ul>\n</li>\n<li>应用场景<ul>\n<li>序准适用于推荐系统，pCTR相对准确，目的是用户价值最大化</li>\n<li>值准适用于商业化系统，pCTR绝对准确，pCTR*cpc，目的是商业价值最大化</li>\n<li>综合公式 score=pCTR^a * cpc^b，进行调权重</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"AUC的由来和计算\"><a href=\"#AUC的由来和计算\" class=\"headerlink\" title=\"AUC的由来和计算\"></a>AUC的由来和计算</h3><p>auc的一些基础知识，可以参考维基百科的解释:</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Receiver_operating_characteristic</a></p>\n<p>这里需要提到一些常见的错误：</p>\n<ul>\n<li><p>错误1：auc是一条光滑曲线<br>auc是一条折线，如下图</p>\n<img src=\"/2017/10/06/auc-n-logloss/1.gif\" alt=\"[1.gif]\" title=\"[1.gif]\">\n</li>\n<li><p>错误2：auc是和预估值有关系的<br>auc只和序有关系，和值无关。</p>\n</li>\n<li><p>错误3：求auc需要画出roc曲线<br>auc计算部分，除了画出roc曲线，还可以直接计算：</p>\n<img src=\"/2017/10/06/auc-n-logloss/2.png\" alt=\"[2.png]\" title=\"[2.png]\">\n<p>其中,<br>M为正类样本的数目，N为负类样本的数目<br>rank是用的tiedrank</p>\n</li>\n</ul>\n<h4 id=\"AUC的物理意义\"><a href=\"#AUC的物理意义\" class=\"headerlink\" title=\"AUC的物理意义\"></a>AUC的物理意义</h4><p>和Wilcoxon-Mann-Witney Test有关，即:<br>auc=“测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score”，也即auc的物理意义。</p>\n<h4 id=\"AUC的计算\"><a href=\"#AUC的计算\" class=\"headerlink\" title=\"AUC的计算\"></a>AUC的计算</h4><ul>\n<li><p>spark</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Compute raw scores on the test set</div><div class=\"line\">val predictionAndLabels = test.map &#123; case LabeledPoint(label, features) =&gt;</div><div class=\"line\">  val prediction = model.predict(features)</div><div class=\"line\">  (prediction, label)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">// Instantiate metrics object</div><div class=\"line\">val metrics = new BinaryClassificationMetrics(predictionAndLabels)</div><div class=\"line\"></div><div class=\"line\">// AUROC</div><div class=\"line\">val auROC = metrics.areaUnderROC</div><div class=\"line\">println(&quot;Area under ROC = &quot; + auROC)</div></pre></td></tr></table></figure>\n</li>\n<li><p>hivemall</p>\n<img src=\"/2017/10/06/auc-n-logloss/3.png\" alt=\"[3.png]\" title=\"[3.png]\">\n</li>\n<li><p>C语言<br>Ref: <a href=\"https://github.com/liuzhiqiangruc/dml/blob/master/regr/auc.c\" target=\"_blank\" rel=\"external\">https://github.com/liuzhiqiangruc/dml/blob/master/regr/auc.c</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">double auc(int n, double *x, double *y) &#123;</div><div class=\"line\">    if (!y || !x) return 0.0;</div><div class=\"line\">    double *rk = (double*) malloc(sizeof(double) * n);</div><div class=\"line\">    AucP *aucp = (AucP *)malloc(sizeof(AucP) * n);</div><div class=\"line\">    int i, tsum;</div><div class=\"line\">    double rksum, auc;</div><div class=\"line\">    for (i = 0; i &lt; n; ++i) &#123;</div><div class=\"line\">        aucp[i].x = x[i];</div><div class=\"line\">        aucp[i].id = i;</div><div class=\"line\">    &#125;</div><div class=\"line\">    tiedrank(n, aucp, rk);</div><div class=\"line\">    for (rksum = 0., tsum = 0, i = 0; i &lt; n; ++i) &#123;</div><div class=\"line\">        if (y[i] &gt;= 1. - 1e-10) &#123;</div><div class=\"line\">            rksum += rk[i];</div><div class=\"line\">            tsum += 1;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    double mn, pst;</div><div class=\"line\">    mn = (double) (n - tsum);</div><div class=\"line\">    mn *= (double) tsum;</div><div class=\"line\">    pst = (double) tsum;</div><div class=\"line\">    pst *= (double) tsum + 1;</div><div class=\"line\">    auc = (rksum - pst / 2.) / mn;</div><div class=\"line\">    free(rk);</div><div class=\"line\">    free(aucp);</div><div class=\"line\">    return auc;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h4 id=\"AUC的弊端和AUPR\"><a href=\"#AUC的弊端和AUPR\" class=\"headerlink\" title=\"AUC的弊端和AUPR\"></a>AUC的弊端和AUPR</h4> <img src=\"/2017/10/06/auc-n-logloss/4.png\" alt=\"[4.png]\" title=\"[4.png]\">\n <img src=\"/2017/10/06/auc-n-logloss/6.png\" alt=\"[6.png]\" title=\"[6.png]\">\n <img src=\"/2017/10/06/auc-n-logloss/7.png\" alt=\"[7.png]\" title=\"[7.png]\">\n<h3 id=\"logloss的由来和计算\"><a href=\"#logloss的由来和计算\" class=\"headerlink\" title=\"logloss的由来和计算\"></a>logloss的由来和计算</h3><h4 id=\"logloss由来\"><a href=\"#logloss由来\" class=\"headerlink\" title=\"logloss由来\"></a>logloss由来</h4><p>logloss是根据最大似然推导得到的，可参考：<br><a href=\"http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/\" target=\"_blank\" rel=\"external\">http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/</a></p>\n<p>有些概念需要区分一下</p>\n<ul>\n<li>loss function: 样本粒度的函数，如logloss, hingeloss等。<br>引用一张名图：<img src=\"/2017/10/06/auc-n-logloss/9.png\" alt=\"[9.png]\" title=\"[9.png]\">\n</li>\n</ul>\n<blockquote>\n<p>Plot of various loss functions. Blue is the 0–1 indicator function. Green is the square loss function. Purple is the hinge loss function. Yellow is the logistic loss function. Note that all surrogates give a loss penalty of 1 for yf(x) = 0</p>\n</blockquote>\n<ul>\n<li>cost function: 集合粒度的函数，即 sum of loss.</li>\n</ul>\n<h4 id=\"logloss计算\"><a href=\"#logloss计算\" class=\"headerlink\" title=\"logloss计算\"></a>logloss计算</h4><p>logloss计算需要避免log0的情况，可以参考kaggle中的计算方式：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">max(min(p,1−10^−15),10^−15)max(min(p,1−10^−15),10^−15).)</div></pre></td></tr></table></figure></p>\n<p>ref: <a href=\"https://www.kaggle.com/wiki/LogLoss\" target=\"_blank\" rel=\"external\">https://www.kaggle.com/wiki/LogLoss</a></p>\n<h3 id=\"AUC和logloss何时不一致\"><a href=\"#AUC和logloss何时不一致\" class=\"headerlink\" title=\"AUC和logloss何时不一致\"></a>AUC和logloss何时不一致</h3><p>在样本不均衡的情况下，AUC和logloss会出现很大的偏差。</p>\n<ol>\n<li><p>logloss低但是AUC也低<br>当负样本过多的时候，人为全部预测为负样本，可以实现低logloss，但是AUC=0.5，并不优秀。</p>\n</li>\n<li><p>AUC高但是logloss也高<br>负样本过多，当位置pCTR顺序不变，AUC不变，pCTR统一扩大到接近1时候，导致logloss会变得非常的高。</p>\n</li>\n</ol>\n<h3 id=\"定向模式-VS-推荐模式\"><a href=\"#定向模式-VS-推荐模式\" class=\"headerlink\" title=\"定向模式 VS 推荐模式\"></a>定向模式 VS 推荐模式</h3><ol>\n<li>PUSH系统：广告为中心，为广告找用户，并push；展示可有可无。</li>\n<li>推荐系统：人为中心，为人找推荐项，并展示；用户来了必须展示。</li>\n</ol>\n<h3 id=\"线下AUC和线上不一致\"><a href=\"#线下AUC和线上不一致\" class=\"headerlink\" title=\"线下AUC和线上不一致\"></a>线下AUC和线上不一致</h3><p>有三种AUC，很多不一致是因为AUC的描述不同造成的<br>假设有user-item-pCTR矩阵，那么可以计算</p>\n<ul>\n<li>横向AUC：每用户AUC，适用于推荐系统</li>\n<li>纵向AUC：每广告AUC，适用于PUSH系统</li>\n<li>全局AUC：统一大模型的AUC</li>\n</ul>\n<p>存在很多种情况：</p>\n<ul>\n<li>纵向AUC高，横向AUC不一定高<br>单广告训练做推荐的典型的问题，举一个例子</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Item1</th>\n<th>Item2</th>\n<th>Item3</th>\n<th>Item4</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>UserA</td>\n<td>0.7(0)</td>\n<td>0.7(1)</td>\n<td>0.7(1)</td>\n<td>0.7(1)</td>\n</tr>\n<tr>\n<td>UserB</td>\n<td>0.6(0)</td>\n<td>0.6(0)</td>\n<td>0.6(1)</td>\n<td>0.6(1)</td>\n</tr>\n<tr>\n<td>UserC</td>\n<td>0.5(0)</td>\n<td>0.5(0)</td>\n<td>0.5(0)</td>\n<td>0.5(1)</td>\n</tr>\n</tbody>\n</table>\n<p>从纵向来看，每个单Item模型的AUC=1.0，但是横向的AUC=0.5，因此纵向AUC高，并不代表横向AUC高。即：<br>从单广告训练的AUC，集合起来，变成真正用户X过来，对用户X进行广告排序，AUC不一定高。<br>这种不一致是由于基于Item的模型并没有发现用户的对比其他人“更”偏好什么。</p>\n<ul>\n<li>横向AUC高，纵向AUC不一定高<br>上图翻转，同理。</li>\n</ul>\n<h3 id=\"AUC-topk-VS-AUC人数加权\"><a href=\"#AUC-topk-VS-AUC人数加权\" class=\"headerlink\" title=\"AUC@topk VS AUC人数加权\"></a>AUC@topk VS AUC人数加权</h3><p>为了和线上的情况保持一致，最好的方式是：</p>\n<ul>\n<li>用户来了必须展示，因此AUC的计算方式是横向AUC，即每个用户计算AUC，然后加权；</li>\n<li>用户往往只看头部，因此只计算AUC@topk，衡量头部排序能力。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h3 id=\"评估指标\"><a href=\"#评估指标\" class=\"headerlink\" title=\"评估指标\"></a>评估指标</h3><p>评估指标大致分为两种，值评估和序评估。</p>","more":"<table>\n<thead>\n<tr>\n<th>值评估</th>\n<th>序评估</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MSE/RMSE</td>\n<td>AUC/AUPR</td>\n</tr>\n<tr>\n<td>R^2</td>\n<td>P@k/MAP/nDCG</td>\n</tr>\n<tr>\n<td>logloss</td>\n<td>Precision/Recall</td>\n</tr>\n<tr>\n<td>MAE</td>\n<td>TP/FP/TN/FN/F1</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"序准-VS-值准\"><a href=\"#序准-VS-值准\" class=\"headerlink\" title=\"序准 VS 值准\"></a>序准 VS 值准</h3><ul>\n<li>指标和目的<ul>\n<li>序评估目的是为了序准</li>\n<li>值评估目的是为了值准</li>\n</ul>\n</li>\n<li>应用场景<ul>\n<li>序准适用于推荐系统，pCTR相对准确，目的是用户价值最大化</li>\n<li>值准适用于商业化系统，pCTR绝对准确，pCTR*cpc，目的是商业价值最大化</li>\n<li>综合公式 score=pCTR^a * cpc^b，进行调权重</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"AUC的由来和计算\"><a href=\"#AUC的由来和计算\" class=\"headerlink\" title=\"AUC的由来和计算\"></a>AUC的由来和计算</h3><p>auc的一些基础知识，可以参考维基百科的解释:</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Receiver_operating_characteristic</a></p>\n<p>这里需要提到一些常见的错误：</p>\n<ul>\n<li><p>错误1：auc是一条光滑曲线<br>auc是一条折线，如下图</p>\n<img src=\"/2017/10/06/auc-n-logloss/1.gif\" alt=\"[1.gif]\" title=\"[1.gif]\">\n</li>\n<li><p>错误2：auc是和预估值有关系的<br>auc只和序有关系，和值无关。</p>\n</li>\n<li><p>错误3：求auc需要画出roc曲线<br>auc计算部分，除了画出roc曲线，还可以直接计算：</p>\n<img src=\"/2017/10/06/auc-n-logloss/2.png\" alt=\"[2.png]\" title=\"[2.png]\">\n<p>其中,<br>M为正类样本的数目，N为负类样本的数目<br>rank是用的tiedrank</p>\n</li>\n</ul>\n<h4 id=\"AUC的物理意义\"><a href=\"#AUC的物理意义\" class=\"headerlink\" title=\"AUC的物理意义\"></a>AUC的物理意义</h4><p>和Wilcoxon-Mann-Witney Test有关，即:<br>auc=“测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score”，也即auc的物理意义。</p>\n<h4 id=\"AUC的计算\"><a href=\"#AUC的计算\" class=\"headerlink\" title=\"AUC的计算\"></a>AUC的计算</h4><ul>\n<li><p>spark</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Compute raw scores on the test set</div><div class=\"line\">val predictionAndLabels = test.map &#123; case LabeledPoint(label, features) =&gt;</div><div class=\"line\">  val prediction = model.predict(features)</div><div class=\"line\">  (prediction, label)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">// Instantiate metrics object</div><div class=\"line\">val metrics = new BinaryClassificationMetrics(predictionAndLabels)</div><div class=\"line\"></div><div class=\"line\">// AUROC</div><div class=\"line\">val auROC = metrics.areaUnderROC</div><div class=\"line\">println(&quot;Area under ROC = &quot; + auROC)</div></pre></td></tr></table></figure>\n</li>\n<li><p>hivemall</p>\n<img src=\"/2017/10/06/auc-n-logloss/3.png\" alt=\"[3.png]\" title=\"[3.png]\">\n</li>\n<li><p>C语言<br>Ref: <a href=\"https://github.com/liuzhiqiangruc/dml/blob/master/regr/auc.c\" target=\"_blank\" rel=\"external\">https://github.com/liuzhiqiangruc/dml/blob/master/regr/auc.c</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">double auc(int n, double *x, double *y) &#123;</div><div class=\"line\">    if (!y || !x) return 0.0;</div><div class=\"line\">    double *rk = (double*) malloc(sizeof(double) * n);</div><div class=\"line\">    AucP *aucp = (AucP *)malloc(sizeof(AucP) * n);</div><div class=\"line\">    int i, tsum;</div><div class=\"line\">    double rksum, auc;</div><div class=\"line\">    for (i = 0; i &lt; n; ++i) &#123;</div><div class=\"line\">        aucp[i].x = x[i];</div><div class=\"line\">        aucp[i].id = i;</div><div class=\"line\">    &#125;</div><div class=\"line\">    tiedrank(n, aucp, rk);</div><div class=\"line\">    for (rksum = 0., tsum = 0, i = 0; i &lt; n; ++i) &#123;</div><div class=\"line\">        if (y[i] &gt;= 1. - 1e-10) &#123;</div><div class=\"line\">            rksum += rk[i];</div><div class=\"line\">            tsum += 1;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    double mn, pst;</div><div class=\"line\">    mn = (double) (n - tsum);</div><div class=\"line\">    mn *= (double) tsum;</div><div class=\"line\">    pst = (double) tsum;</div><div class=\"line\">    pst *= (double) tsum + 1;</div><div class=\"line\">    auc = (rksum - pst / 2.) / mn;</div><div class=\"line\">    free(rk);</div><div class=\"line\">    free(aucp);</div><div class=\"line\">    return auc;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h4 id=\"AUC的弊端和AUPR\"><a href=\"#AUC的弊端和AUPR\" class=\"headerlink\" title=\"AUC的弊端和AUPR\"></a>AUC的弊端和AUPR</h4> <img src=\"/2017/10/06/auc-n-logloss/4.png\" alt=\"[4.png]\" title=\"[4.png]\">\n <img src=\"/2017/10/06/auc-n-logloss/6.png\" alt=\"[6.png]\" title=\"[6.png]\">\n <img src=\"/2017/10/06/auc-n-logloss/7.png\" alt=\"[7.png]\" title=\"[7.png]\">\n<h3 id=\"logloss的由来和计算\"><a href=\"#logloss的由来和计算\" class=\"headerlink\" title=\"logloss的由来和计算\"></a>logloss的由来和计算</h3><h4 id=\"logloss由来\"><a href=\"#logloss由来\" class=\"headerlink\" title=\"logloss由来\"></a>logloss由来</h4><p>logloss是根据最大似然推导得到的，可参考：<br><a href=\"http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/\" target=\"_blank\" rel=\"external\">http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/</a></p>\n<p>有些概念需要区分一下</p>\n<ul>\n<li>loss function: 样本粒度的函数，如logloss, hingeloss等。<br>引用一张名图：<img src=\"/2017/10/06/auc-n-logloss/9.png\" alt=\"[9.png]\" title=\"[9.png]\">\n</li>\n</ul>\n<blockquote>\n<p>Plot of various loss functions. Blue is the 0–1 indicator function. Green is the square loss function. Purple is the hinge loss function. Yellow is the logistic loss function. Note that all surrogates give a loss penalty of 1 for yf(x) = 0</p>\n</blockquote>\n<ul>\n<li>cost function: 集合粒度的函数，即 sum of loss.</li>\n</ul>\n<h4 id=\"logloss计算\"><a href=\"#logloss计算\" class=\"headerlink\" title=\"logloss计算\"></a>logloss计算</h4><p>logloss计算需要避免log0的情况，可以参考kaggle中的计算方式：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">max(min(p,1−10^−15),10^−15)max(min(p,1−10^−15),10^−15).)</div></pre></td></tr></table></figure></p>\n<p>ref: <a href=\"https://www.kaggle.com/wiki/LogLoss\" target=\"_blank\" rel=\"external\">https://www.kaggle.com/wiki/LogLoss</a></p>\n<h3 id=\"AUC和logloss何时不一致\"><a href=\"#AUC和logloss何时不一致\" class=\"headerlink\" title=\"AUC和logloss何时不一致\"></a>AUC和logloss何时不一致</h3><p>在样本不均衡的情况下，AUC和logloss会出现很大的偏差。</p>\n<ol>\n<li><p>logloss低但是AUC也低<br>当负样本过多的时候，人为全部预测为负样本，可以实现低logloss，但是AUC=0.5，并不优秀。</p>\n</li>\n<li><p>AUC高但是logloss也高<br>负样本过多，当位置pCTR顺序不变，AUC不变，pCTR统一扩大到接近1时候，导致logloss会变得非常的高。</p>\n</li>\n</ol>\n<h3 id=\"定向模式-VS-推荐模式\"><a href=\"#定向模式-VS-推荐模式\" class=\"headerlink\" title=\"定向模式 VS 推荐模式\"></a>定向模式 VS 推荐模式</h3><ol>\n<li>PUSH系统：广告为中心，为广告找用户，并push；展示可有可无。</li>\n<li>推荐系统：人为中心，为人找推荐项，并展示；用户来了必须展示。</li>\n</ol>\n<h3 id=\"线下AUC和线上不一致\"><a href=\"#线下AUC和线上不一致\" class=\"headerlink\" title=\"线下AUC和线上不一致\"></a>线下AUC和线上不一致</h3><p>有三种AUC，很多不一致是因为AUC的描述不同造成的<br>假设有user-item-pCTR矩阵，那么可以计算</p>\n<ul>\n<li>横向AUC：每用户AUC，适用于推荐系统</li>\n<li>纵向AUC：每广告AUC，适用于PUSH系统</li>\n<li>全局AUC：统一大模型的AUC</li>\n</ul>\n<p>存在很多种情况：</p>\n<ul>\n<li>纵向AUC高，横向AUC不一定高<br>单广告训练做推荐的典型的问题，举一个例子</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Item1</th>\n<th>Item2</th>\n<th>Item3</th>\n<th>Item4</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>UserA</td>\n<td>0.7(0)</td>\n<td>0.7(1)</td>\n<td>0.7(1)</td>\n<td>0.7(1)</td>\n</tr>\n<tr>\n<td>UserB</td>\n<td>0.6(0)</td>\n<td>0.6(0)</td>\n<td>0.6(1)</td>\n<td>0.6(1)</td>\n</tr>\n<tr>\n<td>UserC</td>\n<td>0.5(0)</td>\n<td>0.5(0)</td>\n<td>0.5(0)</td>\n<td>0.5(1)</td>\n</tr>\n</tbody>\n</table>\n<p>从纵向来看，每个单Item模型的AUC=1.0，但是横向的AUC=0.5，因此纵向AUC高，并不代表横向AUC高。即：<br>从单广告训练的AUC，集合起来，变成真正用户X过来，对用户X进行广告排序，AUC不一定高。<br>这种不一致是由于基于Item的模型并没有发现用户的对比其他人“更”偏好什么。</p>\n<ul>\n<li>横向AUC高，纵向AUC不一定高<br>上图翻转，同理。</li>\n</ul>\n<h3 id=\"AUC-topk-VS-AUC人数加权\"><a href=\"#AUC-topk-VS-AUC人数加权\" class=\"headerlink\" title=\"AUC@topk VS AUC人数加权\"></a>AUC@topk VS AUC人数加权</h3><p>为了和线上的情况保持一致，最好的方式是：</p>\n<ul>\n<li>用户来了必须展示，因此AUC的计算方式是横向AUC，即每个用户计算AUC，然后加权；</li>\n<li>用户往往只看头部，因此只计算AUC@topk，衡量头部排序能力。</li>\n</ul>"},{"title":"bayes","date":"2018-01-16T03:26:02.000Z","_content":"\n### Discriminative VS Generative\n* Discriminative models 判别模型\n直接建模P(c|x)\n* Generative models 生成模型\nP(c|x) = P(x, c) / P(x) = P(c)*P(x|c)/P(x)\n其中，\n\t* P(c)可以通过统计各类样本比例频率来估计 --频率学派\n\t* P(x|c)因为样本x的数据量太小，很难估计准确\n\t\n### 频率学派 VS 贝叶斯学派\n* Frequentist 频率学派 \n参数是一个未知但客观存在的固定值\n* Bayesian 贝叶斯学派 \n参数本身是一个分布 \n\n### Naive Bayes \n* 属性条件独立性假设（假设每个属性独立地对分类结果发生影响）\n* Smoothing: 拉普拉斯修正（Laplacian Correction）\n* Lazy Learning\n\n### semi-naive Bayes classifier\n","source":"_posts/bayes.md","raw":"---\ntitle: bayes\ndate: 2018-01-16 11:26:02\ntags:\n---\n\n### Discriminative VS Generative\n* Discriminative models 判别模型\n直接建模P(c|x)\n* Generative models 生成模型\nP(c|x) = P(x, c) / P(x) = P(c)*P(x|c)/P(x)\n其中，\n\t* P(c)可以通过统计各类样本比例频率来估计 --频率学派\n\t* P(x|c)因为样本x的数据量太小，很难估计准确\n\t\n### 频率学派 VS 贝叶斯学派\n* Frequentist 频率学派 \n参数是一个未知但客观存在的固定值\n* Bayesian 贝叶斯学派 \n参数本身是一个分布 \n\n### Naive Bayes \n* 属性条件独立性假设（假设每个属性独立地对分类结果发生影响）\n* Smoothing: 拉普拉斯修正（Laplacian Correction）\n* Lazy Learning\n\n### semi-naive Bayes classifier\n","slug":"bayes","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jryn0005si9glqy64ua3","content":"<h3 id=\"Discriminative-VS-Generative\"><a href=\"#Discriminative-VS-Generative\" class=\"headerlink\" title=\"Discriminative VS Generative\"></a>Discriminative VS Generative</h3><ul>\n<li>Discriminative models 判别模型<br>直接建模P(c|x)</li>\n<li>Generative models 生成模型<br>P(c|x) = P(x, c) / P(x) = P(c)*P(x|c)/P(x)<br>其中，<ul>\n<li>P(c)可以通过统计各类样本比例频率来估计 –频率学派</li>\n<li>P(x|c)因为样本x的数据量太小，很难估计准确</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"频率学派-VS-贝叶斯学派\"><a href=\"#频率学派-VS-贝叶斯学派\" class=\"headerlink\" title=\"频率学派 VS 贝叶斯学派\"></a>频率学派 VS 贝叶斯学派</h3><ul>\n<li>Frequentist 频率学派<br>参数是一个未知但客观存在的固定值</li>\n<li>Bayesian 贝叶斯学派<br>参数本身是一个分布 </li>\n</ul>\n<h3 id=\"Naive-Bayes\"><a href=\"#Naive-Bayes\" class=\"headerlink\" title=\"Naive Bayes\"></a>Naive Bayes</h3><ul>\n<li>属性条件独立性假设（假设每个属性独立地对分类结果发生影响）</li>\n<li>Smoothing: 拉普拉斯修正（Laplacian Correction）</li>\n<li>Lazy Learning</li>\n</ul>\n<h3 id=\"semi-naive-Bayes-classifier\"><a href=\"#semi-naive-Bayes-classifier\" class=\"headerlink\" title=\"semi-naive Bayes classifier\"></a>semi-naive Bayes classifier</h3>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Discriminative-VS-Generative\"><a href=\"#Discriminative-VS-Generative\" class=\"headerlink\" title=\"Discriminative VS Generative\"></a>Discriminative VS Generative</h3><ul>\n<li>Discriminative models 判别模型<br>直接建模P(c|x)</li>\n<li>Generative models 生成模型<br>P(c|x) = P(x, c) / P(x) = P(c)*P(x|c)/P(x)<br>其中，<ul>\n<li>P(c)可以通过统计各类样本比例频率来估计 –频率学派</li>\n<li>P(x|c)因为样本x的数据量太小，很难估计准确</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"频率学派-VS-贝叶斯学派\"><a href=\"#频率学派-VS-贝叶斯学派\" class=\"headerlink\" title=\"频率学派 VS 贝叶斯学派\"></a>频率学派 VS 贝叶斯学派</h3><ul>\n<li>Frequentist 频率学派<br>参数是一个未知但客观存在的固定值</li>\n<li>Bayesian 贝叶斯学派<br>参数本身是一个分布 </li>\n</ul>\n<h3 id=\"Naive-Bayes\"><a href=\"#Naive-Bayes\" class=\"headerlink\" title=\"Naive Bayes\"></a>Naive Bayes</h3><ul>\n<li>属性条件独立性假设（假设每个属性独立地对分类结果发生影响）</li>\n<li>Smoothing: 拉普拉斯修正（Laplacian Correction）</li>\n<li>Lazy Learning</li>\n</ul>\n<h3 id=\"semi-naive-Bayes-classifier\"><a href=\"#semi-naive-Bayes-classifier\" class=\"headerlink\" title=\"semi-naive Bayes classifier\"></a>semi-naive Bayes classifier</h3>"},{"title":"cnn","date":"2018-03-14T05:51:41.000Z","_content":"\n#### Detect vertical/horizontal edges\n\nCNN kernal below can be used for detecting vertical edges\n\n1 | 0 | -1\n--- | --- | ---\n1 | 0 | -1\n1 | 0 | -1\n\n#### stride & padding\n\nparams | values\n--- | ---\ninput volume size | W\nstride | S\npadding | P\nfilter size | F\noutput volume size | (W−F+2P)/S+1(W−F+2P)/S+1\n\n#### advantages\n* parameter sharing\n* sparsity of connections\n\n#### xavier_initializer\n* uniform distribution: x = sqrt(6. / (in + out)); [-x, x]\n* normal distribution: x = sqrt(2. / (in + out)); [-x, x]\n\n#### Convolution Demo\n* W: width = 5\n* H: Height = 5\n* D: Depth = 3\n* K: number of filters = 2\n* F: Filter size = 3\n* S: Stride = 2\n* P: Padding = 1\n\n {% asset_img \"cnn002.png\" [cnn002.png] %}\n\n<<<<<<< HEAD\n#### Pooling Demo\n\n {% asset_img \"cnn003.png\" [cnn003.png] %}\n\n#### LeNet\n* http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\n\n {% asset_img \"lenet.png\" [lenet.png] %}\n\n* CONV\n* POOL\n* FC\n\n#### AlexNet\n* http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n\n {% asset_img \"lenet.png\" [lenet.png] %}\n\n * Bigger\n * Deeper\n=======\n#### LeNet\n\n#### AlexNet\n>>>>>>> 1be2ebd4ec9a7cfdbd6c6e6d48a5059b0bc4ede5\n\n#### VGGNet\n\n#### GoogleNet\n\n#### ResNet\n\n#### REF\n* http://cs231n.github.io/convolutional-networks/\n","source":"_posts/cnn.md","raw":"---\ntitle: cnn\ndate: 2018-03-14 13:51:41\ntags:\n---\n\n#### Detect vertical/horizontal edges\n\nCNN kernal below can be used for detecting vertical edges\n\n1 | 0 | -1\n--- | --- | ---\n1 | 0 | -1\n1 | 0 | -1\n\n#### stride & padding\n\nparams | values\n--- | ---\ninput volume size | W\nstride | S\npadding | P\nfilter size | F\noutput volume size | (W−F+2P)/S+1(W−F+2P)/S+1\n\n#### advantages\n* parameter sharing\n* sparsity of connections\n\n#### xavier_initializer\n* uniform distribution: x = sqrt(6. / (in + out)); [-x, x]\n* normal distribution: x = sqrt(2. / (in + out)); [-x, x]\n\n#### Convolution Demo\n* W: width = 5\n* H: Height = 5\n* D: Depth = 3\n* K: number of filters = 2\n* F: Filter size = 3\n* S: Stride = 2\n* P: Padding = 1\n\n {% asset_img \"cnn002.png\" [cnn002.png] %}\n\n<<<<<<< HEAD\n#### Pooling Demo\n\n {% asset_img \"cnn003.png\" [cnn003.png] %}\n\n#### LeNet\n* http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\n\n {% asset_img \"lenet.png\" [lenet.png] %}\n\n* CONV\n* POOL\n* FC\n\n#### AlexNet\n* http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n\n {% asset_img \"lenet.png\" [lenet.png] %}\n\n * Bigger\n * Deeper\n=======\n#### LeNet\n\n#### AlexNet\n>>>>>>> 1be2ebd4ec9a7cfdbd6c6e6d48a5059b0bc4ede5\n\n#### VGGNet\n\n#### GoogleNet\n\n#### ResNet\n\n#### REF\n* http://cs231n.github.io/convolutional-networks/\n","slug":"cnn","published":1,"updated":"2018-03-18T16:47:58.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jryw0007si9gqihcx2ap","content":"<h4 id=\"Detect-vertical-horizontal-edges\"><a href=\"#Detect-vertical-horizontal-edges\" class=\"headerlink\" title=\"Detect vertical/horizontal edges\"></a>Detect vertical/horizontal edges</h4><p>CNN kernal below can be used for detecting vertical edges</p>\n<table>\n<thead>\n<tr>\n<th>1</th>\n<th>0</th>\n<th>-1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0</td>\n<td>-1</td>\n</tr>\n<tr>\n<td>1</td>\n<td>0</td>\n<td>-1</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"stride-amp-padding\"><a href=\"#stride-amp-padding\" class=\"headerlink\" title=\"stride &amp; padding\"></a>stride &amp; padding</h4><table>\n<thead>\n<tr>\n<th>params</th>\n<th>values</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>input volume size</td>\n<td>W</td>\n</tr>\n<tr>\n<td>stride</td>\n<td>S</td>\n</tr>\n<tr>\n<td>padding</td>\n<td>P</td>\n</tr>\n<tr>\n<td>filter size</td>\n<td>F</td>\n</tr>\n<tr>\n<td>output volume size</td>\n<td>(W−F+2P)/S+1(W−F+2P)/S+1</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"advantages\"><a href=\"#advantages\" class=\"headerlink\" title=\"advantages\"></a>advantages</h4><ul>\n<li>parameter sharing</li>\n<li>sparsity of connections</li>\n</ul>\n<h4 id=\"xavier-initializer\"><a href=\"#xavier-initializer\" class=\"headerlink\" title=\"xavier_initializer\"></a>xavier_initializer</h4><ul>\n<li>uniform distribution: x = sqrt(6. / (in + out)); [-x, x]</li>\n<li>normal distribution: x = sqrt(2. / (in + out)); [-x, x]</li>\n</ul>\n<h4 id=\"Convolution-Demo\"><a href=\"#Convolution-Demo\" class=\"headerlink\" title=\"Convolution Demo\"></a>Convolution Demo</h4><ul>\n<li>W: width = 5</li>\n<li>H: Height = 5</li>\n<li>D: Depth = 3</li>\n<li>K: number of filters = 2</li>\n<li>F: Filter size = 3</li>\n<li>S: Stride = 2</li>\n<li><p>P: Padding = 1</p>\n<img src=\"/2018/03/14/cnn/cnn002.png\" alt=\"[cnn002.png]\" title=\"[cnn002.png]\">\n</li>\n</ul>\n<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>\n<h4 id=\"Pooling-Demo\"><a href=\"#Pooling-Demo\" class=\"headerlink\" title=\"Pooling Demo\"></a>Pooling Demo</h4> <img src=\"/2018/03/14/cnn/cnn003.png\" alt=\"[cnn003.png]\" title=\"[cnn003.png]\">\n<h4 id=\"LeNet\"><a href=\"#LeNet\" class=\"headerlink\" title=\"LeNet\"></a>LeNet</h4><ul>\n<li><p><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\" target=\"_blank\" rel=\"external\">http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</a></p>\n<img src=\"/2018/03/14/cnn/lenet.png\" alt=\"[lenet.png]\" title=\"[lenet.png]\">\n</li>\n<li><p>CONV</p>\n</li>\n<li>POOL</li>\n<li>FC</li>\n</ul>\n<h4 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4><ul>\n<li><p><a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"external\">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>\n<img src=\"/2018/03/14/cnn/lenet.png\" alt=\"[lenet.png]\" title=\"[lenet.png]\">\n<ul>\n<li>Bigger</li>\n<li><h1 id=\"Deeper\"><a href=\"#Deeper\" class=\"headerlink\" title=\"Deeper\"></a>Deeper</h1><h4 id=\"LeNet-1\"><a href=\"#LeNet-1\" class=\"headerlink\" title=\"LeNet\"></a>LeNet</h4></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"AlexNet-1\"><a href=\"#AlexNet-1\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4><blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>1be2ebd4ec9a7cfdbd6c6e6d48a5059b0bc4ede5</p>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n<h4 id=\"VGGNet\"><a href=\"#VGGNet\" class=\"headerlink\" title=\"VGGNet\"></a>VGGNet</h4><h4 id=\"GoogleNet\"><a href=\"#GoogleNet\" class=\"headerlink\" title=\"GoogleNet\"></a>GoogleNet</h4><h4 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h4><h4 id=\"REF\"><a href=\"#REF\" class=\"headerlink\" title=\"REF\"></a>REF</h4><ul>\n<li><a href=\"http://cs231n.github.io/convolutional-networks/\" target=\"_blank\" rel=\"external\">http://cs231n.github.io/convolutional-networks/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"Detect-vertical-horizontal-edges\"><a href=\"#Detect-vertical-horizontal-edges\" class=\"headerlink\" title=\"Detect vertical/horizontal edges\"></a>Detect vertical/horizontal edges</h4><p>CNN kernal below can be used for detecting vertical edges</p>\n<table>\n<thead>\n<tr>\n<th>1</th>\n<th>0</th>\n<th>-1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0</td>\n<td>-1</td>\n</tr>\n<tr>\n<td>1</td>\n<td>0</td>\n<td>-1</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"stride-amp-padding\"><a href=\"#stride-amp-padding\" class=\"headerlink\" title=\"stride &amp; padding\"></a>stride &amp; padding</h4><table>\n<thead>\n<tr>\n<th>params</th>\n<th>values</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>input volume size</td>\n<td>W</td>\n</tr>\n<tr>\n<td>stride</td>\n<td>S</td>\n</tr>\n<tr>\n<td>padding</td>\n<td>P</td>\n</tr>\n<tr>\n<td>filter size</td>\n<td>F</td>\n</tr>\n<tr>\n<td>output volume size</td>\n<td>(W−F+2P)/S+1(W−F+2P)/S+1</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"advantages\"><a href=\"#advantages\" class=\"headerlink\" title=\"advantages\"></a>advantages</h4><ul>\n<li>parameter sharing</li>\n<li>sparsity of connections</li>\n</ul>\n<h4 id=\"xavier-initializer\"><a href=\"#xavier-initializer\" class=\"headerlink\" title=\"xavier_initializer\"></a>xavier_initializer</h4><ul>\n<li>uniform distribution: x = sqrt(6. / (in + out)); [-x, x]</li>\n<li>normal distribution: x = sqrt(2. / (in + out)); [-x, x]</li>\n</ul>\n<h4 id=\"Convolution-Demo\"><a href=\"#Convolution-Demo\" class=\"headerlink\" title=\"Convolution Demo\"></a>Convolution Demo</h4><ul>\n<li>W: width = 5</li>\n<li>H: Height = 5</li>\n<li>D: Depth = 3</li>\n<li>K: number of filters = 2</li>\n<li>F: Filter size = 3</li>\n<li>S: Stride = 2</li>\n<li><p>P: Padding = 1</p>\n<img src=\"/2018/03/14/cnn/cnn002.png\" alt=\"[cnn002.png]\" title=\"[cnn002.png]\">\n</li>\n</ul>\n<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>\n<h4 id=\"Pooling-Demo\"><a href=\"#Pooling-Demo\" class=\"headerlink\" title=\"Pooling Demo\"></a>Pooling Demo</h4> <img src=\"/2018/03/14/cnn/cnn003.png\" alt=\"[cnn003.png]\" title=\"[cnn003.png]\">\n<h4 id=\"LeNet\"><a href=\"#LeNet\" class=\"headerlink\" title=\"LeNet\"></a>LeNet</h4><ul>\n<li><p><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\" target=\"_blank\" rel=\"external\">http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</a></p>\n<img src=\"/2018/03/14/cnn/lenet.png\" alt=\"[lenet.png]\" title=\"[lenet.png]\">\n</li>\n<li><p>CONV</p>\n</li>\n<li>POOL</li>\n<li>FC</li>\n</ul>\n<h4 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4><ul>\n<li><p><a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"external\">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>\n<img src=\"/2018/03/14/cnn/lenet.png\" alt=\"[lenet.png]\" title=\"[lenet.png]\">\n<ul>\n<li>Bigger</li>\n<li><h1 id=\"Deeper\"><a href=\"#Deeper\" class=\"headerlink\" title=\"Deeper\"></a>Deeper</h1><h4 id=\"LeNet-1\"><a href=\"#LeNet-1\" class=\"headerlink\" title=\"LeNet\"></a>LeNet</h4></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"AlexNet-1\"><a href=\"#AlexNet-1\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4><blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>1be2ebd4ec9a7cfdbd6c6e6d48a5059b0bc4ede5</p>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n<h4 id=\"VGGNet\"><a href=\"#VGGNet\" class=\"headerlink\" title=\"VGGNet\"></a>VGGNet</h4><h4 id=\"GoogleNet\"><a href=\"#GoogleNet\" class=\"headerlink\" title=\"GoogleNet\"></a>GoogleNet</h4><h4 id=\"ResNet\"><a href=\"#ResNet\" class=\"headerlink\" title=\"ResNet\"></a>ResNet</h4><h4 id=\"REF\"><a href=\"#REF\" class=\"headerlink\" title=\"REF\"></a>REF</h4><ul>\n<li><a href=\"http://cs231n.github.io/convolutional-networks/\" target=\"_blank\" rel=\"external\">http://cs231n.github.io/convolutional-networks/</a></li>\n</ul>\n"},{"title":"ctr_recalibration","date":"2017-11-23T12:21:40.000Z","_content":"\n### CTR为什么会不准？\n在计算广告中，pCTR往往对比真实的CTR偏高或者偏低的现象，尤其在\n1. 热门曝光广告和冷门曝光广告之间\n2. 高CTR广告和低CTR广告之间\n\n因此，CTR需要校准。\n\n<!-- more -->\n\n### CTR为什么要校准？\nAUC体现序准；\nlogloss体现值准；\n要计算商业价值最大化，因此需要值准；\n因此需要校准；\n校准之后，logloss会降低。\n\n### CTR如何校准\nCTR校准有很多方法，本质在于“拟合校准前和校准后”，即\nf(pCTR校准前) = pCTR校准后\n如何设计函数f，是校准的关键。\n\n#### binning\nbinning就是样本等频分桶后，每个bin求平均，如下图：\n {% asset_img \"图1.1.png\" [图1.1.png] %}\n\n#### Isotonic regression(保序回归）\n保序回归，就是单调回归（保证按照自变量x和按照因变量y排序序不变，即成正比）\n为何要保序？\n为了保证不影响AUC，即默认原始CTR和校准后CTR的正相关性。\n{% asset_img \"图2.2.png\" [图2.2.png] %}\n\n### Best practice\n#### 分解动作\n* 将统计ctr加入特征中（最好做离散化处理）\n* 建立f(pCTR)=统计CTR的函数\n* 进行将f(pCTR)作为新的CTR\n#### 小demo\n假设训练数据集合为：\n物品3：pCTR_统计=0.8\n物品2：pCTR_统计=0.5\n物品1：pCTR_统计=0.3\n\n##### 原始LR\n```\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nX = [\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3]\n]\ny = [\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n0,\n0]\nLR = LogisticRegression()\nLR.fit(X, y)\ny_p = LR.predict_proba(X)\nscore = log_loss(y, y_p)\n```\n\n##### LR+保序回归\n```\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.isotonic import IsotonicRegression\nX = [\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3]\n]\ny = [\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n0,\n0]\nLR = LogisticRegression()\nLR.fit(X, y)\ny_lr = LR.predict_proba(X)\nir = IsotonicRegression()\ny_ir = ir.fit_transform(map(lambda x:x[1], y_lr), map(lambda x:x[0], X))\nscore = log_loss(y, y_ir)\n```\n\n##### itemID离散化LR\n```\nX = [\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n]\ny = [\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n0,\n0]\nLR = LogisticRegression()\nLR.fit(X, y)\ny_p = LR.predict_proba(X)\nscore = log_loss(y, y_p)\n```\n\n### 采样的校准\n由于负样本抽样后，会造成点击率偏高的假象，需要将预测值还原成真实的值。调整的公式如下：\n\n#### 结论\n{% asset_img \"图4.4.png\" [图4.4.png] %}\n\n#### 推导\n{% asset_img \"图3.3.png\" [图3.3.png] %}\n\nREF:\nhttps://tech.meituan.com/mt_dsp.html\nhttp://blog.csdn.net/lming_08/article/details/40214921\n\n","source":"_posts/ctr-recalibration.md","raw":"---\ntitle: ctr_recalibration\ndate: 2017-11-23 20:21:40\ntags: ML\n---\n\n### CTR为什么会不准？\n在计算广告中，pCTR往往对比真实的CTR偏高或者偏低的现象，尤其在\n1. 热门曝光广告和冷门曝光广告之间\n2. 高CTR广告和低CTR广告之间\n\n因此，CTR需要校准。\n\n<!-- more -->\n\n### CTR为什么要校准？\nAUC体现序准；\nlogloss体现值准；\n要计算商业价值最大化，因此需要值准；\n因此需要校准；\n校准之后，logloss会降低。\n\n### CTR如何校准\nCTR校准有很多方法，本质在于“拟合校准前和校准后”，即\nf(pCTR校准前) = pCTR校准后\n如何设计函数f，是校准的关键。\n\n#### binning\nbinning就是样本等频分桶后，每个bin求平均，如下图：\n {% asset_img \"图1.1.png\" [图1.1.png] %}\n\n#### Isotonic regression(保序回归）\n保序回归，就是单调回归（保证按照自变量x和按照因变量y排序序不变，即成正比）\n为何要保序？\n为了保证不影响AUC，即默认原始CTR和校准后CTR的正相关性。\n{% asset_img \"图2.2.png\" [图2.2.png] %}\n\n### Best practice\n#### 分解动作\n* 将统计ctr加入特征中（最好做离散化处理）\n* 建立f(pCTR)=统计CTR的函数\n* 进行将f(pCTR)作为新的CTR\n#### 小demo\n假设训练数据集合为：\n物品3：pCTR_统计=0.8\n物品2：pCTR_统计=0.5\n物品1：pCTR_统计=0.3\n\n##### 原始LR\n```\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nX = [\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3]\n]\ny = [\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n0,\n0]\nLR = LogisticRegression()\nLR.fit(X, y)\ny_p = LR.predict_proba(X)\nscore = log_loss(y, y_p)\n```\n\n##### LR+保序回归\n```\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.isotonic import IsotonicRegression\nX = [\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.8],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.5],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3],\n[0.3]\n]\ny = [\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n0,\n0]\nLR = LogisticRegression()\nLR.fit(X, y)\ny_lr = LR.predict_proba(X)\nir = IsotonicRegression()\ny_ir = ir.fit_transform(map(lambda x:x[1], y_lr), map(lambda x:x[0], X))\nscore = log_loss(y, y_ir)\n```\n\n##### itemID离散化LR\n```\nX = [\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,0,1],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[0,1,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n[1,0,0],\n]\ny = [\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n1,\n1,\n1,\n0,\n0,\n0,\n0,\n0,\n0,\n0]\nLR = LogisticRegression()\nLR.fit(X, y)\ny_p = LR.predict_proba(X)\nscore = log_loss(y, y_p)\n```\n\n### 采样的校准\n由于负样本抽样后，会造成点击率偏高的假象，需要将预测值还原成真实的值。调整的公式如下：\n\n#### 结论\n{% asset_img \"图4.4.png\" [图4.4.png] %}\n\n#### 推导\n{% asset_img \"图3.3.png\" [图3.3.png] %}\n\nREF:\nhttps://tech.meituan.com/mt_dsp.html\nhttp://blog.csdn.net/lming_08/article/details/40214921\n\n","slug":"ctr-recalibration","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jryz0008si9gz0yzyxuy","content":"<h3 id=\"CTR为什么会不准？\"><a href=\"#CTR为什么会不准？\" class=\"headerlink\" title=\"CTR为什么会不准？\"></a>CTR为什么会不准？</h3><p>在计算广告中，pCTR往往对比真实的CTR偏高或者偏低的现象，尤其在</p>\n<ol>\n<li>热门曝光广告和冷门曝光广告之间</li>\n<li>高CTR广告和低CTR广告之间</li>\n</ol>\n<p>因此，CTR需要校准。</p>\n<a id=\"more\"></a>\n<h3 id=\"CTR为什么要校准？\"><a href=\"#CTR为什么要校准？\" class=\"headerlink\" title=\"CTR为什么要校准？\"></a>CTR为什么要校准？</h3><p>AUC体现序准；<br>logloss体现值准；<br>要计算商业价值最大化，因此需要值准；<br>因此需要校准；<br>校准之后，logloss会降低。</p>\n<h3 id=\"CTR如何校准\"><a href=\"#CTR如何校准\" class=\"headerlink\" title=\"CTR如何校准\"></a>CTR如何校准</h3><p>CTR校准有很多方法，本质在于“拟合校准前和校准后”，即<br>f(pCTR校准前) = pCTR校准后<br>如何设计函数f，是校准的关键。</p>\n<h4 id=\"binning\"><a href=\"#binning\" class=\"headerlink\" title=\"binning\"></a>binning</h4><p>binning就是样本等频分桶后，每个bin求平均，如下图：<br> <img src=\"/2017/11/23/ctr-recalibration/图1.1.png\" alt=\"[图1.1.png]\" title=\"[图1.1.png]\"></p>\n<h4 id=\"Isotonic-regression-保序回归）\"><a href=\"#Isotonic-regression-保序回归）\" class=\"headerlink\" title=\"Isotonic regression(保序回归）\"></a>Isotonic regression(保序回归）</h4><p>保序回归，就是单调回归（保证按照自变量x和按照因变量y排序序不变，即成正比）<br>为何要保序？<br>为了保证不影响AUC，即默认原始CTR和校准后CTR的正相关性。<br><img src=\"/2017/11/23/ctr-recalibration/图2.2.png\" alt=\"[图2.2.png]\" title=\"[图2.2.png]\"></p>\n<h3 id=\"Best-practice\"><a href=\"#Best-practice\" class=\"headerlink\" title=\"Best practice\"></a>Best practice</h3><h4 id=\"分解动作\"><a href=\"#分解动作\" class=\"headerlink\" title=\"分解动作\"></a>分解动作</h4><ul>\n<li>将统计ctr加入特征中（最好做离散化处理）</li>\n<li>建立f(pCTR)=统计CTR的函数</li>\n<li>进行将f(pCTR)作为新的CTR<h4 id=\"小demo\"><a href=\"#小demo\" class=\"headerlink\" title=\"小demo\"></a>小demo</h4>假设训练数据集合为：<br>物品3：pCTR<em>统计=0.8<br>物品2：pCTR</em>统计=0.5<br>物品1：pCTR_统计=0.3</li>\n</ul>\n<h5 id=\"原始LR\"><a href=\"#原始LR\" class=\"headerlink\" title=\"原始LR\"></a>原始LR</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div></pre></td><td class=\"code\"><pre><div class=\"line\">from sklearn.metrics import log_loss</div><div class=\"line\">from sklearn.linear_model import LogisticRegression</div><div class=\"line\">X = [</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3]</div><div class=\"line\">]</div><div class=\"line\">y = [</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0]</div><div class=\"line\">LR = LogisticRegression()</div><div class=\"line\">LR.fit(X, y)</div><div class=\"line\">y_p = LR.predict_proba(X)</div><div class=\"line\">score = log_loss(y, y_p)</div></pre></td></tr></table></figure>\n<h5 id=\"LR-保序回归\"><a href=\"#LR-保序回归\" class=\"headerlink\" title=\"LR+保序回归\"></a>LR+保序回归</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div></pre></td><td class=\"code\"><pre><div class=\"line\">from sklearn.metrics import log_loss</div><div class=\"line\">from sklearn.linear_model import LogisticRegression</div><div class=\"line\">from sklearn.isotonic import IsotonicRegression</div><div class=\"line\">X = [</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3]</div><div class=\"line\">]</div><div class=\"line\">y = [</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0]</div><div class=\"line\">LR = LogisticRegression()</div><div class=\"line\">LR.fit(X, y)</div><div class=\"line\">y_lr = LR.predict_proba(X)</div><div class=\"line\">ir = IsotonicRegression()</div><div class=\"line\">y_ir = ir.fit_transform(map(lambda x:x[1], y_lr), map(lambda x:x[0], X))</div><div class=\"line\">score = log_loss(y, y_ir)</div></pre></td></tr></table></figure>\n<h5 id=\"itemID离散化LR\"><a href=\"#itemID离散化LR\" class=\"headerlink\" title=\"itemID离散化LR\"></a>itemID离散化LR</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div></pre></td><td class=\"code\"><pre><div class=\"line\">X = [</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">]</div><div class=\"line\">y = [</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0]</div><div class=\"line\">LR = LogisticRegression()</div><div class=\"line\">LR.fit(X, y)</div><div class=\"line\">y_p = LR.predict_proba(X)</div><div class=\"line\">score = log_loss(y, y_p)</div></pre></td></tr></table></figure>\n<h3 id=\"采样的校准\"><a href=\"#采样的校准\" class=\"headerlink\" title=\"采样的校准\"></a>采样的校准</h3><p>由于负样本抽样后，会造成点击率偏高的假象，需要将预测值还原成真实的值。调整的公式如下：</p>\n<h4 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h4><img src=\"/2017/11/23/ctr-recalibration/图4.4.png\" alt=\"[图4.4.png]\" title=\"[图4.4.png]\">\n<h4 id=\"推导\"><a href=\"#推导\" class=\"headerlink\" title=\"推导\"></a>推导</h4><img src=\"/2017/11/23/ctr-recalibration/图3.3.png\" alt=\"[图3.3.png]\" title=\"[图3.3.png]\">\n<p>REF:<br><a href=\"https://tech.meituan.com/mt_dsp.html\" target=\"_blank\" rel=\"external\">https://tech.meituan.com/mt_dsp.html</a><br><a href=\"http://blog.csdn.net/lming_08/article/details/40214921\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/lming_08/article/details/40214921</a></p>\n","site":{"data":{}},"excerpt":"<h3 id=\"CTR为什么会不准？\"><a href=\"#CTR为什么会不准？\" class=\"headerlink\" title=\"CTR为什么会不准？\"></a>CTR为什么会不准？</h3><p>在计算广告中，pCTR往往对比真实的CTR偏高或者偏低的现象，尤其在</p>\n<ol>\n<li>热门曝光广告和冷门曝光广告之间</li>\n<li>高CTR广告和低CTR广告之间</li>\n</ol>\n<p>因此，CTR需要校准。</p>","more":"<h3 id=\"CTR为什么要校准？\"><a href=\"#CTR为什么要校准？\" class=\"headerlink\" title=\"CTR为什么要校准？\"></a>CTR为什么要校准？</h3><p>AUC体现序准；<br>logloss体现值准；<br>要计算商业价值最大化，因此需要值准；<br>因此需要校准；<br>校准之后，logloss会降低。</p>\n<h3 id=\"CTR如何校准\"><a href=\"#CTR如何校准\" class=\"headerlink\" title=\"CTR如何校准\"></a>CTR如何校准</h3><p>CTR校准有很多方法，本质在于“拟合校准前和校准后”，即<br>f(pCTR校准前) = pCTR校准后<br>如何设计函数f，是校准的关键。</p>\n<h4 id=\"binning\"><a href=\"#binning\" class=\"headerlink\" title=\"binning\"></a>binning</h4><p>binning就是样本等频分桶后，每个bin求平均，如下图：<br> <img src=\"/2017/11/23/ctr-recalibration/图1.1.png\" alt=\"[图1.1.png]\" title=\"[图1.1.png]\"></p>\n<h4 id=\"Isotonic-regression-保序回归）\"><a href=\"#Isotonic-regression-保序回归）\" class=\"headerlink\" title=\"Isotonic regression(保序回归）\"></a>Isotonic regression(保序回归）</h4><p>保序回归，就是单调回归（保证按照自变量x和按照因变量y排序序不变，即成正比）<br>为何要保序？<br>为了保证不影响AUC，即默认原始CTR和校准后CTR的正相关性。<br><img src=\"/2017/11/23/ctr-recalibration/图2.2.png\" alt=\"[图2.2.png]\" title=\"[图2.2.png]\"></p>\n<h3 id=\"Best-practice\"><a href=\"#Best-practice\" class=\"headerlink\" title=\"Best practice\"></a>Best practice</h3><h4 id=\"分解动作\"><a href=\"#分解动作\" class=\"headerlink\" title=\"分解动作\"></a>分解动作</h4><ul>\n<li>将统计ctr加入特征中（最好做离散化处理）</li>\n<li>建立f(pCTR)=统计CTR的函数</li>\n<li>进行将f(pCTR)作为新的CTR<h4 id=\"小demo\"><a href=\"#小demo\" class=\"headerlink\" title=\"小demo\"></a>小demo</h4>假设训练数据集合为：<br>物品3：pCTR<em>统计=0.8<br>物品2：pCTR</em>统计=0.5<br>物品1：pCTR_统计=0.3</li>\n</ul>\n<h5 id=\"原始LR\"><a href=\"#原始LR\" class=\"headerlink\" title=\"原始LR\"></a>原始LR</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div></pre></td><td class=\"code\"><pre><div class=\"line\">from sklearn.metrics import log_loss</div><div class=\"line\">from sklearn.linear_model import LogisticRegression</div><div class=\"line\">X = [</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3]</div><div class=\"line\">]</div><div class=\"line\">y = [</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0]</div><div class=\"line\">LR = LogisticRegression()</div><div class=\"line\">LR.fit(X, y)</div><div class=\"line\">y_p = LR.predict_proba(X)</div><div class=\"line\">score = log_loss(y, y_p)</div></pre></td></tr></table></figure>\n<h5 id=\"LR-保序回归\"><a href=\"#LR-保序回归\" class=\"headerlink\" title=\"LR+保序回归\"></a>LR+保序回归</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div></pre></td><td class=\"code\"><pre><div class=\"line\">from sklearn.metrics import log_loss</div><div class=\"line\">from sklearn.linear_model import LogisticRegression</div><div class=\"line\">from sklearn.isotonic import IsotonicRegression</div><div class=\"line\">X = [</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.8],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.5],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3],</div><div class=\"line\">[0.3]</div><div class=\"line\">]</div><div class=\"line\">y = [</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0]</div><div class=\"line\">LR = LogisticRegression()</div><div class=\"line\">LR.fit(X, y)</div><div class=\"line\">y_lr = LR.predict_proba(X)</div><div class=\"line\">ir = IsotonicRegression()</div><div class=\"line\">y_ir = ir.fit_transform(map(lambda x:x[1], y_lr), map(lambda x:x[0], X))</div><div class=\"line\">score = log_loss(y, y_ir)</div></pre></td></tr></table></figure>\n<h5 id=\"itemID离散化LR\"><a href=\"#itemID离散化LR\" class=\"headerlink\" title=\"itemID离散化LR\"></a>itemID离散化LR</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div></pre></td><td class=\"code\"><pre><div class=\"line\">X = [</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,0,1],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[0,1,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">[1,0,0],</div><div class=\"line\">]</div><div class=\"line\">y = [</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">1,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0,</div><div class=\"line\">0]</div><div class=\"line\">LR = LogisticRegression()</div><div class=\"line\">LR.fit(X, y)</div><div class=\"line\">y_p = LR.predict_proba(X)</div><div class=\"line\">score = log_loss(y, y_p)</div></pre></td></tr></table></figure>\n<h3 id=\"采样的校准\"><a href=\"#采样的校准\" class=\"headerlink\" title=\"采样的校准\"></a>采样的校准</h3><p>由于负样本抽样后，会造成点击率偏高的假象，需要将预测值还原成真实的值。调整的公式如下：</p>\n<h4 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h4><img src=\"/2017/11/23/ctr-recalibration/图4.4.png\" alt=\"[图4.4.png]\" title=\"[图4.4.png]\">\n<h4 id=\"推导\"><a href=\"#推导\" class=\"headerlink\" title=\"推导\"></a>推导</h4><img src=\"/2017/11/23/ctr-recalibration/图3.3.png\" alt=\"[图3.3.png]\" title=\"[图3.3.png]\">\n<p>REF:<br><a href=\"https://tech.meituan.com/mt_dsp.html\" target=\"_blank\" rel=\"external\">https://tech.meituan.com/mt_dsp.html</a><br><a href=\"http://blog.csdn.net/lming_08/article/details/40214921\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/lming_08/article/details/40214921</a></p>"},{"title":"ctr_smooth","date":"2017-12-07T15:05:56.000Z","_content":"\n#### 最好的非个性化模型\n最好的非个性化模型，即CTR倒序模型，那如何得到准确的统计CTR，是本文的关键。\n\n#### 统计的小数效应（置信度和pCTR）\n风控模型中，有两个值非常重要：\n* confidence：用户的数据有多可靠，比如交易记录越多，越可靠\n* score: 用户的资产有多少，比如交易额度越大，资产越多\n\n两个case：\n* 高confidence低score：卖矿泉水的小商贩，转账频繁，大多都是一两块钱\n* 低confidence高score：偶尔用信用卡买了一辆车的大老板\n\n同样的在统计CTR中，也有对应的两个概念：\n* CTR可信不可信\n* CTR是多少\n\n同样的两个case：\n* itemA：10次曝光5次点击，可能受到随机影响，所以confidence低，pCTR高（随机影响也可能导致pCTR偏低）\n* itemB：10000次曝光1000次点击，大数效应，比较可信，confidence高，pCTR低\n\n#### 三个变量的权衡\n为了得到真实的CTR，可以从日志中统计得到：\nexposure, click, ctr\n三个变量\n\nitem | exposure | click | ctr\n---|---|---|---\nA | 100000 | 20000 | 0.2\nB | 10000 | 1000 | 0.1\nC | 10 | 5 | 0.5\n\n那么哪款item最优先级被推荐？\n\n理想情况下是：\n* exposure和click越高越高，confidence越大\n* ctr越高越好，score越大\n\n但是当两者矛盾的时候，就需要平衡一下，综合来看A是最佳的。\n\n#### 简单暴力的bayes平滑\n根据贝叶斯有：先验+事件=后验，那么我们为模型增加人为的知识：\n\n\"所有样本，统一增加b个样本（其中a个正样本)\"\n\na和b的相对值的确定，可以用a/b等于整体平均ctr*rate等来确定，rate常常略微小于1 \n\na和b的绝对值的确定，很有意思：\n\n我常常用excel对优质数据进行标注，看如何设置可以使得优质数据上浮顶部。\n\n#### 贝叶斯平滑物理意义 和 极大后验\n贝叶斯平滑，相当于增加先验，即增加正则，先举一个L2正则的例子：\n线性回归的loss function\n（PS：loss function和cost function是不一样的，cost function是loss function在data上的累计总和）\n\nloss function: (y - f(x))^2\nmaximum likelihood: guass_function(y-f(x))\n\nL2正则相当于对参数的分布增加一个属于高斯分布的假设，\nguass_function(theta)*guass_function(y-f(x))\n\n将这个似然最大化，\nargmax(guass_function(theta)*guass_function(y-f(x)))\n=> argmax(log(guass_function(theta)*guass_function(y-f(x))))\n=> argmax(log(guass_function(theta))+log(guass_function(y-f(x))))\n=> argmin((theta)^2+(y_f(x))^2)\n\n即极大后验。\n\n因此，从参数估计的角度，贝叶斯平滑是将极大似然估计（直接除）变成极大后验估计（分子分母各加一个值）\n\n\n#### REF\n1. http://myslide.cn/slides/977\n2. http://www.jianshu.com/p/a47c46153326","source":"_posts/ctr-smooth.md","raw":"---\ntitle: ctr_smooth\ndate: 2017-12-07 23:05:56\ntags:\n---\n\n#### 最好的非个性化模型\n最好的非个性化模型，即CTR倒序模型，那如何得到准确的统计CTR，是本文的关键。\n\n#### 统计的小数效应（置信度和pCTR）\n风控模型中，有两个值非常重要：\n* confidence：用户的数据有多可靠，比如交易记录越多，越可靠\n* score: 用户的资产有多少，比如交易额度越大，资产越多\n\n两个case：\n* 高confidence低score：卖矿泉水的小商贩，转账频繁，大多都是一两块钱\n* 低confidence高score：偶尔用信用卡买了一辆车的大老板\n\n同样的在统计CTR中，也有对应的两个概念：\n* CTR可信不可信\n* CTR是多少\n\n同样的两个case：\n* itemA：10次曝光5次点击，可能受到随机影响，所以confidence低，pCTR高（随机影响也可能导致pCTR偏低）\n* itemB：10000次曝光1000次点击，大数效应，比较可信，confidence高，pCTR低\n\n#### 三个变量的权衡\n为了得到真实的CTR，可以从日志中统计得到：\nexposure, click, ctr\n三个变量\n\nitem | exposure | click | ctr\n---|---|---|---\nA | 100000 | 20000 | 0.2\nB | 10000 | 1000 | 0.1\nC | 10 | 5 | 0.5\n\n那么哪款item最优先级被推荐？\n\n理想情况下是：\n* exposure和click越高越高，confidence越大\n* ctr越高越好，score越大\n\n但是当两者矛盾的时候，就需要平衡一下，综合来看A是最佳的。\n\n#### 简单暴力的bayes平滑\n根据贝叶斯有：先验+事件=后验，那么我们为模型增加人为的知识：\n\n\"所有样本，统一增加b个样本（其中a个正样本)\"\n\na和b的相对值的确定，可以用a/b等于整体平均ctr*rate等来确定，rate常常略微小于1 \n\na和b的绝对值的确定，很有意思：\n\n我常常用excel对优质数据进行标注，看如何设置可以使得优质数据上浮顶部。\n\n#### 贝叶斯平滑物理意义 和 极大后验\n贝叶斯平滑，相当于增加先验，即增加正则，先举一个L2正则的例子：\n线性回归的loss function\n（PS：loss function和cost function是不一样的，cost function是loss function在data上的累计总和）\n\nloss function: (y - f(x))^2\nmaximum likelihood: guass_function(y-f(x))\n\nL2正则相当于对参数的分布增加一个属于高斯分布的假设，\nguass_function(theta)*guass_function(y-f(x))\n\n将这个似然最大化，\nargmax(guass_function(theta)*guass_function(y-f(x)))\n=> argmax(log(guass_function(theta)*guass_function(y-f(x))))\n=> argmax(log(guass_function(theta))+log(guass_function(y-f(x))))\n=> argmin((theta)^2+(y_f(x))^2)\n\n即极大后验。\n\n因此，从参数估计的角度，贝叶斯平滑是将极大似然估计（直接除）变成极大后验估计（分子分母各加一个值）\n\n\n#### REF\n1. http://myslide.cn/slides/977\n2. http://www.jianshu.com/p/a47c46153326","slug":"ctr-smooth","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrz20009si9gdqtly1ci","content":"<h4 id=\"最好的非个性化模型\"><a href=\"#最好的非个性化模型\" class=\"headerlink\" title=\"最好的非个性化模型\"></a>最好的非个性化模型</h4><p>最好的非个性化模型，即CTR倒序模型，那如何得到准确的统计CTR，是本文的关键。</p>\n<h4 id=\"统计的小数效应（置信度和pCTR）\"><a href=\"#统计的小数效应（置信度和pCTR）\" class=\"headerlink\" title=\"统计的小数效应（置信度和pCTR）\"></a>统计的小数效应（置信度和pCTR）</h4><p>风控模型中，有两个值非常重要：</p>\n<ul>\n<li>confidence：用户的数据有多可靠，比如交易记录越多，越可靠</li>\n<li>score: 用户的资产有多少，比如交易额度越大，资产越多</li>\n</ul>\n<p>两个case：</p>\n<ul>\n<li>高confidence低score：卖矿泉水的小商贩，转账频繁，大多都是一两块钱</li>\n<li>低confidence高score：偶尔用信用卡买了一辆车的大老板</li>\n</ul>\n<p>同样的在统计CTR中，也有对应的两个概念：</p>\n<ul>\n<li>CTR可信不可信</li>\n<li>CTR是多少</li>\n</ul>\n<p>同样的两个case：</p>\n<ul>\n<li>itemA：10次曝光5次点击，可能受到随机影响，所以confidence低，pCTR高（随机影响也可能导致pCTR偏低）</li>\n<li>itemB：10000次曝光1000次点击，大数效应，比较可信，confidence高，pCTR低</li>\n</ul>\n<h4 id=\"三个变量的权衡\"><a href=\"#三个变量的权衡\" class=\"headerlink\" title=\"三个变量的权衡\"></a>三个变量的权衡</h4><p>为了得到真实的CTR，可以从日志中统计得到：<br>exposure, click, ctr<br>三个变量</p>\n<table>\n<thead>\n<tr>\n<th>item</th>\n<th>exposure</th>\n<th>click</th>\n<th>ctr</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>100000</td>\n<td>20000</td>\n<td>0.2</td>\n</tr>\n<tr>\n<td>B</td>\n<td>10000</td>\n<td>1000</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>C</td>\n<td>10</td>\n<td>5</td>\n<td>0.5</td>\n</tr>\n</tbody>\n</table>\n<p>那么哪款item最优先级被推荐？</p>\n<p>理想情况下是：</p>\n<ul>\n<li>exposure和click越高越高，confidence越大</li>\n<li>ctr越高越好，score越大</li>\n</ul>\n<p>但是当两者矛盾的时候，就需要平衡一下，综合来看A是最佳的。</p>\n<h4 id=\"简单暴力的bayes平滑\"><a href=\"#简单暴力的bayes平滑\" class=\"headerlink\" title=\"简单暴力的bayes平滑\"></a>简单暴力的bayes平滑</h4><p>根据贝叶斯有：先验+事件=后验，那么我们为模型增加人为的知识：</p>\n<p>“所有样本，统一增加b个样本（其中a个正样本)”</p>\n<p>a和b的相对值的确定，可以用a/b等于整体平均ctr*rate等来确定，rate常常略微小于1 </p>\n<p>a和b的绝对值的确定，很有意思：</p>\n<p>我常常用excel对优质数据进行标注，看如何设置可以使得优质数据上浮顶部。</p>\n<h4 id=\"贝叶斯平滑物理意义-和-极大后验\"><a href=\"#贝叶斯平滑物理意义-和-极大后验\" class=\"headerlink\" title=\"贝叶斯平滑物理意义 和 极大后验\"></a>贝叶斯平滑物理意义 和 极大后验</h4><p>贝叶斯平滑，相当于增加先验，即增加正则，先举一个L2正则的例子：<br>线性回归的loss function<br>（PS：loss function和cost function是不一样的，cost function是loss function在data上的累计总和）</p>\n<p>loss function: (y - f(x))^2<br>maximum likelihood: guass_function(y-f(x))</p>\n<p>L2正则相当于对参数的分布增加一个属于高斯分布的假设，<br>guass_function(theta)*guass_function(y-f(x))</p>\n<p>将这个似然最大化，<br>argmax(guass_function(theta)<em>guass_function(y-f(x)))<br>=&gt; argmax(log(guass_function(theta)</em>guass_function(y-f(x))))<br>=&gt; argmax(log(guass_function(theta))+log(guass_function(y-f(x))))<br>=&gt; argmin((theta)^2+(y_f(x))^2)</p>\n<p>即极大后验。</p>\n<p>因此，从参数估计的角度，贝叶斯平滑是将极大似然估计（直接除）变成极大后验估计（分子分母各加一个值）</p>\n<h4 id=\"REF\"><a href=\"#REF\" class=\"headerlink\" title=\"REF\"></a>REF</h4><ol>\n<li><a href=\"http://myslide.cn/slides/977\" target=\"_blank\" rel=\"external\">http://myslide.cn/slides/977</a></li>\n<li><a href=\"http://www.jianshu.com/p/a47c46153326\" target=\"_blank\" rel=\"external\">http://www.jianshu.com/p/a47c46153326</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"最好的非个性化模型\"><a href=\"#最好的非个性化模型\" class=\"headerlink\" title=\"最好的非个性化模型\"></a>最好的非个性化模型</h4><p>最好的非个性化模型，即CTR倒序模型，那如何得到准确的统计CTR，是本文的关键。</p>\n<h4 id=\"统计的小数效应（置信度和pCTR）\"><a href=\"#统计的小数效应（置信度和pCTR）\" class=\"headerlink\" title=\"统计的小数效应（置信度和pCTR）\"></a>统计的小数效应（置信度和pCTR）</h4><p>风控模型中，有两个值非常重要：</p>\n<ul>\n<li>confidence：用户的数据有多可靠，比如交易记录越多，越可靠</li>\n<li>score: 用户的资产有多少，比如交易额度越大，资产越多</li>\n</ul>\n<p>两个case：</p>\n<ul>\n<li>高confidence低score：卖矿泉水的小商贩，转账频繁，大多都是一两块钱</li>\n<li>低confidence高score：偶尔用信用卡买了一辆车的大老板</li>\n</ul>\n<p>同样的在统计CTR中，也有对应的两个概念：</p>\n<ul>\n<li>CTR可信不可信</li>\n<li>CTR是多少</li>\n</ul>\n<p>同样的两个case：</p>\n<ul>\n<li>itemA：10次曝光5次点击，可能受到随机影响，所以confidence低，pCTR高（随机影响也可能导致pCTR偏低）</li>\n<li>itemB：10000次曝光1000次点击，大数效应，比较可信，confidence高，pCTR低</li>\n</ul>\n<h4 id=\"三个变量的权衡\"><a href=\"#三个变量的权衡\" class=\"headerlink\" title=\"三个变量的权衡\"></a>三个变量的权衡</h4><p>为了得到真实的CTR，可以从日志中统计得到：<br>exposure, click, ctr<br>三个变量</p>\n<table>\n<thead>\n<tr>\n<th>item</th>\n<th>exposure</th>\n<th>click</th>\n<th>ctr</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>100000</td>\n<td>20000</td>\n<td>0.2</td>\n</tr>\n<tr>\n<td>B</td>\n<td>10000</td>\n<td>1000</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>C</td>\n<td>10</td>\n<td>5</td>\n<td>0.5</td>\n</tr>\n</tbody>\n</table>\n<p>那么哪款item最优先级被推荐？</p>\n<p>理想情况下是：</p>\n<ul>\n<li>exposure和click越高越高，confidence越大</li>\n<li>ctr越高越好，score越大</li>\n</ul>\n<p>但是当两者矛盾的时候，就需要平衡一下，综合来看A是最佳的。</p>\n<h4 id=\"简单暴力的bayes平滑\"><a href=\"#简单暴力的bayes平滑\" class=\"headerlink\" title=\"简单暴力的bayes平滑\"></a>简单暴力的bayes平滑</h4><p>根据贝叶斯有：先验+事件=后验，那么我们为模型增加人为的知识：</p>\n<p>“所有样本，统一增加b个样本（其中a个正样本)”</p>\n<p>a和b的相对值的确定，可以用a/b等于整体平均ctr*rate等来确定，rate常常略微小于1 </p>\n<p>a和b的绝对值的确定，很有意思：</p>\n<p>我常常用excel对优质数据进行标注，看如何设置可以使得优质数据上浮顶部。</p>\n<h4 id=\"贝叶斯平滑物理意义-和-极大后验\"><a href=\"#贝叶斯平滑物理意义-和-极大后验\" class=\"headerlink\" title=\"贝叶斯平滑物理意义 和 极大后验\"></a>贝叶斯平滑物理意义 和 极大后验</h4><p>贝叶斯平滑，相当于增加先验，即增加正则，先举一个L2正则的例子：<br>线性回归的loss function<br>（PS：loss function和cost function是不一样的，cost function是loss function在data上的累计总和）</p>\n<p>loss function: (y - f(x))^2<br>maximum likelihood: guass_function(y-f(x))</p>\n<p>L2正则相当于对参数的分布增加一个属于高斯分布的假设，<br>guass_function(theta)*guass_function(y-f(x))</p>\n<p>将这个似然最大化，<br>argmax(guass_function(theta)<em>guass_function(y-f(x)))<br>=&gt; argmax(log(guass_function(theta)</em>guass_function(y-f(x))))<br>=&gt; argmax(log(guass_function(theta))+log(guass_function(y-f(x))))<br>=&gt; argmin((theta)^2+(y_f(x))^2)</p>\n<p>即极大后验。</p>\n<p>因此，从参数估计的角度，贝叶斯平滑是将极大似然估计（直接除）变成极大后验估计（分子分母各加一个值）</p>\n<h4 id=\"REF\"><a href=\"#REF\" class=\"headerlink\" title=\"REF\"></a>REF</h4><ol>\n<li><a href=\"http://myslide.cn/slides/977\" target=\"_blank\" rel=\"external\">http://myslide.cn/slides/977</a></li>\n<li><a href=\"http://www.jianshu.com/p/a47c46153326\" target=\"_blank\" rel=\"external\">http://www.jianshu.com/p/a47c46153326</a></li>\n</ol>\n"},{"title":"distributed_tf","date":"2017-12-14T08:20:18.000Z","_content":"\n#### basic concepts\n* cluster: a set of tasks, one or more jobs\n* server: master+worker\n","source":"_posts/distributed-tf.md","raw":"---\ntitle: distributed_tf\ndate: 2017-12-14 16:20:18\ntags:\n---\n\n#### basic concepts\n* cluster: a set of tasks, one or more jobs\n* server: master+worker\n","slug":"distributed-tf","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrz4000bsi9goat4n804","content":"<h4 id=\"basic-concepts\"><a href=\"#basic-concepts\" class=\"headerlink\" title=\"basic concepts\"></a>basic concepts</h4><ul>\n<li>cluster: a set of tasks, one or more jobs</li>\n<li>server: master+worker</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"basic-concepts\"><a href=\"#basic-concepts\" class=\"headerlink\" title=\"basic concepts\"></a>basic concepts</h4><ul>\n<li>cluster: a set of tasks, one or more jobs</li>\n<li>server: master+worker</li>\n</ul>\n"},{"title":"dt","date":"2017-12-13T13:45:15.000Z","_content":"\n\n### entropy, information gain, gain ratio\n\n","source":"_posts/dt.md","raw":"---\ntitle: dt\ndate: 2017-12-13 21:45:15\ntags:\n---\n\n\n### entropy, information gain, gain ratio\n\n","slug":"dt","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrz5000csi9ggremh0rm","content":"<h3 id=\"entropy-information-gain-gain-ratio\"><a href=\"#entropy-information-gain-gain-ratio\" class=\"headerlink\" title=\"entropy, information gain, gain ratio\"></a>entropy, information gain, gain ratio</h3>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"entropy-information-gain-gain-ratio\"><a href=\"#entropy-information-gain-gain-ratio\" class=\"headerlink\" title=\"entropy, information gain, gain ratio\"></a>entropy, information gain, gain ratio</h3>"},{"title":"edge-rank","date":"2018-01-15T12:37:48.000Z","_content":"\n### ","source":"_posts/edge-rank.md","raw":"---\ntitle: edge-rank\ndate: 2018-01-15 20:37:48\ntags:\n---\n\n### ","slug":"edge-rank","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrz8000esi9gke8jbnk3","content":"<p>### </p>\n","site":{"data":{}},"excerpt":"","more":"<p>### </p>\n"},{"title":"ee-n-dqn","date":"2017-10-24T15:59:18.000Z","_content":"\n### 先有鸡还是先有蛋？\n\n#### 数据闭环\n推荐系统根据用户日志来进行建模推荐，即：\n日志 -> 推荐算法 -> 用户\n\n<!-- more -->\n\n日志也是由用户产生的，即：\n用户 -> 日志\n\n两者拼成一个环状，我们称之为\"数据闭环\"，即：\n{% asset_img \"1.png\" [1.png] %}\n\n#### \"数据闭环\"和\"越推越窄\"\n这是一个\"先有鸡还是先有蛋？\"的问题\n> 问：为什么给A推荐\"摇滚\"歌曲？\n> 答：因为A过去听的都是\"摇滚\"歌曲，所以A喜欢\"摇滚\"。\n> 问：推荐系统不给A用户推\"非摇滚\"，用户怎么能听到\"非摇滚\"？\n\n在数据闭环中流转的都是\"老Item\"，新\"Item\"并没有多少展现机会，推荐变得越来越窄\n\n#### \"越推越窄\"解决方案\n越推越窄是典型的EE问题(explore & exploit)\n解决方案有两类：\n1. Bandit: epsilon-greedy, thompson sampling, UCB, linUCB\n2. RL\n\n#### Bandit的方案\nbandit方案可以参考 http://banditalgs.com/ ，此处不做详细解释, 常见有以下方法：\n* epsilon-greedy\n* Thompson Sampling\n* UCB\n* linUCB\n\n### RL的方案\nRL解决了ML解决不了的两大问题：\n* 延迟reward问题\n* 数据缺失问题（EE问题，先有鸡先有单\nRL有两大实体：\n* agent\n\t* agent可以从environment中得到reward\n\t* agent需要知道自己的state, agent可以选择自己的action，即是一个p(action|state)的求解过程\n* environment\n\t* environment需提供一个reward函数（往往自定义设计）\n\t* environment需进行state的状态转移（往往是黑盒子）\n\t* environment需接收agent的action\n\n两大实体互相作用，有几大重要的元素:\n* action: 动作，由agent产生，作用于environment\n* reward: 奖赏，environment针对agent的state+action产生的奖赏or惩罚\n* state: agent的状态，由action实现状态转移，即p(state_x+1|state_x, action_x)的马尔科夫转移过程\n* observation: 即state的外在表现\n\n用图可视化即\n{% asset_img \"2.png\" [2.png] %}\n\n### 两种observation\nobservation是state的外在表现，那么observation也有两种：\n1. state space: 直接表达state的空间\n\t比如cartpole中的observation(state)的定义是[position of cart, velocity of cart, angle of pole, rotation rate of pole]\n\t有意思的是，并不需要（往往也不知道）其具体的含义，只知道是一个四维数组\n2. pixels: \n\t直接从像素级别（声音，嗅觉，味觉，触觉）等得到observation\n\t有意思的是，某时刻的图片不一定能够表达全部信息（比如速度），因此可能用图片串表示observation\n\tp(action_t|pixel_t, pixel_t-1, pixel_t-2, ..., pixel_1)\n\t\n### RL\nreinforcement learning有两个比较通用的算法\n* Q learning \n* policy gradients\n\n### Q-learning\nQ-learning的核心是计算Q值，那么Q值的定义是：\nQ value =  what our return would be, if we were to take an action in a given state\n即Q是一个两维空间[observation, action]，表示在某个observation时执行某个action的总的reward和（立即的reward和之后的reward的discount）\n\n#### Q值 -> action \n假设已经有了Q值，那么如何sample出一个action，可以简单用目前observation下的最大的Q，顺便加一些随机性来探索。\n\n#### Q值更新\nQ值的更新需要用到Bellman equation，即：\nQ(s,a) = r + γ(max(Q(s’,a’))\n其中,\ns表示state，也即observation\na表示action\nr表示current reward\ns’表示next state，即state下做出action之后到达的new state\na’表示next state后的策略，max(Q(s’,a’)表示s’后的最佳策略的Q值\nγ表示future reward的一个discount\n\n有意思的是，我们用差分，设置步长，确定方向，来逼近这个值：\n```\nQ[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n```\n\nOpenAI的FrozenLake-v0完整的code如下：\n```\nimport gym\nimport numpy as np\nenv = gym.make('FrozenLake-v0')\n#Initialize table with all zeros\nQ = np.zeros([env.observation_space.n,env.action_space.n])\n# Set learning parameters\nlr = .8\ny = .95\nnum_episodes = 2000\n#create lists to contain total rewards and steps per episode\n#jList = []\nrList = []\nfor i in range(num_episodes):\n    #Reset environment and get first new observation\n    s = env.reset()\n    rAll = 0\n    d = False\n    j = 0\n    #The Q-Table learning algorithm\n    while j < 99:\n        j+=1\n        #Choose an action by greedily (with noise) picking from Q table\n        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n        #Get new state and reward from environment\n        s1,r,d,_ = env.step(a)\n        #Update Q-Table with new knowledge\n        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n        rAll += r\n        s = s1\n        if d == True:\n            break\n    #jList.append(j)\n    rList.append(rAll)\n```\n\n\n### DQN(Deep Q Network)\n比如利用CNN来做observation来表达state，即是DQN，后续再更新。\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/ee-n-dqn.md","raw":"---\ntitle: ee-n-dqn\ndate: 2017-10-24 23:59:18\ntags:\n---\n\n### 先有鸡还是先有蛋？\n\n#### 数据闭环\n推荐系统根据用户日志来进行建模推荐，即：\n日志 -> 推荐算法 -> 用户\n\n<!-- more -->\n\n日志也是由用户产生的，即：\n用户 -> 日志\n\n两者拼成一个环状，我们称之为\"数据闭环\"，即：\n{% asset_img \"1.png\" [1.png] %}\n\n#### \"数据闭环\"和\"越推越窄\"\n这是一个\"先有鸡还是先有蛋？\"的问题\n> 问：为什么给A推荐\"摇滚\"歌曲？\n> 答：因为A过去听的都是\"摇滚\"歌曲，所以A喜欢\"摇滚\"。\n> 问：推荐系统不给A用户推\"非摇滚\"，用户怎么能听到\"非摇滚\"？\n\n在数据闭环中流转的都是\"老Item\"，新\"Item\"并没有多少展现机会，推荐变得越来越窄\n\n#### \"越推越窄\"解决方案\n越推越窄是典型的EE问题(explore & exploit)\n解决方案有两类：\n1. Bandit: epsilon-greedy, thompson sampling, UCB, linUCB\n2. RL\n\n#### Bandit的方案\nbandit方案可以参考 http://banditalgs.com/ ，此处不做详细解释, 常见有以下方法：\n* epsilon-greedy\n* Thompson Sampling\n* UCB\n* linUCB\n\n### RL的方案\nRL解决了ML解决不了的两大问题：\n* 延迟reward问题\n* 数据缺失问题（EE问题，先有鸡先有单\nRL有两大实体：\n* agent\n\t* agent可以从environment中得到reward\n\t* agent需要知道自己的state, agent可以选择自己的action，即是一个p(action|state)的求解过程\n* environment\n\t* environment需提供一个reward函数（往往自定义设计）\n\t* environment需进行state的状态转移（往往是黑盒子）\n\t* environment需接收agent的action\n\n两大实体互相作用，有几大重要的元素:\n* action: 动作，由agent产生，作用于environment\n* reward: 奖赏，environment针对agent的state+action产生的奖赏or惩罚\n* state: agent的状态，由action实现状态转移，即p(state_x+1|state_x, action_x)的马尔科夫转移过程\n* observation: 即state的外在表现\n\n用图可视化即\n{% asset_img \"2.png\" [2.png] %}\n\n### 两种observation\nobservation是state的外在表现，那么observation也有两种：\n1. state space: 直接表达state的空间\n\t比如cartpole中的observation(state)的定义是[position of cart, velocity of cart, angle of pole, rotation rate of pole]\n\t有意思的是，并不需要（往往也不知道）其具体的含义，只知道是一个四维数组\n2. pixels: \n\t直接从像素级别（声音，嗅觉，味觉，触觉）等得到observation\n\t有意思的是，某时刻的图片不一定能够表达全部信息（比如速度），因此可能用图片串表示observation\n\tp(action_t|pixel_t, pixel_t-1, pixel_t-2, ..., pixel_1)\n\t\n### RL\nreinforcement learning有两个比较通用的算法\n* Q learning \n* policy gradients\n\n### Q-learning\nQ-learning的核心是计算Q值，那么Q值的定义是：\nQ value =  what our return would be, if we were to take an action in a given state\n即Q是一个两维空间[observation, action]，表示在某个observation时执行某个action的总的reward和（立即的reward和之后的reward的discount）\n\n#### Q值 -> action \n假设已经有了Q值，那么如何sample出一个action，可以简单用目前observation下的最大的Q，顺便加一些随机性来探索。\n\n#### Q值更新\nQ值的更新需要用到Bellman equation，即：\nQ(s,a) = r + γ(max(Q(s’,a’))\n其中,\ns表示state，也即observation\na表示action\nr表示current reward\ns’表示next state，即state下做出action之后到达的new state\na’表示next state后的策略，max(Q(s’,a’)表示s’后的最佳策略的Q值\nγ表示future reward的一个discount\n\n有意思的是，我们用差分，设置步长，确定方向，来逼近这个值：\n```\nQ[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n```\n\nOpenAI的FrozenLake-v0完整的code如下：\n```\nimport gym\nimport numpy as np\nenv = gym.make('FrozenLake-v0')\n#Initialize table with all zeros\nQ = np.zeros([env.observation_space.n,env.action_space.n])\n# Set learning parameters\nlr = .8\ny = .95\nnum_episodes = 2000\n#create lists to contain total rewards and steps per episode\n#jList = []\nrList = []\nfor i in range(num_episodes):\n    #Reset environment and get first new observation\n    s = env.reset()\n    rAll = 0\n    d = False\n    j = 0\n    #The Q-Table learning algorithm\n    while j < 99:\n        j+=1\n        #Choose an action by greedily (with noise) picking from Q table\n        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n        #Get new state and reward from environment\n        s1,r,d,_ = env.step(a)\n        #Update Q-Table with new knowledge\n        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n        rAll += r\n        s = s1\n        if d == True:\n            break\n    #jList.append(j)\n    rList.append(rAll)\n```\n\n\n### DQN(Deep Q Network)\n比如利用CNN来做observation来表达state，即是DQN，后续再更新。\n\n\n\n\n\n\n\n\n\n\n","slug":"ee-n-dqn","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrza000gsi9g7irg4z99","content":"<h3 id=\"先有鸡还是先有蛋？\"><a href=\"#先有鸡还是先有蛋？\" class=\"headerlink\" title=\"先有鸡还是先有蛋？\"></a>先有鸡还是先有蛋？</h3><h4 id=\"数据闭环\"><a href=\"#数据闭环\" class=\"headerlink\" title=\"数据闭环\"></a>数据闭环</h4><p>推荐系统根据用户日志来进行建模推荐，即：<br>日志 -&gt; 推荐算法 -&gt; 用户</p>\n<a id=\"more\"></a>\n<p>日志也是由用户产生的，即：<br>用户 -&gt; 日志</p>\n<p>两者拼成一个环状，我们称之为”数据闭环”，即：<br><img src=\"/2017/10/24/ee-n-dqn/1.png\" alt=\"[1.png]\" title=\"[1.png]\"></p>\n<h4 id=\"“数据闭环”和”越推越窄”\"><a href=\"#“数据闭环”和”越推越窄”\" class=\"headerlink\" title=\"“数据闭环”和”越推越窄”\"></a>“数据闭环”和”越推越窄”</h4><p>这是一个”先有鸡还是先有蛋？”的问题</p>\n<blockquote>\n<p>问：为什么给A推荐”摇滚”歌曲？<br>答：因为A过去听的都是”摇滚”歌曲，所以A喜欢”摇滚”。<br>问：推荐系统不给A用户推”非摇滚”，用户怎么能听到”非摇滚”？</p>\n</blockquote>\n<p>在数据闭环中流转的都是”老Item”，新”Item”并没有多少展现机会，推荐变得越来越窄</p>\n<h4 id=\"“越推越窄”解决方案\"><a href=\"#“越推越窄”解决方案\" class=\"headerlink\" title=\"“越推越窄”解决方案\"></a>“越推越窄”解决方案</h4><p>越推越窄是典型的EE问题(explore &amp; exploit)<br>解决方案有两类：</p>\n<ol>\n<li>Bandit: epsilon-greedy, thompson sampling, UCB, linUCB</li>\n<li>RL</li>\n</ol>\n<h4 id=\"Bandit的方案\"><a href=\"#Bandit的方案\" class=\"headerlink\" title=\"Bandit的方案\"></a>Bandit的方案</h4><p>bandit方案可以参考 <a href=\"http://banditalgs.com/\" target=\"_blank\" rel=\"external\">http://banditalgs.com/</a> ，此处不做详细解释, 常见有以下方法：</p>\n<ul>\n<li>epsilon-greedy</li>\n<li>Thompson Sampling</li>\n<li>UCB</li>\n<li>linUCB</li>\n</ul>\n<h3 id=\"RL的方案\"><a href=\"#RL的方案\" class=\"headerlink\" title=\"RL的方案\"></a>RL的方案</h3><p>RL解决了ML解决不了的两大问题：</p>\n<ul>\n<li>延迟reward问题</li>\n<li>数据缺失问题（EE问题，先有鸡先有单<br>RL有两大实体：</li>\n<li>agent<ul>\n<li>agent可以从environment中得到reward</li>\n<li>agent需要知道自己的state, agent可以选择自己的action，即是一个p(action|state)的求解过程</li>\n</ul>\n</li>\n<li>environment<ul>\n<li>environment需提供一个reward函数（往往自定义设计）</li>\n<li>environment需进行state的状态转移（往往是黑盒子）</li>\n<li>environment需接收agent的action</li>\n</ul>\n</li>\n</ul>\n<p>两大实体互相作用，有几大重要的元素:</p>\n<ul>\n<li>action: 动作，由agent产生，作用于environment</li>\n<li>reward: 奖赏，environment针对agent的state+action产生的奖赏or惩罚</li>\n<li>state: agent的状态，由action实现状态转移，即p(state_x+1|state_x, action_x)的马尔科夫转移过程</li>\n<li>observation: 即state的外在表现</li>\n</ul>\n<p>用图可视化即<br><img src=\"/2017/10/24/ee-n-dqn/2.png\" alt=\"[2.png]\" title=\"[2.png]\"></p>\n<h3 id=\"两种observation\"><a href=\"#两种observation\" class=\"headerlink\" title=\"两种observation\"></a>两种observation</h3><p>observation是state的外在表现，那么observation也有两种：</p>\n<ol>\n<li>state space: 直接表达state的空间<br> 比如cartpole中的observation(state)的定义是[position of cart, velocity of cart, angle of pole, rotation rate of pole]<br> 有意思的是，并不需要（往往也不知道）其具体的含义，只知道是一个四维数组</li>\n<li>pixels:<br> 直接从像素级别（声音，嗅觉，味觉，触觉）等得到observation<br> 有意思的是，某时刻的图片不一定能够表达全部信息（比如速度），因此可能用图片串表示observation<br> p(action_t|pixel_t, pixel_t-1, pixel_t-2, …, pixel_1)</li>\n</ol>\n<h3 id=\"RL\"><a href=\"#RL\" class=\"headerlink\" title=\"RL\"></a>RL</h3><p>reinforcement learning有两个比较通用的算法</p>\n<ul>\n<li>Q learning </li>\n<li>policy gradients</li>\n</ul>\n<h3 id=\"Q-learning\"><a href=\"#Q-learning\" class=\"headerlink\" title=\"Q-learning\"></a>Q-learning</h3><p>Q-learning的核心是计算Q值，那么Q值的定义是：<br>Q value =  what our return would be, if we were to take an action in a given state<br>即Q是一个两维空间[observation, action]，表示在某个observation时执行某个action的总的reward和（立即的reward和之后的reward的discount）</p>\n<h4 id=\"Q值-gt-action\"><a href=\"#Q值-gt-action\" class=\"headerlink\" title=\"Q值 -&gt; action\"></a>Q值 -&gt; action</h4><p>假设已经有了Q值，那么如何sample出一个action，可以简单用目前observation下的最大的Q，顺便加一些随机性来探索。</p>\n<h4 id=\"Q值更新\"><a href=\"#Q值更新\" class=\"headerlink\" title=\"Q值更新\"></a>Q值更新</h4><p>Q值的更新需要用到Bellman equation，即：<br>Q(s,a) = r + γ(max(Q(s’,a’))<br>其中,<br>s表示state，也即observation<br>a表示action<br>r表示current reward<br>s’表示next state，即state下做出action之后到达的new state<br>a’表示next state后的策略，max(Q(s’,a’)表示s’后的最佳策略的Q值<br>γ表示future reward的一个discount</p>\n<p>有意思的是，我们用差分，设置步长，确定方向，来逼近这个值：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])</div></pre></td></tr></table></figure></p>\n<p>OpenAI的FrozenLake-v0完整的code如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">import gym</div><div class=\"line\">import numpy as np</div><div class=\"line\">env = gym.make(&apos;FrozenLake-v0&apos;)</div><div class=\"line\">#Initialize table with all zeros</div><div class=\"line\">Q = np.zeros([env.observation_space.n,env.action_space.n])</div><div class=\"line\"># Set learning parameters</div><div class=\"line\">lr = .8</div><div class=\"line\">y = .95</div><div class=\"line\">num_episodes = 2000</div><div class=\"line\">#create lists to contain total rewards and steps per episode</div><div class=\"line\">#jList = []</div><div class=\"line\">rList = []</div><div class=\"line\">for i in range(num_episodes):</div><div class=\"line\">    #Reset environment and get first new observation</div><div class=\"line\">    s = env.reset()</div><div class=\"line\">    rAll = 0</div><div class=\"line\">    d = False</div><div class=\"line\">    j = 0</div><div class=\"line\">    #The Q-Table learning algorithm</div><div class=\"line\">    while j &lt; 99:</div><div class=\"line\">        j+=1</div><div class=\"line\">        #Choose an action by greedily (with noise) picking from Q table</div><div class=\"line\">        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))</div><div class=\"line\">        #Get new state and reward from environment</div><div class=\"line\">        s1,r,d,_ = env.step(a)</div><div class=\"line\">        #Update Q-Table with new knowledge</div><div class=\"line\">        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])</div><div class=\"line\">        rAll += r</div><div class=\"line\">        s = s1</div><div class=\"line\">        if d == True:</div><div class=\"line\">            break</div><div class=\"line\">    #jList.append(j)</div><div class=\"line\">    rList.append(rAll)</div></pre></td></tr></table></figure></p>\n<h3 id=\"DQN-Deep-Q-Network\"><a href=\"#DQN-Deep-Q-Network\" class=\"headerlink\" title=\"DQN(Deep Q Network)\"></a>DQN(Deep Q Network)</h3><p>比如利用CNN来做observation来表达state，即是DQN，后续再更新。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"先有鸡还是先有蛋？\"><a href=\"#先有鸡还是先有蛋？\" class=\"headerlink\" title=\"先有鸡还是先有蛋？\"></a>先有鸡还是先有蛋？</h3><h4 id=\"数据闭环\"><a href=\"#数据闭环\" class=\"headerlink\" title=\"数据闭环\"></a>数据闭环</h4><p>推荐系统根据用户日志来进行建模推荐，即：<br>日志 -&gt; 推荐算法 -&gt; 用户</p>","more":"<p>日志也是由用户产生的，即：<br>用户 -&gt; 日志</p>\n<p>两者拼成一个环状，我们称之为”数据闭环”，即：<br><img src=\"/2017/10/24/ee-n-dqn/1.png\" alt=\"[1.png]\" title=\"[1.png]\"></p>\n<h4 id=\"“数据闭环”和”越推越窄”\"><a href=\"#“数据闭环”和”越推越窄”\" class=\"headerlink\" title=\"“数据闭环”和”越推越窄”\"></a>“数据闭环”和”越推越窄”</h4><p>这是一个”先有鸡还是先有蛋？”的问题</p>\n<blockquote>\n<p>问：为什么给A推荐”摇滚”歌曲？<br>答：因为A过去听的都是”摇滚”歌曲，所以A喜欢”摇滚”。<br>问：推荐系统不给A用户推”非摇滚”，用户怎么能听到”非摇滚”？</p>\n</blockquote>\n<p>在数据闭环中流转的都是”老Item”，新”Item”并没有多少展现机会，推荐变得越来越窄</p>\n<h4 id=\"“越推越窄”解决方案\"><a href=\"#“越推越窄”解决方案\" class=\"headerlink\" title=\"“越推越窄”解决方案\"></a>“越推越窄”解决方案</h4><p>越推越窄是典型的EE问题(explore &amp; exploit)<br>解决方案有两类：</p>\n<ol>\n<li>Bandit: epsilon-greedy, thompson sampling, UCB, linUCB</li>\n<li>RL</li>\n</ol>\n<h4 id=\"Bandit的方案\"><a href=\"#Bandit的方案\" class=\"headerlink\" title=\"Bandit的方案\"></a>Bandit的方案</h4><p>bandit方案可以参考 <a href=\"http://banditalgs.com/\" target=\"_blank\" rel=\"external\">http://banditalgs.com/</a> ，此处不做详细解释, 常见有以下方法：</p>\n<ul>\n<li>epsilon-greedy</li>\n<li>Thompson Sampling</li>\n<li>UCB</li>\n<li>linUCB</li>\n</ul>\n<h3 id=\"RL的方案\"><a href=\"#RL的方案\" class=\"headerlink\" title=\"RL的方案\"></a>RL的方案</h3><p>RL解决了ML解决不了的两大问题：</p>\n<ul>\n<li>延迟reward问题</li>\n<li>数据缺失问题（EE问题，先有鸡先有单<br>RL有两大实体：</li>\n<li>agent<ul>\n<li>agent可以从environment中得到reward</li>\n<li>agent需要知道自己的state, agent可以选择自己的action，即是一个p(action|state)的求解过程</li>\n</ul>\n</li>\n<li>environment<ul>\n<li>environment需提供一个reward函数（往往自定义设计）</li>\n<li>environment需进行state的状态转移（往往是黑盒子）</li>\n<li>environment需接收agent的action</li>\n</ul>\n</li>\n</ul>\n<p>两大实体互相作用，有几大重要的元素:</p>\n<ul>\n<li>action: 动作，由agent产生，作用于environment</li>\n<li>reward: 奖赏，environment针对agent的state+action产生的奖赏or惩罚</li>\n<li>state: agent的状态，由action实现状态转移，即p(state_x+1|state_x, action_x)的马尔科夫转移过程</li>\n<li>observation: 即state的外在表现</li>\n</ul>\n<p>用图可视化即<br><img src=\"/2017/10/24/ee-n-dqn/2.png\" alt=\"[2.png]\" title=\"[2.png]\"></p>\n<h3 id=\"两种observation\"><a href=\"#两种observation\" class=\"headerlink\" title=\"两种observation\"></a>两种observation</h3><p>observation是state的外在表现，那么observation也有两种：</p>\n<ol>\n<li>state space: 直接表达state的空间<br> 比如cartpole中的observation(state)的定义是[position of cart, velocity of cart, angle of pole, rotation rate of pole]<br> 有意思的是，并不需要（往往也不知道）其具体的含义，只知道是一个四维数组</li>\n<li>pixels:<br> 直接从像素级别（声音，嗅觉，味觉，触觉）等得到observation<br> 有意思的是，某时刻的图片不一定能够表达全部信息（比如速度），因此可能用图片串表示observation<br> p(action_t|pixel_t, pixel_t-1, pixel_t-2, …, pixel_1)</li>\n</ol>\n<h3 id=\"RL\"><a href=\"#RL\" class=\"headerlink\" title=\"RL\"></a>RL</h3><p>reinforcement learning有两个比较通用的算法</p>\n<ul>\n<li>Q learning </li>\n<li>policy gradients</li>\n</ul>\n<h3 id=\"Q-learning\"><a href=\"#Q-learning\" class=\"headerlink\" title=\"Q-learning\"></a>Q-learning</h3><p>Q-learning的核心是计算Q值，那么Q值的定义是：<br>Q value =  what our return would be, if we were to take an action in a given state<br>即Q是一个两维空间[observation, action]，表示在某个observation时执行某个action的总的reward和（立即的reward和之后的reward的discount）</p>\n<h4 id=\"Q值-gt-action\"><a href=\"#Q值-gt-action\" class=\"headerlink\" title=\"Q值 -&gt; action\"></a>Q值 -&gt; action</h4><p>假设已经有了Q值，那么如何sample出一个action，可以简单用目前observation下的最大的Q，顺便加一些随机性来探索。</p>\n<h4 id=\"Q值更新\"><a href=\"#Q值更新\" class=\"headerlink\" title=\"Q值更新\"></a>Q值更新</h4><p>Q值的更新需要用到Bellman equation，即：<br>Q(s,a) = r + γ(max(Q(s’,a’))<br>其中,<br>s表示state，也即observation<br>a表示action<br>r表示current reward<br>s’表示next state，即state下做出action之后到达的new state<br>a’表示next state后的策略，max(Q(s’,a’)表示s’后的最佳策略的Q值<br>γ表示future reward的一个discount</p>\n<p>有意思的是，我们用差分，设置步长，确定方向，来逼近这个值：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])</div></pre></td></tr></table></figure></p>\n<p>OpenAI的FrozenLake-v0完整的code如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">import gym</div><div class=\"line\">import numpy as np</div><div class=\"line\">env = gym.make(&apos;FrozenLake-v0&apos;)</div><div class=\"line\">#Initialize table with all zeros</div><div class=\"line\">Q = np.zeros([env.observation_space.n,env.action_space.n])</div><div class=\"line\"># Set learning parameters</div><div class=\"line\">lr = .8</div><div class=\"line\">y = .95</div><div class=\"line\">num_episodes = 2000</div><div class=\"line\">#create lists to contain total rewards and steps per episode</div><div class=\"line\">#jList = []</div><div class=\"line\">rList = []</div><div class=\"line\">for i in range(num_episodes):</div><div class=\"line\">    #Reset environment and get first new observation</div><div class=\"line\">    s = env.reset()</div><div class=\"line\">    rAll = 0</div><div class=\"line\">    d = False</div><div class=\"line\">    j = 0</div><div class=\"line\">    #The Q-Table learning algorithm</div><div class=\"line\">    while j &lt; 99:</div><div class=\"line\">        j+=1</div><div class=\"line\">        #Choose an action by greedily (with noise) picking from Q table</div><div class=\"line\">        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))</div><div class=\"line\">        #Get new state and reward from environment</div><div class=\"line\">        s1,r,d,_ = env.step(a)</div><div class=\"line\">        #Update Q-Table with new knowledge</div><div class=\"line\">        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])</div><div class=\"line\">        rAll += r</div><div class=\"line\">        s = s1</div><div class=\"line\">        if d == True:</div><div class=\"line\">            break</div><div class=\"line\">    #jList.append(j)</div><div class=\"line\">    rList.append(rAll)</div></pre></td></tr></table></figure></p>\n<h3 id=\"DQN-Deep-Q-Network\"><a href=\"#DQN-Deep-Q-Network\" class=\"headerlink\" title=\"DQN(Deep Q Network)\"></a>DQN(Deep Q Network)</h3><p>比如利用CNN来做observation来表达state，即是DQN，后续再更新。</p>"},{"title":"exploding_n_vanishing","date":"2017-11-23T06:33:26.000Z","_content":"\nOpinions:\n1. This two problems becomes worth as the number of layers in the architecture increases.\n2. \n\n\n\n* vanishing gradients\n假设有一个N层的DNN模型，那么y可以\n\ny = Wn * Wn-1 * Wn-2 *...* W1 * X\n\n\n","source":"_posts/exploding-n-vanishing.md","raw":"---\ntitle: exploding_n_vanishing\ndate: 2017-11-23 14:33:26\ntags: DNN\n---\n\nOpinions:\n1. This two problems becomes worth as the number of layers in the architecture increases.\n2. \n\n\n\n* vanishing gradients\n假设有一个N层的DNN模型，那么y可以\n\ny = Wn * Wn-1 * Wn-2 *...* W1 * X\n\n\n","slug":"exploding-n-vanishing","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrzb000hsi9gc6fmyf5j","content":"<p>Opinions:</p>\n<ol>\n<li>This two problems becomes worth as the number of layers in the architecture increases.</li>\n<li></li>\n</ol>\n<ul>\n<li>vanishing gradients<br>假设有一个N层的DNN模型，那么y可以</li>\n</ul>\n<p>y = Wn <em> Wn-1 </em> Wn-2 <em>…</em> W1 * X</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Opinions:</p>\n<ol>\n<li>This two problems becomes worth as the number of layers in the architecture increases.</li>\n<li></li>\n</ol>\n<ul>\n<li>vanishing gradients<br>假设有一个N层的DNN模型，那么y可以</li>\n</ul>\n<p>y = Wn <em> Wn-1 </em> Wn-2 <em>…</em> W1 * X</p>\n"},{"title":"聊聊特征工程","date":"2017-10-02T09:52:35.000Z","_content":"\n### 特征工程\n\n在机器学习项目中，往往受到关注的是高大上的机器学习模型，特征工程很少有人问津，可能唯一提到的便是浮夸的一句“我们的模型使用了百万级别的特征”。然而特征工程对于线上效果的贡献，往往远远大于模型，所以一个健全的特征工程方法论非常的重要。\n\n<!-- more -->\n\n### 最有效的特征是什么\n\n在pCTR项目中，决定是否点击的最重要的因素，是Item本身和User本身，即ItemID和UserID特征。\n* ItemID: 推荐“王者荣耀”和“全民超神”，大家都会选择“王者荣耀”，因为你的朋友都在这款游戏里，所以个人偏好远远小于物品属性的影响。\n* UserID: 用户需求明确（就是来找一款MOBA手游），点击率自然高；用户就是来逛逛，刷刷页面，那点击率自然低。\n* 其他的特征: 时间地点场景年龄性别星级类型等对模型的影响是次要的。\n\n### ID特征太多怎么办\n如果ID数量太多不便处理，可以简单用统计CTR特征来代替，纯粹ID特征等价于纯粹CTR特征，从理论推导和代码实践上皆可证明。\n\n#### 实验经验\n自己曾经做了一次实验，单CTR特征模型AUC=0.7+，其他所有特征（单单排除CTR特征）模型AUC=0.6+，所有特征一起建模AUC=0.8+。\n\n#### CTR特征的坏处\n但是用CTR特征的坏处是，交叉的时候相对于ID特征，会丢失信息。\n\n### 最好的非个性化模型\n对于某个UserX来说，ItemID特征（CTR特征）起到主导作用，其他特征只是辅助，那么最好的非个性化模型即是CTR排序模型（或只有ItemID的特征的模型）。\n\n### Item个数和提升天花板\n当Item数量越少，Item之间差别越大的时候，个性化的能够提升的空间越小（比如某业务只有40+个特征，个性化模型只能比CTR热门提升6%左右）；当Item数量非常庞大的时候（如淘宝），或者用户偏好非常分散的时候（如书籍，各个年龄性别行业都不同），推荐才有大的发挥空间。\n\n### 连续特征 VS 离散特征\n在工程实践中，有2种类型的特征：连续特征和离散特征。而“百万级别特征”里往往大部分是离散特征，以App推荐为例，有User/Item ID，城市，地区，标签特征，分类特征，厂商等等，经过one-hot之后，数量急剧爆炸；而连续特征有很多是人造统计特征，比如：下载量，访问量，ltv，arpu，实时ctr等等，成本高，数量少。\n\n### 特征工程\n\n#### 人工特征工程\n##### 特征提取\n特征的提取，很大程度上是人的工作（除去一些端到端的NN方案），初期依照业务知识，自行YY出一些特征出来。以APP推荐为例，CTR特征保证高转化，下载量特征保证热门，星级特征保证质量，用户安装使用/APP类别特征保证个性化。\n从划分来看，特征可以有以下来源：\n1. 基础属性：不随时间变化的属性。如User的性别，年龄，职业，住址等；Item的自身属性（如APP的星级，公司，包大小等）\n2. 统计属性：简单统计可以得到的特征。如User的下载量，点击量，CTR值等；Item的曝光，点击，下载，ARPU，LTV，留存等。\n3. 标签转移属性：标签转移是建设画像的一种重要思路。APP画像转移到用户画像上的有：点击的类型分布，下载的类型分布等；用户画像转移到APP画像上的有：男女使用分布，性别安装分布，地域点击率分布等。\n4. 场景属性：事情发生的时间，地点，场景等，如：APP的某个页面ID，猜你喜欢的第X位等。\n5. 设备属性：手机的好坏。ROM，RAM大小等非常影响用户的游戏下载属性。\n6. 迁移属性：画像的特点就是知识迁移方便。广告业务的特征用到APP业务上，WiFi的特征用到流量业务上，非常的常见。\n7. （人工）交叉特征：比如User的三级分类画像和APP的三级分类画像，每一个相对应的特征，交叉一遍，得到的人工交叉特征。\n8. 实时特征：讲上述的特征，尤其是统计特征，实时化。获取当前热点信息。\n\n##### 特征选择（特征重要性）\n特征选择有非常多的方法，一个常见的错误是将LR的权重作为特征选择的依据。因为LR中每个Feature的量纲是不同的（比如年龄1-100，温度是-10-40，下载量是几十万），所以特征值大权重小，特征值小权重大。所以LR的权重只有参考意义，不能盲目信任。\n个人列举一些常用的选择的方法：\n1. 单特征AUC（最常用）\n2. 单特征gini index（信息增益，信息增益率）\n3. 相关系数，卡方检验\n4. L1模型自动选择\n5. RF/GBDT打印Feature Importance\n6. wrapper：1-n逐个增加特征，有用就加，无用就抛弃（同事用过，个人经验不足）\n\n##### 特征归一化\n即Z-score，minmax，log变换等，在这里不再赘述。\n需要了解的是：归一化本身并不增加模型精读，只是将特征统一量纲，加速训练。\n\n##### 特征分段\n1. 等宽：1-10,11-20,21-30等距离分。\n2. 等频：先rank，top1-100,top101-200,top201-300等频率分。\n3. 人工：0-17未成年，18-25青年，25-35工作，35-45中年，45以上...\n4. Label决定：如先分桶，通过gini index求最佳分隔点；如使用如下CTR图\n\n{% asset_img \"年龄画段.png\" [年龄画段] %}\n\n##### 特征组合\n1. one-hot特征交叉：01交叉得0, 11交叉得1\n2. one-real特征交叉：0-real交叉得0, 1-real交叉得real\n3. 强强联合：两个强特征进行交叉\n\n#### 自动化特征工程\n上述人工特征工程实在是费心费力，所以建议不使用人工特征工程，全部使用”最原始“特征交给模型来做。首先将特征分成”连续特征“和”离散特征“两种，然后将特征扔进GBDT，GBDT自动进行：\n1. 特征选择：不好的特征，根本进不去树里面。\n2. 特征分段：树的split的分支，即是分段方案。\n3. 特征组合：叶子节点路径，即使特征组合。\n强烈推荐。\n\n### 0 or missing?\n最后讨论一个小问题，libsvm中被稀疏掉的特征，表示0还是表示missing？\n答案是0，libsvm中默认没有missing。\n但是xgboost中对libsvm的处理，是按照missing来处理的，将0和missing分开的方法是：\n1. 连续特征：增加隐控制变量表达是否missing，另一个变量表示值。\n2. 离散特征：将missing枚举为一个离散值。\n","source":"_posts/feature-engineer.md","raw":"---\ntitle: 聊聊特征工程\ndate: 2017-10-02 17:52:35\ntags:\ncategories: 机器学习\n---\n\n### 特征工程\n\n在机器学习项目中，往往受到关注的是高大上的机器学习模型，特征工程很少有人问津，可能唯一提到的便是浮夸的一句“我们的模型使用了百万级别的特征”。然而特征工程对于线上效果的贡献，往往远远大于模型，所以一个健全的特征工程方法论非常的重要。\n\n<!-- more -->\n\n### 最有效的特征是什么\n\n在pCTR项目中，决定是否点击的最重要的因素，是Item本身和User本身，即ItemID和UserID特征。\n* ItemID: 推荐“王者荣耀”和“全民超神”，大家都会选择“王者荣耀”，因为你的朋友都在这款游戏里，所以个人偏好远远小于物品属性的影响。\n* UserID: 用户需求明确（就是来找一款MOBA手游），点击率自然高；用户就是来逛逛，刷刷页面，那点击率自然低。\n* 其他的特征: 时间地点场景年龄性别星级类型等对模型的影响是次要的。\n\n### ID特征太多怎么办\n如果ID数量太多不便处理，可以简单用统计CTR特征来代替，纯粹ID特征等价于纯粹CTR特征，从理论推导和代码实践上皆可证明。\n\n#### 实验经验\n自己曾经做了一次实验，单CTR特征模型AUC=0.7+，其他所有特征（单单排除CTR特征）模型AUC=0.6+，所有特征一起建模AUC=0.8+。\n\n#### CTR特征的坏处\n但是用CTR特征的坏处是，交叉的时候相对于ID特征，会丢失信息。\n\n### 最好的非个性化模型\n对于某个UserX来说，ItemID特征（CTR特征）起到主导作用，其他特征只是辅助，那么最好的非个性化模型即是CTR排序模型（或只有ItemID的特征的模型）。\n\n### Item个数和提升天花板\n当Item数量越少，Item之间差别越大的时候，个性化的能够提升的空间越小（比如某业务只有40+个特征，个性化模型只能比CTR热门提升6%左右）；当Item数量非常庞大的时候（如淘宝），或者用户偏好非常分散的时候（如书籍，各个年龄性别行业都不同），推荐才有大的发挥空间。\n\n### 连续特征 VS 离散特征\n在工程实践中，有2种类型的特征：连续特征和离散特征。而“百万级别特征”里往往大部分是离散特征，以App推荐为例，有User/Item ID，城市，地区，标签特征，分类特征，厂商等等，经过one-hot之后，数量急剧爆炸；而连续特征有很多是人造统计特征，比如：下载量，访问量，ltv，arpu，实时ctr等等，成本高，数量少。\n\n### 特征工程\n\n#### 人工特征工程\n##### 特征提取\n特征的提取，很大程度上是人的工作（除去一些端到端的NN方案），初期依照业务知识，自行YY出一些特征出来。以APP推荐为例，CTR特征保证高转化，下载量特征保证热门，星级特征保证质量，用户安装使用/APP类别特征保证个性化。\n从划分来看，特征可以有以下来源：\n1. 基础属性：不随时间变化的属性。如User的性别，年龄，职业，住址等；Item的自身属性（如APP的星级，公司，包大小等）\n2. 统计属性：简单统计可以得到的特征。如User的下载量，点击量，CTR值等；Item的曝光，点击，下载，ARPU，LTV，留存等。\n3. 标签转移属性：标签转移是建设画像的一种重要思路。APP画像转移到用户画像上的有：点击的类型分布，下载的类型分布等；用户画像转移到APP画像上的有：男女使用分布，性别安装分布，地域点击率分布等。\n4. 场景属性：事情发生的时间，地点，场景等，如：APP的某个页面ID，猜你喜欢的第X位等。\n5. 设备属性：手机的好坏。ROM，RAM大小等非常影响用户的游戏下载属性。\n6. 迁移属性：画像的特点就是知识迁移方便。广告业务的特征用到APP业务上，WiFi的特征用到流量业务上，非常的常见。\n7. （人工）交叉特征：比如User的三级分类画像和APP的三级分类画像，每一个相对应的特征，交叉一遍，得到的人工交叉特征。\n8. 实时特征：讲上述的特征，尤其是统计特征，实时化。获取当前热点信息。\n\n##### 特征选择（特征重要性）\n特征选择有非常多的方法，一个常见的错误是将LR的权重作为特征选择的依据。因为LR中每个Feature的量纲是不同的（比如年龄1-100，温度是-10-40，下载量是几十万），所以特征值大权重小，特征值小权重大。所以LR的权重只有参考意义，不能盲目信任。\n个人列举一些常用的选择的方法：\n1. 单特征AUC（最常用）\n2. 单特征gini index（信息增益，信息增益率）\n3. 相关系数，卡方检验\n4. L1模型自动选择\n5. RF/GBDT打印Feature Importance\n6. wrapper：1-n逐个增加特征，有用就加，无用就抛弃（同事用过，个人经验不足）\n\n##### 特征归一化\n即Z-score，minmax，log变换等，在这里不再赘述。\n需要了解的是：归一化本身并不增加模型精读，只是将特征统一量纲，加速训练。\n\n##### 特征分段\n1. 等宽：1-10,11-20,21-30等距离分。\n2. 等频：先rank，top1-100,top101-200,top201-300等频率分。\n3. 人工：0-17未成年，18-25青年，25-35工作，35-45中年，45以上...\n4. Label决定：如先分桶，通过gini index求最佳分隔点；如使用如下CTR图\n\n{% asset_img \"年龄画段.png\" [年龄画段] %}\n\n##### 特征组合\n1. one-hot特征交叉：01交叉得0, 11交叉得1\n2. one-real特征交叉：0-real交叉得0, 1-real交叉得real\n3. 强强联合：两个强特征进行交叉\n\n#### 自动化特征工程\n上述人工特征工程实在是费心费力，所以建议不使用人工特征工程，全部使用”最原始“特征交给模型来做。首先将特征分成”连续特征“和”离散特征“两种，然后将特征扔进GBDT，GBDT自动进行：\n1. 特征选择：不好的特征，根本进不去树里面。\n2. 特征分段：树的split的分支，即是分段方案。\n3. 特征组合：叶子节点路径，即使特征组合。\n强烈推荐。\n\n### 0 or missing?\n最后讨论一个小问题，libsvm中被稀疏掉的特征，表示0还是表示missing？\n答案是0，libsvm中默认没有missing。\n但是xgboost中对libsvm的处理，是按照missing来处理的，将0和missing分开的方法是：\n1. 连续特征：增加隐控制变量表达是否missing，另一个变量表示值。\n2. 离散特征：将missing枚举为一个离散值。\n","slug":"feature-engineer","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrzc000isi9g8e3m8jgr","content":"<h3 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><p>在机器学习项目中，往往受到关注的是高大上的机器学习模型，特征工程很少有人问津，可能唯一提到的便是浮夸的一句“我们的模型使用了百万级别的特征”。然而特征工程对于线上效果的贡献，往往远远大于模型，所以一个健全的特征工程方法论非常的重要。</p>\n<a id=\"more\"></a>\n<h3 id=\"最有效的特征是什么\"><a href=\"#最有效的特征是什么\" class=\"headerlink\" title=\"最有效的特征是什么\"></a>最有效的特征是什么</h3><p>在pCTR项目中，决定是否点击的最重要的因素，是Item本身和User本身，即ItemID和UserID特征。</p>\n<ul>\n<li>ItemID: 推荐“王者荣耀”和“全民超神”，大家都会选择“王者荣耀”，因为你的朋友都在这款游戏里，所以个人偏好远远小于物品属性的影响。</li>\n<li>UserID: 用户需求明确（就是来找一款MOBA手游），点击率自然高；用户就是来逛逛，刷刷页面，那点击率自然低。</li>\n<li>其他的特征: 时间地点场景年龄性别星级类型等对模型的影响是次要的。</li>\n</ul>\n<h3 id=\"ID特征太多怎么办\"><a href=\"#ID特征太多怎么办\" class=\"headerlink\" title=\"ID特征太多怎么办\"></a>ID特征太多怎么办</h3><p>如果ID数量太多不便处理，可以简单用统计CTR特征来代替，纯粹ID特征等价于纯粹CTR特征，从理论推导和代码实践上皆可证明。</p>\n<h4 id=\"实验经验\"><a href=\"#实验经验\" class=\"headerlink\" title=\"实验经验\"></a>实验经验</h4><p>自己曾经做了一次实验，单CTR特征模型AUC=0.7+，其他所有特征（单单排除CTR特征）模型AUC=0.6+，所有特征一起建模AUC=0.8+。</p>\n<h4 id=\"CTR特征的坏处\"><a href=\"#CTR特征的坏处\" class=\"headerlink\" title=\"CTR特征的坏处\"></a>CTR特征的坏处</h4><p>但是用CTR特征的坏处是，交叉的时候相对于ID特征，会丢失信息。</p>\n<h3 id=\"最好的非个性化模型\"><a href=\"#最好的非个性化模型\" class=\"headerlink\" title=\"最好的非个性化模型\"></a>最好的非个性化模型</h3><p>对于某个UserX来说，ItemID特征（CTR特征）起到主导作用，其他特征只是辅助，那么最好的非个性化模型即是CTR排序模型（或只有ItemID的特征的模型）。</p>\n<h3 id=\"Item个数和提升天花板\"><a href=\"#Item个数和提升天花板\" class=\"headerlink\" title=\"Item个数和提升天花板\"></a>Item个数和提升天花板</h3><p>当Item数量越少，Item之间差别越大的时候，个性化的能够提升的空间越小（比如某业务只有40+个特征，个性化模型只能比CTR热门提升6%左右）；当Item数量非常庞大的时候（如淘宝），或者用户偏好非常分散的时候（如书籍，各个年龄性别行业都不同），推荐才有大的发挥空间。</p>\n<h3 id=\"连续特征-VS-离散特征\"><a href=\"#连续特征-VS-离散特征\" class=\"headerlink\" title=\"连续特征 VS 离散特征\"></a>连续特征 VS 离散特征</h3><p>在工程实践中，有2种类型的特征：连续特征和离散特征。而“百万级别特征”里往往大部分是离散特征，以App推荐为例，有User/Item ID，城市，地区，标签特征，分类特征，厂商等等，经过one-hot之后，数量急剧爆炸；而连续特征有很多是人造统计特征，比如：下载量，访问量，ltv，arpu，实时ctr等等，成本高，数量少。</p>\n<h3 id=\"特征工程-1\"><a href=\"#特征工程-1\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><h4 id=\"人工特征工程\"><a href=\"#人工特征工程\" class=\"headerlink\" title=\"人工特征工程\"></a>人工特征工程</h4><h5 id=\"特征提取\"><a href=\"#特征提取\" class=\"headerlink\" title=\"特征提取\"></a>特征提取</h5><p>特征的提取，很大程度上是人的工作（除去一些端到端的NN方案），初期依照业务知识，自行YY出一些特征出来。以APP推荐为例，CTR特征保证高转化，下载量特征保证热门，星级特征保证质量，用户安装使用/APP类别特征保证个性化。<br>从划分来看，特征可以有以下来源：</p>\n<ol>\n<li>基础属性：不随时间变化的属性。如User的性别，年龄，职业，住址等；Item的自身属性（如APP的星级，公司，包大小等）</li>\n<li>统计属性：简单统计可以得到的特征。如User的下载量，点击量，CTR值等；Item的曝光，点击，下载，ARPU，LTV，留存等。</li>\n<li>标签转移属性：标签转移是建设画像的一种重要思路。APP画像转移到用户画像上的有：点击的类型分布，下载的类型分布等；用户画像转移到APP画像上的有：男女使用分布，性别安装分布，地域点击率分布等。</li>\n<li>场景属性：事情发生的时间，地点，场景等，如：APP的某个页面ID，猜你喜欢的第X位等。</li>\n<li>设备属性：手机的好坏。ROM，RAM大小等非常影响用户的游戏下载属性。</li>\n<li>迁移属性：画像的特点就是知识迁移方便。广告业务的特征用到APP业务上，WiFi的特征用到流量业务上，非常的常见。</li>\n<li>（人工）交叉特征：比如User的三级分类画像和APP的三级分类画像，每一个相对应的特征，交叉一遍，得到的人工交叉特征。</li>\n<li>实时特征：讲上述的特征，尤其是统计特征，实时化。获取当前热点信息。</li>\n</ol>\n<h5 id=\"特征选择（特征重要性）\"><a href=\"#特征选择（特征重要性）\" class=\"headerlink\" title=\"特征选择（特征重要性）\"></a>特征选择（特征重要性）</h5><p>特征选择有非常多的方法，一个常见的错误是将LR的权重作为特征选择的依据。因为LR中每个Feature的量纲是不同的（比如年龄1-100，温度是-10-40，下载量是几十万），所以特征值大权重小，特征值小权重大。所以LR的权重只有参考意义，不能盲目信任。<br>个人列举一些常用的选择的方法：</p>\n<ol>\n<li>单特征AUC（最常用）</li>\n<li>单特征gini index（信息增益，信息增益率）</li>\n<li>相关系数，卡方检验</li>\n<li>L1模型自动选择</li>\n<li>RF/GBDT打印Feature Importance</li>\n<li>wrapper：1-n逐个增加特征，有用就加，无用就抛弃（同事用过，个人经验不足）</li>\n</ol>\n<h5 id=\"特征归一化\"><a href=\"#特征归一化\" class=\"headerlink\" title=\"特征归一化\"></a>特征归一化</h5><p>即Z-score，minmax，log变换等，在这里不再赘述。<br>需要了解的是：归一化本身并不增加模型精读，只是将特征统一量纲，加速训练。</p>\n<h5 id=\"特征分段\"><a href=\"#特征分段\" class=\"headerlink\" title=\"特征分段\"></a>特征分段</h5><ol>\n<li>等宽：1-10,11-20,21-30等距离分。</li>\n<li>等频：先rank，top1-100,top101-200,top201-300等频率分。</li>\n<li>人工：0-17未成年，18-25青年，25-35工作，35-45中年，45以上…</li>\n<li>Label决定：如先分桶，通过gini index求最佳分隔点；如使用如下CTR图</li>\n</ol>\n\n<h5 id=\"特征组合\"><a href=\"#特征组合\" class=\"headerlink\" title=\"特征组合\"></a>特征组合</h5><ol>\n<li>one-hot特征交叉：01交叉得0, 11交叉得1</li>\n<li>one-real特征交叉：0-real交叉得0, 1-real交叉得real</li>\n<li>强强联合：两个强特征进行交叉</li>\n</ol>\n<h4 id=\"自动化特征工程\"><a href=\"#自动化特征工程\" class=\"headerlink\" title=\"自动化特征工程\"></a>自动化特征工程</h4><p>上述人工特征工程实在是费心费力，所以建议不使用人工特征工程，全部使用”最原始“特征交给模型来做。首先将特征分成”连续特征“和”离散特征“两种，然后将特征扔进GBDT，GBDT自动进行：</p>\n<ol>\n<li>特征选择：不好的特征，根本进不去树里面。</li>\n<li>特征分段：树的split的分支，即是分段方案。</li>\n<li>特征组合：叶子节点路径，即使特征组合。<br>强烈推荐。</li>\n</ol>\n<h3 id=\"0-or-missing\"><a href=\"#0-or-missing\" class=\"headerlink\" title=\"0 or missing?\"></a>0 or missing?</h3><p>最后讨论一个小问题，libsvm中被稀疏掉的特征，表示0还是表示missing？<br>答案是0，libsvm中默认没有missing。<br>但是xgboost中对libsvm的处理，是按照missing来处理的，将0和missing分开的方法是：</p>\n<ol>\n<li>连续特征：增加隐控制变量表达是否missing，另一个变量表示值。</li>\n<li>离散特征：将missing枚举为一个离散值。</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h3 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><p>在机器学习项目中，往往受到关注的是高大上的机器学习模型，特征工程很少有人问津，可能唯一提到的便是浮夸的一句“我们的模型使用了百万级别的特征”。然而特征工程对于线上效果的贡献，往往远远大于模型，所以一个健全的特征工程方法论非常的重要。</p>","more":"<h3 id=\"最有效的特征是什么\"><a href=\"#最有效的特征是什么\" class=\"headerlink\" title=\"最有效的特征是什么\"></a>最有效的特征是什么</h3><p>在pCTR项目中，决定是否点击的最重要的因素，是Item本身和User本身，即ItemID和UserID特征。</p>\n<ul>\n<li>ItemID: 推荐“王者荣耀”和“全民超神”，大家都会选择“王者荣耀”，因为你的朋友都在这款游戏里，所以个人偏好远远小于物品属性的影响。</li>\n<li>UserID: 用户需求明确（就是来找一款MOBA手游），点击率自然高；用户就是来逛逛，刷刷页面，那点击率自然低。</li>\n<li>其他的特征: 时间地点场景年龄性别星级类型等对模型的影响是次要的。</li>\n</ul>\n<h3 id=\"ID特征太多怎么办\"><a href=\"#ID特征太多怎么办\" class=\"headerlink\" title=\"ID特征太多怎么办\"></a>ID特征太多怎么办</h3><p>如果ID数量太多不便处理，可以简单用统计CTR特征来代替，纯粹ID特征等价于纯粹CTR特征，从理论推导和代码实践上皆可证明。</p>\n<h4 id=\"实验经验\"><a href=\"#实验经验\" class=\"headerlink\" title=\"实验经验\"></a>实验经验</h4><p>自己曾经做了一次实验，单CTR特征模型AUC=0.7+，其他所有特征（单单排除CTR特征）模型AUC=0.6+，所有特征一起建模AUC=0.8+。</p>\n<h4 id=\"CTR特征的坏处\"><a href=\"#CTR特征的坏处\" class=\"headerlink\" title=\"CTR特征的坏处\"></a>CTR特征的坏处</h4><p>但是用CTR特征的坏处是，交叉的时候相对于ID特征，会丢失信息。</p>\n<h3 id=\"最好的非个性化模型\"><a href=\"#最好的非个性化模型\" class=\"headerlink\" title=\"最好的非个性化模型\"></a>最好的非个性化模型</h3><p>对于某个UserX来说，ItemID特征（CTR特征）起到主导作用，其他特征只是辅助，那么最好的非个性化模型即是CTR排序模型（或只有ItemID的特征的模型）。</p>\n<h3 id=\"Item个数和提升天花板\"><a href=\"#Item个数和提升天花板\" class=\"headerlink\" title=\"Item个数和提升天花板\"></a>Item个数和提升天花板</h3><p>当Item数量越少，Item之间差别越大的时候，个性化的能够提升的空间越小（比如某业务只有40+个特征，个性化模型只能比CTR热门提升6%左右）；当Item数量非常庞大的时候（如淘宝），或者用户偏好非常分散的时候（如书籍，各个年龄性别行业都不同），推荐才有大的发挥空间。</p>\n<h3 id=\"连续特征-VS-离散特征\"><a href=\"#连续特征-VS-离散特征\" class=\"headerlink\" title=\"连续特征 VS 离散特征\"></a>连续特征 VS 离散特征</h3><p>在工程实践中，有2种类型的特征：连续特征和离散特征。而“百万级别特征”里往往大部分是离散特征，以App推荐为例，有User/Item ID，城市，地区，标签特征，分类特征，厂商等等，经过one-hot之后，数量急剧爆炸；而连续特征有很多是人造统计特征，比如：下载量，访问量，ltv，arpu，实时ctr等等，成本高，数量少。</p>\n<h3 id=\"特征工程-1\"><a href=\"#特征工程-1\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><h4 id=\"人工特征工程\"><a href=\"#人工特征工程\" class=\"headerlink\" title=\"人工特征工程\"></a>人工特征工程</h4><h5 id=\"特征提取\"><a href=\"#特征提取\" class=\"headerlink\" title=\"特征提取\"></a>特征提取</h5><p>特征的提取，很大程度上是人的工作（除去一些端到端的NN方案），初期依照业务知识，自行YY出一些特征出来。以APP推荐为例，CTR特征保证高转化，下载量特征保证热门，星级特征保证质量，用户安装使用/APP类别特征保证个性化。<br>从划分来看，特征可以有以下来源：</p>\n<ol>\n<li>基础属性：不随时间变化的属性。如User的性别，年龄，职业，住址等；Item的自身属性（如APP的星级，公司，包大小等）</li>\n<li>统计属性：简单统计可以得到的特征。如User的下载量，点击量，CTR值等；Item的曝光，点击，下载，ARPU，LTV，留存等。</li>\n<li>标签转移属性：标签转移是建设画像的一种重要思路。APP画像转移到用户画像上的有：点击的类型分布，下载的类型分布等；用户画像转移到APP画像上的有：男女使用分布，性别安装分布，地域点击率分布等。</li>\n<li>场景属性：事情发生的时间，地点，场景等，如：APP的某个页面ID，猜你喜欢的第X位等。</li>\n<li>设备属性：手机的好坏。ROM，RAM大小等非常影响用户的游戏下载属性。</li>\n<li>迁移属性：画像的特点就是知识迁移方便。广告业务的特征用到APP业务上，WiFi的特征用到流量业务上，非常的常见。</li>\n<li>（人工）交叉特征：比如User的三级分类画像和APP的三级分类画像，每一个相对应的特征，交叉一遍，得到的人工交叉特征。</li>\n<li>实时特征：讲上述的特征，尤其是统计特征，实时化。获取当前热点信息。</li>\n</ol>\n<h5 id=\"特征选择（特征重要性）\"><a href=\"#特征选择（特征重要性）\" class=\"headerlink\" title=\"特征选择（特征重要性）\"></a>特征选择（特征重要性）</h5><p>特征选择有非常多的方法，一个常见的错误是将LR的权重作为特征选择的依据。因为LR中每个Feature的量纲是不同的（比如年龄1-100，温度是-10-40，下载量是几十万），所以特征值大权重小，特征值小权重大。所以LR的权重只有参考意义，不能盲目信任。<br>个人列举一些常用的选择的方法：</p>\n<ol>\n<li>单特征AUC（最常用）</li>\n<li>单特征gini index（信息增益，信息增益率）</li>\n<li>相关系数，卡方检验</li>\n<li>L1模型自动选择</li>\n<li>RF/GBDT打印Feature Importance</li>\n<li>wrapper：1-n逐个增加特征，有用就加，无用就抛弃（同事用过，个人经验不足）</li>\n</ol>\n<h5 id=\"特征归一化\"><a href=\"#特征归一化\" class=\"headerlink\" title=\"特征归一化\"></a>特征归一化</h5><p>即Z-score，minmax，log变换等，在这里不再赘述。<br>需要了解的是：归一化本身并不增加模型精读，只是将特征统一量纲，加速训练。</p>\n<h5 id=\"特征分段\"><a href=\"#特征分段\" class=\"headerlink\" title=\"特征分段\"></a>特征分段</h5><ol>\n<li>等宽：1-10,11-20,21-30等距离分。</li>\n<li>等频：先rank，top1-100,top101-200,top201-300等频率分。</li>\n<li>人工：0-17未成年，18-25青年，25-35工作，35-45中年，45以上…</li>\n<li>Label决定：如先分桶，通过gini index求最佳分隔点；如使用如下CTR图</li>\n</ol>\n\n<h5 id=\"特征组合\"><a href=\"#特征组合\" class=\"headerlink\" title=\"特征组合\"></a>特征组合</h5><ol>\n<li>one-hot特征交叉：01交叉得0, 11交叉得1</li>\n<li>one-real特征交叉：0-real交叉得0, 1-real交叉得real</li>\n<li>强强联合：两个强特征进行交叉</li>\n</ol>\n<h4 id=\"自动化特征工程\"><a href=\"#自动化特征工程\" class=\"headerlink\" title=\"自动化特征工程\"></a>自动化特征工程</h4><p>上述人工特征工程实在是费心费力，所以建议不使用人工特征工程，全部使用”最原始“特征交给模型来做。首先将特征分成”连续特征“和”离散特征“两种，然后将特征扔进GBDT，GBDT自动进行：</p>\n<ol>\n<li>特征选择：不好的特征，根本进不去树里面。</li>\n<li>特征分段：树的split的分支，即是分段方案。</li>\n<li>特征组合：叶子节点路径，即使特征组合。<br>强烈推荐。</li>\n</ol>\n<h3 id=\"0-or-missing\"><a href=\"#0-or-missing\" class=\"headerlink\" title=\"0 or missing?\"></a>0 or missing?</h3><p>最后讨论一个小问题，libsvm中被稀疏掉的特征，表示0还是表示missing？<br>答案是0，libsvm中默认没有missing。<br>但是xgboost中对libsvm的处理，是按照missing来处理的，将0和missing分开的方法是：</p>\n<ol>\n<li>连续特征：增加隐控制变量表达是否missing，另一个变量表示值。</li>\n<li>离散特征：将missing枚举为一个离散值。</li>\n</ol>"},{"title":"kafka","date":"2017-12-15T02:49:48.000Z","_content":"\n* topics\n* producers\n* consumers\n* brokers: all node in the cluster is called a Kafka broker\n\n","source":"_posts/kafka.md","raw":"---\ntitle: kafka\ndate: 2017-12-15 10:49:48\ntags:\n---\n\n* topics\n* producers\n* consumers\n* brokers: all node in the cluster is called a Kafka broker\n\n","slug":"kafka","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrzd000ksi9g6370yt0u","content":"<ul>\n<li>topics</li>\n<li>producers</li>\n<li>consumers</li>\n<li>brokers: all node in the cluster is called a Kafka broker</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<ul>\n<li>topics</li>\n<li>producers</li>\n<li>consumers</li>\n<li>brokers: all node in the cluster is called a Kafka broker</li>\n</ul>\n"},{"title":"lda","date":"2017-10-05T06:36:05.000Z","_content":"\n#### 理论\n* **痛点**<br>\n“乔布斯离我们而去了” 和 “苹果什么时候降价”如何关联？\n\n<!-- more -->\n\n* **思路**\n  * 将word映射到topic维度<br>\n  {% asset_img \"图片1.png\" [图片1] %}\n  * 概率表示<br>\n  {% asset_img \"图片2.png\" [图片2] %}\n  * 概率表示<br>\n  {% asset_img \"图片3.png\" [图片3] %}\n* **演进：Unigram Model**<br>\n  {% asset_img \"图片4.png\" [图片4] %}\n* **演进：Bayes Unigram Model**<br>\n  {% asset_img \"图片5.png\" [图片5] %}\n* **演进：PLSA**<br>\n  {% asset_img \"图片6.png\" [图片6] %}\n  {% asset_img \"图片7.png\" [图片7] %}\n* **演进：LDA**<br>\n  {% asset_img \"图片8.png\" [图片8] %}\n  {% asset_img \"图片9.png\" [图片9] %}\n* **参数估计：统计**<br>\n  {% asset_img \"图片100.png\" [图片9] %}\n* **参数估计：似然**<br>\n  {% asset_img \"图片101.png\" [图片9] %}\n* **参数估计：后验**<br>\n  {% asset_img \"图片102.png\" [图片9] %}\n* **参数估计：贝叶斯**<br>\n  {% asset_img \"图片103.png\" [图片9] %}\n* **参数估计：对比**<br>\n  {% asset_img \"图片104.png\" [图片9] %}\n* **马尔可夫链条**<br>\n  {% asset_img \"图片105.png\" [图片9] %}\n* **吉布斯采样**<br>\n  {% asset_img \"图片106.png\" [图片9] %}\n* **实现代码**<br>\n  {% asset_img \"图片201.png\" [图片9] %}\n* **Ref:**<br>\n  * Parameter estimation for text analysis （http://www.arbylon.net/publications/text-est.pdf）\n  * LDA数学八卦\n  * LDA简介 http://blog.csdn.net/huagong_adu/article/details/7937616\n  * Gibbs采样 https://www.youtube.com/watch?v=a_08GKWHFWo\n\n#### 实践\n* 基础数据\n  * 豌豆荚软件的描述信息\n  * 星级>3星\n  * 下载数>100\n  * 安装数>100\n  * 用户数>100\n* 目的\n  * 得到基于内容（描述）的item2item\n  * 得到“词--主题--包名” 的关系\n* 代码\n  * [lda_code](../NLP/LDA原理和实践/README.md)\n\n\n* LDA工具<br>\n  https://github.com/liuzhiqiangruc/dml/tree/master/tm\n* 获取数据<br>\n```\nhive -e \"\nselect a.user_id, a.item_id, a.preference\nfrom\n(\n   ...\n)\n\" > input_lda\n```\n\n* 数据概况\n  * 基础数据获取：见hql\n  * 数据整理：cat input_lda | awk -F\"\\t\" '{ print $1\"\\t\"$2 }' > input\n  * 数据形式：user_id \\t item_id （后期可考虑tf-idf优化）\n  * 行数：1849296\n  * 用户数：678588\n  * 游戏数：3377\n* 运行命令\n```\n./lda -a 0.2 -b 0.01 -k 50 -n 1000 -s 100 -d ./input -o ./output\n\n    参数说明:\n     --------------------------------------------\n           -t               算法类型1:基本lda，2:lda-collective，3:lda_time\n           -r               运行模式，1:建模，2:burn-in\n           -a               p(z|d) 的 Dirichlet 参数\n           -b               p(w|z) 的 Dirichlet 参数\n           -k               Topic个数\n           -n               迭代次数\n           -s               每多少次迭代输出一次结果\n           -d               输入数据\n           -o               输出文件目录,实现需要存在\n\n  运行时长：10分钟左右\n```\n* 关联名称<br>\n  * 处理word_topic矩阵，将ID和名称关联起来<br>\n\n```\nHql如下，\nset hive.exec.compress.output=false;\ncreate table xxxx\n(\n    id  int\n) row format delimited\nfields terminated by '\\t';\n\nload data local inpath '/output/f_word_topic' OVERWRITE  into table xxxx;\n```\n\n* Item2Item计算<br>\n\n```\nmport sys\nimport math\nimport heapq\n\nitems_D = {} ## key: id\n\ndef load_data():\n    global items_D\n    inFp = open(\"lda_norm_10.csv\", 'r')\n    while True:\n        line = inFp.readline()\n        if not line:\n            break\n        items = line.strip().split(',')\n        if len(items) != 54:\n            continue\n        item_D = {}\n        item_D['soft_package_name'] = items[0]\n        item_D['name'] = items[1]\n        item_D['id'] = int(items[2])\n        item_D['topics'] = map(float, items[3:53])\n        item_D['sum'] = float(items[53])\n        items_D[item_D['id']] = item_D\n\n\ndef dis1(A, B):\n    return sum( A['topics'][i] * B['topics'][i] for i in range(50))\n\ndef dis2(A, B):\n    return sum( 100 - abs(A['topics'][i] - B['topics'][i]) for i in range(50))\n\ndef search_similar():\n    while True:\n        line = sys.stdin.readline()\n        idx = int(line.strip())\n        itemX = items_D[idx]\n        sim = -1.0\n        for idy, itemy in items_D.items():\n            simy = dis1(items_D[idx], items_D[idy])\n            if (simy > sim or sim < 0) and idx!=idy:\n                sim = simy\n                itemY = itemy\n        print \"%s\\tass\\t%s\"%(itemX['name'], itemY['name'])\n\nload_data()\nsearch_similar()\n```\n\n* 效果展示<br>\n{% asset_img \"302.png\" [图片1] %}\n* doc2topic<br>\n{% asset_img \"401.png\" [图片1] %}\n* topic2word<br>\n{% asset_img \"402.png\" [图片1] %}\n\n* 矩阵分解图谱<br>\n{% asset_img \"501.png\" [图片1] %}\n\n* 生成模型 VS 判别模型<br>\n  * 判别方法：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。<br>\n  * 生成方法：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)<br>\n\n#### 手写LDA\n* code<br>\n\n```\nimport sys\nimport random\n\nt_c = {}\ntw_c = {}\ntd_c = {}\n\nd_w = {}\nd_w_t = {}\nw_S = set()\n\nITER_NUM = 10000\nTOPIC_NUM = 2\nALPHA = 0.01\nBETA = 0.01\n\np_k = [0] * TOPIC_NUM\nprint p_k\n\ndef input():\n    while True:\n        line = sys.stdin.readline()\n        if not line:\n            break\n        items = line.strip().split('\\t')\n        doc = items[0]\n        word_L = items[1:]\n        for word in word_L:\n            d_w.setdefault(doc, list())\n            d_w[doc].append(word)\n            w_S.add(word)\n\ndef init():\n    for d, w_L in d_w.items():\n        for w in w_L:\n            for t in range(TOPIC_NUM):\n                t_c.setdefault(t, 0)\n                tw_c.setdefault(t, dict())\n                tw_c[t].setdefault(w, 0)\n                td_c.setdefault(t, dict())\n                td_c[t].setdefault(d, 0)\n\n    for d, w_L in d_w.items():\n        for w in w_L:\n            r = random.random()\n            if r < 0.5:\n                t = 0\n            else:\n                t = 1\n\n            d_w_t.setdefault(d, dict())\n            d_w_t[d].setdefault(w, t)\n\n            t_c[t] += 1\n            tw_c[t][w] += 1\n            td_c[t][d] += 1\n\n            print d_w_t[d][w]\n\ndef sampling():\n    for iter in range(ITER_NUM):\n        print \"iters is %d\" % iter\n        for d, w_L in d_w.items():\n            for w in w_L:\n                t = d_w_t[d][w]\n                t_c[t] -= 1\n                tw_c[t][w] -= 1\n                td_c[t][d] -= 1\n\n                for k in range(TOPIC_NUM):\n                    p_k[k] = (tw_c[k][w] + BETA) * (td_c[k][d] + ALPHA) * 1.0 / (t_c[k] + BETA*len(w_S))\n                sum = 0\n                for k in range(TOPIC_NUM):\n                    sum += p_k[k]\n                for k in range(TOPIC_NUM):\n                    p_k[k] /= sum\n                for k in range(1, TOPIC_NUM):\n                    p_k[k] += p_k[k-1]\n                r = random.random()\n                for k in range(TOPIC_NUM):\n                    if(r<=p_k[k]):\n                        t = k\n                        break\n                d_w_t[d][w] = t\n                t_c[t] += 1\n                tw_c[t][w] += 1\n                td_c[t][d] += 1\n\ndef output():\n    for d, w_L in d_w.items():\n        for w in w_L:\n            print \"%s\\t%s\\t%d\" % (d, w, d_w_t[d][w])\n\nif __name__ == \"__main__\":\n    input()\n    print \"input end...\"\n    init()\n    print \"init end...\"\n    sampling()\n    print \"samplint end...\"\n    output()\n    print \"output end...\"\n```\n\n* train corpus<br>\n```\ndoc1    枪      游戏    计算机  dota    电脑\ndoc4    娃娃    美丽    面膜    高跟鞋  裙子\ndoc5    购物    娃娃    裙子    SPA     指甲\ndoc2    枪      帅      电脑    坦克    飞机\ndoc3    游戏    坦克    飞机    数学    美丽\ndoc7    计算机  帅      枪      dota\ndoc6    美丽    购物    面膜    SPA     飘柔\n```\n\n* result<br>\n```\ndoc2    枪      1\ndoc2    帅      1\ndoc2    电脑    1\ndoc2    坦克    1\ndoc2    飞机    1\ndoc3    游戏    1\ndoc3    坦克    1\ndoc3    飞机    1\ndoc3    数学    1\ndoc3    美丽    0\ndoc1    枪      1\ndoc1    游戏    1\ndoc1    计算机  1\ndoc1    dota    1\ndoc1    电脑    1\ndoc6    美丽    0\ndoc6    购物    0\ndoc6    面膜    0\ndoc6    SPA     0\ndoc6    飘柔    0\ndoc7    计算机  1\ndoc7    帅      1\ndoc7    枪      1\ndoc7    dota    1\ndoc4    娃娃    0\ndoc4    美丽    0\ndoc4    面膜    0\ndoc4    高跟鞋  0\ndoc4    裙子    0\ndoc5    购物    0\ndoc5    娃娃    0\ndoc5    裙子    0\ndoc5    SPA     0\ndoc5    指甲    0\n```\n\n写的样例默认有2个主题，一个是男生主题，一个是女生主题，lda的结果是可以把两个topic分开的。1-男生，0-女生。","source":"_posts/lda.md","raw":"---\ntitle: lda\ndate: 2017-10-05 14:36:05\ntags:\ncategories: 数据挖掘\n---\n\n#### 理论\n* **痛点**<br>\n“乔布斯离我们而去了” 和 “苹果什么时候降价”如何关联？\n\n<!-- more -->\n\n* **思路**\n  * 将word映射到topic维度<br>\n  {% asset_img \"图片1.png\" [图片1] %}\n  * 概率表示<br>\n  {% asset_img \"图片2.png\" [图片2] %}\n  * 概率表示<br>\n  {% asset_img \"图片3.png\" [图片3] %}\n* **演进：Unigram Model**<br>\n  {% asset_img \"图片4.png\" [图片4] %}\n* **演进：Bayes Unigram Model**<br>\n  {% asset_img \"图片5.png\" [图片5] %}\n* **演进：PLSA**<br>\n  {% asset_img \"图片6.png\" [图片6] %}\n  {% asset_img \"图片7.png\" [图片7] %}\n* **演进：LDA**<br>\n  {% asset_img \"图片8.png\" [图片8] %}\n  {% asset_img \"图片9.png\" [图片9] %}\n* **参数估计：统计**<br>\n  {% asset_img \"图片100.png\" [图片9] %}\n* **参数估计：似然**<br>\n  {% asset_img \"图片101.png\" [图片9] %}\n* **参数估计：后验**<br>\n  {% asset_img \"图片102.png\" [图片9] %}\n* **参数估计：贝叶斯**<br>\n  {% asset_img \"图片103.png\" [图片9] %}\n* **参数估计：对比**<br>\n  {% asset_img \"图片104.png\" [图片9] %}\n* **马尔可夫链条**<br>\n  {% asset_img \"图片105.png\" [图片9] %}\n* **吉布斯采样**<br>\n  {% asset_img \"图片106.png\" [图片9] %}\n* **实现代码**<br>\n  {% asset_img \"图片201.png\" [图片9] %}\n* **Ref:**<br>\n  * Parameter estimation for text analysis （http://www.arbylon.net/publications/text-est.pdf）\n  * LDA数学八卦\n  * LDA简介 http://blog.csdn.net/huagong_adu/article/details/7937616\n  * Gibbs采样 https://www.youtube.com/watch?v=a_08GKWHFWo\n\n#### 实践\n* 基础数据\n  * 豌豆荚软件的描述信息\n  * 星级>3星\n  * 下载数>100\n  * 安装数>100\n  * 用户数>100\n* 目的\n  * 得到基于内容（描述）的item2item\n  * 得到“词--主题--包名” 的关系\n* 代码\n  * [lda_code](../NLP/LDA原理和实践/README.md)\n\n\n* LDA工具<br>\n  https://github.com/liuzhiqiangruc/dml/tree/master/tm\n* 获取数据<br>\n```\nhive -e \"\nselect a.user_id, a.item_id, a.preference\nfrom\n(\n   ...\n)\n\" > input_lda\n```\n\n* 数据概况\n  * 基础数据获取：见hql\n  * 数据整理：cat input_lda | awk -F\"\\t\" '{ print $1\"\\t\"$2 }' > input\n  * 数据形式：user_id \\t item_id （后期可考虑tf-idf优化）\n  * 行数：1849296\n  * 用户数：678588\n  * 游戏数：3377\n* 运行命令\n```\n./lda -a 0.2 -b 0.01 -k 50 -n 1000 -s 100 -d ./input -o ./output\n\n    参数说明:\n     --------------------------------------------\n           -t               算法类型1:基本lda，2:lda-collective，3:lda_time\n           -r               运行模式，1:建模，2:burn-in\n           -a               p(z|d) 的 Dirichlet 参数\n           -b               p(w|z) 的 Dirichlet 参数\n           -k               Topic个数\n           -n               迭代次数\n           -s               每多少次迭代输出一次结果\n           -d               输入数据\n           -o               输出文件目录,实现需要存在\n\n  运行时长：10分钟左右\n```\n* 关联名称<br>\n  * 处理word_topic矩阵，将ID和名称关联起来<br>\n\n```\nHql如下，\nset hive.exec.compress.output=false;\ncreate table xxxx\n(\n    id  int\n) row format delimited\nfields terminated by '\\t';\n\nload data local inpath '/output/f_word_topic' OVERWRITE  into table xxxx;\n```\n\n* Item2Item计算<br>\n\n```\nmport sys\nimport math\nimport heapq\n\nitems_D = {} ## key: id\n\ndef load_data():\n    global items_D\n    inFp = open(\"lda_norm_10.csv\", 'r')\n    while True:\n        line = inFp.readline()\n        if not line:\n            break\n        items = line.strip().split(',')\n        if len(items) != 54:\n            continue\n        item_D = {}\n        item_D['soft_package_name'] = items[0]\n        item_D['name'] = items[1]\n        item_D['id'] = int(items[2])\n        item_D['topics'] = map(float, items[3:53])\n        item_D['sum'] = float(items[53])\n        items_D[item_D['id']] = item_D\n\n\ndef dis1(A, B):\n    return sum( A['topics'][i] * B['topics'][i] for i in range(50))\n\ndef dis2(A, B):\n    return sum( 100 - abs(A['topics'][i] - B['topics'][i]) for i in range(50))\n\ndef search_similar():\n    while True:\n        line = sys.stdin.readline()\n        idx = int(line.strip())\n        itemX = items_D[idx]\n        sim = -1.0\n        for idy, itemy in items_D.items():\n            simy = dis1(items_D[idx], items_D[idy])\n            if (simy > sim or sim < 0) and idx!=idy:\n                sim = simy\n                itemY = itemy\n        print \"%s\\tass\\t%s\"%(itemX['name'], itemY['name'])\n\nload_data()\nsearch_similar()\n```\n\n* 效果展示<br>\n{% asset_img \"302.png\" [图片1] %}\n* doc2topic<br>\n{% asset_img \"401.png\" [图片1] %}\n* topic2word<br>\n{% asset_img \"402.png\" [图片1] %}\n\n* 矩阵分解图谱<br>\n{% asset_img \"501.png\" [图片1] %}\n\n* 生成模型 VS 判别模型<br>\n  * 判别方法：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。<br>\n  * 生成方法：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)<br>\n\n#### 手写LDA\n* code<br>\n\n```\nimport sys\nimport random\n\nt_c = {}\ntw_c = {}\ntd_c = {}\n\nd_w = {}\nd_w_t = {}\nw_S = set()\n\nITER_NUM = 10000\nTOPIC_NUM = 2\nALPHA = 0.01\nBETA = 0.01\n\np_k = [0] * TOPIC_NUM\nprint p_k\n\ndef input():\n    while True:\n        line = sys.stdin.readline()\n        if not line:\n            break\n        items = line.strip().split('\\t')\n        doc = items[0]\n        word_L = items[1:]\n        for word in word_L:\n            d_w.setdefault(doc, list())\n            d_w[doc].append(word)\n            w_S.add(word)\n\ndef init():\n    for d, w_L in d_w.items():\n        for w in w_L:\n            for t in range(TOPIC_NUM):\n                t_c.setdefault(t, 0)\n                tw_c.setdefault(t, dict())\n                tw_c[t].setdefault(w, 0)\n                td_c.setdefault(t, dict())\n                td_c[t].setdefault(d, 0)\n\n    for d, w_L in d_w.items():\n        for w in w_L:\n            r = random.random()\n            if r < 0.5:\n                t = 0\n            else:\n                t = 1\n\n            d_w_t.setdefault(d, dict())\n            d_w_t[d].setdefault(w, t)\n\n            t_c[t] += 1\n            tw_c[t][w] += 1\n            td_c[t][d] += 1\n\n            print d_w_t[d][w]\n\ndef sampling():\n    for iter in range(ITER_NUM):\n        print \"iters is %d\" % iter\n        for d, w_L in d_w.items():\n            for w in w_L:\n                t = d_w_t[d][w]\n                t_c[t] -= 1\n                tw_c[t][w] -= 1\n                td_c[t][d] -= 1\n\n                for k in range(TOPIC_NUM):\n                    p_k[k] = (tw_c[k][w] + BETA) * (td_c[k][d] + ALPHA) * 1.0 / (t_c[k] + BETA*len(w_S))\n                sum = 0\n                for k in range(TOPIC_NUM):\n                    sum += p_k[k]\n                for k in range(TOPIC_NUM):\n                    p_k[k] /= sum\n                for k in range(1, TOPIC_NUM):\n                    p_k[k] += p_k[k-1]\n                r = random.random()\n                for k in range(TOPIC_NUM):\n                    if(r<=p_k[k]):\n                        t = k\n                        break\n                d_w_t[d][w] = t\n                t_c[t] += 1\n                tw_c[t][w] += 1\n                td_c[t][d] += 1\n\ndef output():\n    for d, w_L in d_w.items():\n        for w in w_L:\n            print \"%s\\t%s\\t%d\" % (d, w, d_w_t[d][w])\n\nif __name__ == \"__main__\":\n    input()\n    print \"input end...\"\n    init()\n    print \"init end...\"\n    sampling()\n    print \"samplint end...\"\n    output()\n    print \"output end...\"\n```\n\n* train corpus<br>\n```\ndoc1    枪      游戏    计算机  dota    电脑\ndoc4    娃娃    美丽    面膜    高跟鞋  裙子\ndoc5    购物    娃娃    裙子    SPA     指甲\ndoc2    枪      帅      电脑    坦克    飞机\ndoc3    游戏    坦克    飞机    数学    美丽\ndoc7    计算机  帅      枪      dota\ndoc6    美丽    购物    面膜    SPA     飘柔\n```\n\n* result<br>\n```\ndoc2    枪      1\ndoc2    帅      1\ndoc2    电脑    1\ndoc2    坦克    1\ndoc2    飞机    1\ndoc3    游戏    1\ndoc3    坦克    1\ndoc3    飞机    1\ndoc3    数学    1\ndoc3    美丽    0\ndoc1    枪      1\ndoc1    游戏    1\ndoc1    计算机  1\ndoc1    dota    1\ndoc1    电脑    1\ndoc6    美丽    0\ndoc6    购物    0\ndoc6    面膜    0\ndoc6    SPA     0\ndoc6    飘柔    0\ndoc7    计算机  1\ndoc7    帅      1\ndoc7    枪      1\ndoc7    dota    1\ndoc4    娃娃    0\ndoc4    美丽    0\ndoc4    面膜    0\ndoc4    高跟鞋  0\ndoc4    裙子    0\ndoc5    购物    0\ndoc5    娃娃    0\ndoc5    裙子    0\ndoc5    SPA     0\ndoc5    指甲    0\n```\n\n写的样例默认有2个主题，一个是男生主题，一个是女生主题，lda的结果是可以把两个topic分开的。1-男生，0-女生。","slug":"lda","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrze000lsi9grm9b9qvt","content":"<h4 id=\"理论\"><a href=\"#理论\" class=\"headerlink\" title=\"理论\"></a>理论</h4><ul>\n<li><strong>痛点</strong><br><br>“乔布斯离我们而去了” 和 “苹果什么时候降价”如何关联？</li>\n</ul>\n<a id=\"more\"></a>\n<ul>\n<li><strong>思路</strong><ul>\n<li>将word映射到topic维度<br></li>\n<li>概率表示<br></li>\n<li>概率表示<br></li>\n</ul>\n</li>\n<li><strong>演进：Unigram Model</strong><br></li>\n<li><strong>演进：Bayes Unigram Model</strong><br></li>\n<li><strong>演进：PLSA</strong><br>\n</li>\n<li><strong>演进：LDA</strong><br>\n</li>\n<li><strong>参数估计：统计</strong><br></li>\n<li><strong>参数估计：似然</strong><br></li>\n<li><strong>参数估计：后验</strong><br></li>\n<li><strong>参数估计：贝叶斯</strong><br></li>\n<li><strong>参数估计：对比</strong><br></li>\n<li><strong>马尔可夫链条</strong><br></li>\n<li><strong>吉布斯采样</strong><br></li>\n<li><strong>实现代码</strong><br></li>\n<li><strong>Ref:</strong><br><ul>\n<li>Parameter estimation for text analysis （<a href=\"http://www.arbylon.net/publications/text-est.pdf）\" target=\"_blank\" rel=\"external\">http://www.arbylon.net/publications/text-est.pdf）</a></li>\n<li>LDA数学八卦</li>\n<li>LDA简介 <a href=\"http://blog.csdn.net/huagong_adu/article/details/7937616\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/huagong_adu/article/details/7937616</a></li>\n<li>Gibbs采样 <a href=\"https://www.youtube.com/watch?v=a_08GKWHFWo\" target=\"_blank\" rel=\"external\">https://www.youtube.com/watch?v=a_08GKWHFWo</a></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h4><ul>\n<li>基础数据<ul>\n<li>豌豆荚软件的描述信息</li>\n<li>星级&gt;3星</li>\n<li>下载数&gt;100</li>\n<li>安装数&gt;100</li>\n<li>用户数&gt;100</li>\n</ul>\n</li>\n<li>目的<ul>\n<li>得到基于内容（描述）的item2item</li>\n<li>得到“词–主题–包名” 的关系</li>\n</ul>\n</li>\n<li>代码<ul>\n<li><a href=\"../NLP/LDA原理和实践/README.md\">lda_code</a></li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>LDA工具<br><br><a href=\"https://github.com/liuzhiqiangruc/dml/tree/master/tm\" target=\"_blank\" rel=\"external\">https://github.com/liuzhiqiangruc/dml/tree/master/tm</a></li>\n<li><p>获取数据<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">hive -e &quot;</div><div class=\"line\">select a.user_id, a.item_id, a.preference</div><div class=\"line\">from</div><div class=\"line\">(</div><div class=\"line\">   ...</div><div class=\"line\">)</div><div class=\"line\">&quot; &gt; input_lda</div></pre></td></tr></table></figure>\n</li>\n<li><p>数据概况</p>\n<ul>\n<li>基础数据获取：见hql</li>\n<li>数据整理：cat input_lda | awk -F”\\t” ‘{ print $1”\\t”$2 }’ &gt; input</li>\n<li>数据形式：user_id \\t item_id （后期可考虑tf-idf优化）</li>\n<li>行数：1849296</li>\n<li>用户数：678588</li>\n<li>游戏数：3377</li>\n</ul>\n</li>\n<li><p>运行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">./lda -a 0.2 -b 0.01 -k 50 -n 1000 -s 100 -d ./input -o ./output</div><div class=\"line\"></div><div class=\"line\">    参数说明:</div><div class=\"line\">     --------------------------------------------</div><div class=\"line\">           -t               算法类型1:基本lda，2:lda-collective，3:lda_time</div><div class=\"line\">           -r               运行模式，1:建模，2:burn-in</div><div class=\"line\">           -a               p(z|d) 的 Dirichlet 参数</div><div class=\"line\">           -b               p(w|z) 的 Dirichlet 参数</div><div class=\"line\">           -k               Topic个数</div><div class=\"line\">           -n               迭代次数</div><div class=\"line\">           -s               每多少次迭代输出一次结果</div><div class=\"line\">           -d               输入数据</div><div class=\"line\">           -o               输出文件目录,实现需要存在</div><div class=\"line\"></div><div class=\"line\">  运行时长：10分钟左右</div></pre></td></tr></table></figure>\n</li>\n<li><p>关联名称<br></p>\n<ul>\n<li>处理word_topic矩阵，将ID和名称关联起来<br></li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">Hql如下，</div><div class=\"line\">set hive.exec.compress.output=false;</div><div class=\"line\">create table xxxx</div><div class=\"line\">(</div><div class=\"line\">    id  int</div><div class=\"line\">) row format delimited</div><div class=\"line\">fields terminated by &apos;\\t&apos;;</div><div class=\"line\"></div><div class=\"line\">load data local inpath &apos;/output/f_word_topic&apos; OVERWRITE  into table xxxx;</div></pre></td></tr></table></figure>\n<ul>\n<li>Item2Item计算<br></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div></pre></td><td class=\"code\"><pre><div class=\"line\">mport sys</div><div class=\"line\">import math</div><div class=\"line\">import heapq</div><div class=\"line\"></div><div class=\"line\">items_D = &#123;&#125; ## key: id</div><div class=\"line\"></div><div class=\"line\">def load_data():</div><div class=\"line\">    global items_D</div><div class=\"line\">    inFp = open(&quot;lda_norm_10.csv&quot;, &apos;r&apos;)</div><div class=\"line\">    while True:</div><div class=\"line\">        line = inFp.readline()</div><div class=\"line\">        if not line:</div><div class=\"line\">            break</div><div class=\"line\">        items = line.strip().split(&apos;,&apos;)</div><div class=\"line\">        if len(items) != 54:</div><div class=\"line\">            continue</div><div class=\"line\">        item_D = &#123;&#125;</div><div class=\"line\">        item_D[&apos;soft_package_name&apos;] = items[0]</div><div class=\"line\">        item_D[&apos;name&apos;] = items[1]</div><div class=\"line\">        item_D[&apos;id&apos;] = int(items[2])</div><div class=\"line\">        item_D[&apos;topics&apos;] = map(float, items[3:53])</div><div class=\"line\">        item_D[&apos;sum&apos;] = float(items[53])</div><div class=\"line\">        items_D[item_D[&apos;id&apos;]] = item_D</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def dis1(A, B):</div><div class=\"line\">    return sum( A[&apos;topics&apos;][i] * B[&apos;topics&apos;][i] for i in range(50))</div><div class=\"line\"></div><div class=\"line\">def dis2(A, B):</div><div class=\"line\">    return sum( 100 - abs(A[&apos;topics&apos;][i] - B[&apos;topics&apos;][i]) for i in range(50))</div><div class=\"line\"></div><div class=\"line\">def search_similar():</div><div class=\"line\">    while True:</div><div class=\"line\">        line = sys.stdin.readline()</div><div class=\"line\">        idx = int(line.strip())</div><div class=\"line\">        itemX = items_D[idx]</div><div class=\"line\">        sim = -1.0</div><div class=\"line\">        for idy, itemy in items_D.items():</div><div class=\"line\">            simy = dis1(items_D[idx], items_D[idy])</div><div class=\"line\">            if (simy &gt; sim or sim &lt; 0) and idx!=idy:</div><div class=\"line\">                sim = simy</div><div class=\"line\">                itemY = itemy</div><div class=\"line\">        print &quot;%s\\tass\\t%s&quot;%(itemX[&apos;name&apos;], itemY[&apos;name&apos;])</div><div class=\"line\"></div><div class=\"line\">load_data()</div><div class=\"line\">search_similar()</div></pre></td></tr></table></figure>\n<ul>\n<li>效果展示<br><img src=\"/2017/10/05/lda/302.png\" alt=\"[图片1]\" title=\"[图片1]\"></li>\n<li>doc2topic<br><img src=\"/2017/10/05/lda/401.png\" alt=\"[图片1]\" title=\"[图片1]\"></li>\n<li><p>topic2word<br></p>\n<img src=\"/2017/10/05/lda/402.png\" alt=\"[图片1]\" title=\"[图片1]\">\n</li>\n<li><p>矩阵分解图谱<br></p>\n<img src=\"/2017/10/05/lda/501.png\" alt=\"[图片1]\" title=\"[图片1]\">\n</li>\n<li><p>生成模型 VS 判别模型<br></p>\n<ul>\n<li>判别方法：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。<br></li>\n<li>生成方法：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)<br></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"手写LDA\"><a href=\"#手写LDA\" class=\"headerlink\" title=\"手写LDA\"></a>手写LDA</h4><ul>\n<li>code<br></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div></pre></td><td class=\"code\"><pre><div class=\"line\">import sys</div><div class=\"line\">import random</div><div class=\"line\"></div><div class=\"line\">t_c = &#123;&#125;</div><div class=\"line\">tw_c = &#123;&#125;</div><div class=\"line\">td_c = &#123;&#125;</div><div class=\"line\"></div><div class=\"line\">d_w = &#123;&#125;</div><div class=\"line\">d_w_t = &#123;&#125;</div><div class=\"line\">w_S = set()</div><div class=\"line\"></div><div class=\"line\">ITER_NUM = 10000</div><div class=\"line\">TOPIC_NUM = 2</div><div class=\"line\">ALPHA = 0.01</div><div class=\"line\">BETA = 0.01</div><div class=\"line\"></div><div class=\"line\">p_k = [0] * TOPIC_NUM</div><div class=\"line\">print p_k</div><div class=\"line\"></div><div class=\"line\">def input():</div><div class=\"line\">    while True:</div><div class=\"line\">        line = sys.stdin.readline()</div><div class=\"line\">        if not line:</div><div class=\"line\">            break</div><div class=\"line\">        items = line.strip().split(&apos;\\t&apos;)</div><div class=\"line\">        doc = items[0]</div><div class=\"line\">        word_L = items[1:]</div><div class=\"line\">        for word in word_L:</div><div class=\"line\">            d_w.setdefault(doc, list())</div><div class=\"line\">            d_w[doc].append(word)</div><div class=\"line\">            w_S.add(word)</div><div class=\"line\"></div><div class=\"line\">def init():</div><div class=\"line\">    for d, w_L in d_w.items():</div><div class=\"line\">        for w in w_L:</div><div class=\"line\">            for t in range(TOPIC_NUM):</div><div class=\"line\">                t_c.setdefault(t, 0)</div><div class=\"line\">                tw_c.setdefault(t, dict())</div><div class=\"line\">                tw_c[t].setdefault(w, 0)</div><div class=\"line\">                td_c.setdefault(t, dict())</div><div class=\"line\">                td_c[t].setdefault(d, 0)</div><div class=\"line\"></div><div class=\"line\">    for d, w_L in d_w.items():</div><div class=\"line\">        for w in w_L:</div><div class=\"line\">            r = random.random()</div><div class=\"line\">            if r &lt; 0.5:</div><div class=\"line\">                t = 0</div><div class=\"line\">            else:</div><div class=\"line\">                t = 1</div><div class=\"line\"></div><div class=\"line\">            d_w_t.setdefault(d, dict())</div><div class=\"line\">            d_w_t[d].setdefault(w, t)</div><div class=\"line\"></div><div class=\"line\">            t_c[t] += 1</div><div class=\"line\">            tw_c[t][w] += 1</div><div class=\"line\">            td_c[t][d] += 1</div><div class=\"line\"></div><div class=\"line\">            print d_w_t[d][w]</div><div class=\"line\"></div><div class=\"line\">def sampling():</div><div class=\"line\">    for iter in range(ITER_NUM):</div><div class=\"line\">        print &quot;iters is %d&quot; % iter</div><div class=\"line\">        for d, w_L in d_w.items():</div><div class=\"line\">            for w in w_L:</div><div class=\"line\">                t = d_w_t[d][w]</div><div class=\"line\">                t_c[t] -= 1</div><div class=\"line\">                tw_c[t][w] -= 1</div><div class=\"line\">                td_c[t][d] -= 1</div><div class=\"line\"></div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    p_k[k] = (tw_c[k][w] + BETA) * (td_c[k][d] + ALPHA) * 1.0 / (t_c[k] + BETA*len(w_S))</div><div class=\"line\">                sum = 0</div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    sum += p_k[k]</div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    p_k[k] /= sum</div><div class=\"line\">                for k in range(1, TOPIC_NUM):</div><div class=\"line\">                    p_k[k] += p_k[k-1]</div><div class=\"line\">                r = random.random()</div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    if(r&lt;=p_k[k]):</div><div class=\"line\">                        t = k</div><div class=\"line\">                        break</div><div class=\"line\">                d_w_t[d][w] = t</div><div class=\"line\">                t_c[t] += 1</div><div class=\"line\">                tw_c[t][w] += 1</div><div class=\"line\">                td_c[t][d] += 1</div><div class=\"line\"></div><div class=\"line\">def output():</div><div class=\"line\">    for d, w_L in d_w.items():</div><div class=\"line\">        for w in w_L:</div><div class=\"line\">            print &quot;%s\\t%s\\t%d&quot; % (d, w, d_w_t[d][w])</div><div class=\"line\"></div><div class=\"line\">if __name__ == &quot;__main__&quot;:</div><div class=\"line\">    input()</div><div class=\"line\">    print &quot;input end...&quot;</div><div class=\"line\">    init()</div><div class=\"line\">    print &quot;init end...&quot;</div><div class=\"line\">    sampling()</div><div class=\"line\">    print &quot;samplint end...&quot;</div><div class=\"line\">    output()</div><div class=\"line\">    print &quot;output end...&quot;</div></pre></td></tr></table></figure>\n<ul>\n<li><p>train corpus<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">doc1    枪      游戏    计算机  dota    电脑</div><div class=\"line\">doc4    娃娃    美丽    面膜    高跟鞋  裙子</div><div class=\"line\">doc5    购物    娃娃    裙子    SPA     指甲</div><div class=\"line\">doc2    枪      帅      电脑    坦克    飞机</div><div class=\"line\">doc3    游戏    坦克    飞机    数学    美丽</div><div class=\"line\">doc7    计算机  帅      枪      dota</div><div class=\"line\">doc6    美丽    购物    面膜    SPA     飘柔</div></pre></td></tr></table></figure>\n</li>\n<li><p>result<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div></pre></td><td class=\"code\"><pre><div class=\"line\">doc2    枪      1</div><div class=\"line\">doc2    帅      1</div><div class=\"line\">doc2    电脑    1</div><div class=\"line\">doc2    坦克    1</div><div class=\"line\">doc2    飞机    1</div><div class=\"line\">doc3    游戏    1</div><div class=\"line\">doc3    坦克    1</div><div class=\"line\">doc3    飞机    1</div><div class=\"line\">doc3    数学    1</div><div class=\"line\">doc3    美丽    0</div><div class=\"line\">doc1    枪      1</div><div class=\"line\">doc1    游戏    1</div><div class=\"line\">doc1    计算机  1</div><div class=\"line\">doc1    dota    1</div><div class=\"line\">doc1    电脑    1</div><div class=\"line\">doc6    美丽    0</div><div class=\"line\">doc6    购物    0</div><div class=\"line\">doc6    面膜    0</div><div class=\"line\">doc6    SPA     0</div><div class=\"line\">doc6    飘柔    0</div><div class=\"line\">doc7    计算机  1</div><div class=\"line\">doc7    帅      1</div><div class=\"line\">doc7    枪      1</div><div class=\"line\">doc7    dota    1</div><div class=\"line\">doc4    娃娃    0</div><div class=\"line\">doc4    美丽    0</div><div class=\"line\">doc4    面膜    0</div><div class=\"line\">doc4    高跟鞋  0</div><div class=\"line\">doc4    裙子    0</div><div class=\"line\">doc5    购物    0</div><div class=\"line\">doc5    娃娃    0</div><div class=\"line\">doc5    裙子    0</div><div class=\"line\">doc5    SPA     0</div><div class=\"line\">doc5    指甲    0</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>写的样例默认有2个主题，一个是男生主题，一个是女生主题，lda的结果是可以把两个topic分开的。1-男生，0-女生。</p>\n","site":{"data":{}},"excerpt":"<h4 id=\"理论\"><a href=\"#理论\" class=\"headerlink\" title=\"理论\"></a>理论</h4><ul>\n<li><strong>痛点</strong><br><br>“乔布斯离我们而去了” 和 “苹果什么时候降价”如何关联？</li>\n</ul>","more":"<ul>\n<li><strong>思路</strong><ul>\n<li>将word映射到topic维度<br></li>\n<li>概率表示<br></li>\n<li>概率表示<br></li>\n</ul>\n</li>\n<li><strong>演进：Unigram Model</strong><br></li>\n<li><strong>演进：Bayes Unigram Model</strong><br></li>\n<li><strong>演进：PLSA</strong><br>\n</li>\n<li><strong>演进：LDA</strong><br>\n</li>\n<li><strong>参数估计：统计</strong><br></li>\n<li><strong>参数估计：似然</strong><br></li>\n<li><strong>参数估计：后验</strong><br></li>\n<li><strong>参数估计：贝叶斯</strong><br></li>\n<li><strong>参数估计：对比</strong><br></li>\n<li><strong>马尔可夫链条</strong><br></li>\n<li><strong>吉布斯采样</strong><br></li>\n<li><strong>实现代码</strong><br></li>\n<li><strong>Ref:</strong><br><ul>\n<li>Parameter estimation for text analysis （<a href=\"http://www.arbylon.net/publications/text-est.pdf）\" target=\"_blank\" rel=\"external\">http://www.arbylon.net/publications/text-est.pdf）</a></li>\n<li>LDA数学八卦</li>\n<li>LDA简介 <a href=\"http://blog.csdn.net/huagong_adu/article/details/7937616\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/huagong_adu/article/details/7937616</a></li>\n<li>Gibbs采样 <a href=\"https://www.youtube.com/watch?v=a_08GKWHFWo\" target=\"_blank\" rel=\"external\">https://www.youtube.com/watch?v=a_08GKWHFWo</a></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h4><ul>\n<li>基础数据<ul>\n<li>豌豆荚软件的描述信息</li>\n<li>星级&gt;3星</li>\n<li>下载数&gt;100</li>\n<li>安装数&gt;100</li>\n<li>用户数&gt;100</li>\n</ul>\n</li>\n<li>目的<ul>\n<li>得到基于内容（描述）的item2item</li>\n<li>得到“词–主题–包名” 的关系</li>\n</ul>\n</li>\n<li>代码<ul>\n<li><a href=\"../NLP/LDA原理和实践/README.md\">lda_code</a></li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>LDA工具<br><br><a href=\"https://github.com/liuzhiqiangruc/dml/tree/master/tm\" target=\"_blank\" rel=\"external\">https://github.com/liuzhiqiangruc/dml/tree/master/tm</a></li>\n<li><p>获取数据<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">hive -e &quot;</div><div class=\"line\">select a.user_id, a.item_id, a.preference</div><div class=\"line\">from</div><div class=\"line\">(</div><div class=\"line\">   ...</div><div class=\"line\">)</div><div class=\"line\">&quot; &gt; input_lda</div></pre></td></tr></table></figure>\n</li>\n<li><p>数据概况</p>\n<ul>\n<li>基础数据获取：见hql</li>\n<li>数据整理：cat input_lda | awk -F”\\t” ‘{ print $1”\\t”$2 }’ &gt; input</li>\n<li>数据形式：user_id \\t item_id （后期可考虑tf-idf优化）</li>\n<li>行数：1849296</li>\n<li>用户数：678588</li>\n<li>游戏数：3377</li>\n</ul>\n</li>\n<li><p>运行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">./lda -a 0.2 -b 0.01 -k 50 -n 1000 -s 100 -d ./input -o ./output</div><div class=\"line\"></div><div class=\"line\">    参数说明:</div><div class=\"line\">     --------------------------------------------</div><div class=\"line\">           -t               算法类型1:基本lda，2:lda-collective，3:lda_time</div><div class=\"line\">           -r               运行模式，1:建模，2:burn-in</div><div class=\"line\">           -a               p(z|d) 的 Dirichlet 参数</div><div class=\"line\">           -b               p(w|z) 的 Dirichlet 参数</div><div class=\"line\">           -k               Topic个数</div><div class=\"line\">           -n               迭代次数</div><div class=\"line\">           -s               每多少次迭代输出一次结果</div><div class=\"line\">           -d               输入数据</div><div class=\"line\">           -o               输出文件目录,实现需要存在</div><div class=\"line\"></div><div class=\"line\">  运行时长：10分钟左右</div></pre></td></tr></table></figure>\n</li>\n<li><p>关联名称<br></p>\n<ul>\n<li>处理word_topic矩阵，将ID和名称关联起来<br></li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">Hql如下，</div><div class=\"line\">set hive.exec.compress.output=false;</div><div class=\"line\">create table xxxx</div><div class=\"line\">(</div><div class=\"line\">    id  int</div><div class=\"line\">) row format delimited</div><div class=\"line\">fields terminated by &apos;\\t&apos;;</div><div class=\"line\"></div><div class=\"line\">load data local inpath &apos;/output/f_word_topic&apos; OVERWRITE  into table xxxx;</div></pre></td></tr></table></figure>\n<ul>\n<li>Item2Item计算<br></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div></pre></td><td class=\"code\"><pre><div class=\"line\">mport sys</div><div class=\"line\">import math</div><div class=\"line\">import heapq</div><div class=\"line\"></div><div class=\"line\">items_D = &#123;&#125; ## key: id</div><div class=\"line\"></div><div class=\"line\">def load_data():</div><div class=\"line\">    global items_D</div><div class=\"line\">    inFp = open(&quot;lda_norm_10.csv&quot;, &apos;r&apos;)</div><div class=\"line\">    while True:</div><div class=\"line\">        line = inFp.readline()</div><div class=\"line\">        if not line:</div><div class=\"line\">            break</div><div class=\"line\">        items = line.strip().split(&apos;,&apos;)</div><div class=\"line\">        if len(items) != 54:</div><div class=\"line\">            continue</div><div class=\"line\">        item_D = &#123;&#125;</div><div class=\"line\">        item_D[&apos;soft_package_name&apos;] = items[0]</div><div class=\"line\">        item_D[&apos;name&apos;] = items[1]</div><div class=\"line\">        item_D[&apos;id&apos;] = int(items[2])</div><div class=\"line\">        item_D[&apos;topics&apos;] = map(float, items[3:53])</div><div class=\"line\">        item_D[&apos;sum&apos;] = float(items[53])</div><div class=\"line\">        items_D[item_D[&apos;id&apos;]] = item_D</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def dis1(A, B):</div><div class=\"line\">    return sum( A[&apos;topics&apos;][i] * B[&apos;topics&apos;][i] for i in range(50))</div><div class=\"line\"></div><div class=\"line\">def dis2(A, B):</div><div class=\"line\">    return sum( 100 - abs(A[&apos;topics&apos;][i] - B[&apos;topics&apos;][i]) for i in range(50))</div><div class=\"line\"></div><div class=\"line\">def search_similar():</div><div class=\"line\">    while True:</div><div class=\"line\">        line = sys.stdin.readline()</div><div class=\"line\">        idx = int(line.strip())</div><div class=\"line\">        itemX = items_D[idx]</div><div class=\"line\">        sim = -1.0</div><div class=\"line\">        for idy, itemy in items_D.items():</div><div class=\"line\">            simy = dis1(items_D[idx], items_D[idy])</div><div class=\"line\">            if (simy &gt; sim or sim &lt; 0) and idx!=idy:</div><div class=\"line\">                sim = simy</div><div class=\"line\">                itemY = itemy</div><div class=\"line\">        print &quot;%s\\tass\\t%s&quot;%(itemX[&apos;name&apos;], itemY[&apos;name&apos;])</div><div class=\"line\"></div><div class=\"line\">load_data()</div><div class=\"line\">search_similar()</div></pre></td></tr></table></figure>\n<ul>\n<li>效果展示<br><img src=\"/2017/10/05/lda/302.png\" alt=\"[图片1]\" title=\"[图片1]\"></li>\n<li>doc2topic<br><img src=\"/2017/10/05/lda/401.png\" alt=\"[图片1]\" title=\"[图片1]\"></li>\n<li><p>topic2word<br></p>\n<img src=\"/2017/10/05/lda/402.png\" alt=\"[图片1]\" title=\"[图片1]\">\n</li>\n<li><p>矩阵分解图谱<br></p>\n<img src=\"/2017/10/05/lda/501.png\" alt=\"[图片1]\" title=\"[图片1]\">\n</li>\n<li><p>生成模型 VS 判别模型<br></p>\n<ul>\n<li>判别方法：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。<br></li>\n<li>生成方法：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)<br></li>\n</ul>\n</li>\n</ul>\n<h4 id=\"手写LDA\"><a href=\"#手写LDA\" class=\"headerlink\" title=\"手写LDA\"></a>手写LDA</h4><ul>\n<li>code<br></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div></pre></td><td class=\"code\"><pre><div class=\"line\">import sys</div><div class=\"line\">import random</div><div class=\"line\"></div><div class=\"line\">t_c = &#123;&#125;</div><div class=\"line\">tw_c = &#123;&#125;</div><div class=\"line\">td_c = &#123;&#125;</div><div class=\"line\"></div><div class=\"line\">d_w = &#123;&#125;</div><div class=\"line\">d_w_t = &#123;&#125;</div><div class=\"line\">w_S = set()</div><div class=\"line\"></div><div class=\"line\">ITER_NUM = 10000</div><div class=\"line\">TOPIC_NUM = 2</div><div class=\"line\">ALPHA = 0.01</div><div class=\"line\">BETA = 0.01</div><div class=\"line\"></div><div class=\"line\">p_k = [0] * TOPIC_NUM</div><div class=\"line\">print p_k</div><div class=\"line\"></div><div class=\"line\">def input():</div><div class=\"line\">    while True:</div><div class=\"line\">        line = sys.stdin.readline()</div><div class=\"line\">        if not line:</div><div class=\"line\">            break</div><div class=\"line\">        items = line.strip().split(&apos;\\t&apos;)</div><div class=\"line\">        doc = items[0]</div><div class=\"line\">        word_L = items[1:]</div><div class=\"line\">        for word in word_L:</div><div class=\"line\">            d_w.setdefault(doc, list())</div><div class=\"line\">            d_w[doc].append(word)</div><div class=\"line\">            w_S.add(word)</div><div class=\"line\"></div><div class=\"line\">def init():</div><div class=\"line\">    for d, w_L in d_w.items():</div><div class=\"line\">        for w in w_L:</div><div class=\"line\">            for t in range(TOPIC_NUM):</div><div class=\"line\">                t_c.setdefault(t, 0)</div><div class=\"line\">                tw_c.setdefault(t, dict())</div><div class=\"line\">                tw_c[t].setdefault(w, 0)</div><div class=\"line\">                td_c.setdefault(t, dict())</div><div class=\"line\">                td_c[t].setdefault(d, 0)</div><div class=\"line\"></div><div class=\"line\">    for d, w_L in d_w.items():</div><div class=\"line\">        for w in w_L:</div><div class=\"line\">            r = random.random()</div><div class=\"line\">            if r &lt; 0.5:</div><div class=\"line\">                t = 0</div><div class=\"line\">            else:</div><div class=\"line\">                t = 1</div><div class=\"line\"></div><div class=\"line\">            d_w_t.setdefault(d, dict())</div><div class=\"line\">            d_w_t[d].setdefault(w, t)</div><div class=\"line\"></div><div class=\"line\">            t_c[t] += 1</div><div class=\"line\">            tw_c[t][w] += 1</div><div class=\"line\">            td_c[t][d] += 1</div><div class=\"line\"></div><div class=\"line\">            print d_w_t[d][w]</div><div class=\"line\"></div><div class=\"line\">def sampling():</div><div class=\"line\">    for iter in range(ITER_NUM):</div><div class=\"line\">        print &quot;iters is %d&quot; % iter</div><div class=\"line\">        for d, w_L in d_w.items():</div><div class=\"line\">            for w in w_L:</div><div class=\"line\">                t = d_w_t[d][w]</div><div class=\"line\">                t_c[t] -= 1</div><div class=\"line\">                tw_c[t][w] -= 1</div><div class=\"line\">                td_c[t][d] -= 1</div><div class=\"line\"></div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    p_k[k] = (tw_c[k][w] + BETA) * (td_c[k][d] + ALPHA) * 1.0 / (t_c[k] + BETA*len(w_S))</div><div class=\"line\">                sum = 0</div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    sum += p_k[k]</div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    p_k[k] /= sum</div><div class=\"line\">                for k in range(1, TOPIC_NUM):</div><div class=\"line\">                    p_k[k] += p_k[k-1]</div><div class=\"line\">                r = random.random()</div><div class=\"line\">                for k in range(TOPIC_NUM):</div><div class=\"line\">                    if(r&lt;=p_k[k]):</div><div class=\"line\">                        t = k</div><div class=\"line\">                        break</div><div class=\"line\">                d_w_t[d][w] = t</div><div class=\"line\">                t_c[t] += 1</div><div class=\"line\">                tw_c[t][w] += 1</div><div class=\"line\">                td_c[t][d] += 1</div><div class=\"line\"></div><div class=\"line\">def output():</div><div class=\"line\">    for d, w_L in d_w.items():</div><div class=\"line\">        for w in w_L:</div><div class=\"line\">            print &quot;%s\\t%s\\t%d&quot; % (d, w, d_w_t[d][w])</div><div class=\"line\"></div><div class=\"line\">if __name__ == &quot;__main__&quot;:</div><div class=\"line\">    input()</div><div class=\"line\">    print &quot;input end...&quot;</div><div class=\"line\">    init()</div><div class=\"line\">    print &quot;init end...&quot;</div><div class=\"line\">    sampling()</div><div class=\"line\">    print &quot;samplint end...&quot;</div><div class=\"line\">    output()</div><div class=\"line\">    print &quot;output end...&quot;</div></pre></td></tr></table></figure>\n<ul>\n<li><p>train corpus<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">doc1    枪      游戏    计算机  dota    电脑</div><div class=\"line\">doc4    娃娃    美丽    面膜    高跟鞋  裙子</div><div class=\"line\">doc5    购物    娃娃    裙子    SPA     指甲</div><div class=\"line\">doc2    枪      帅      电脑    坦克    飞机</div><div class=\"line\">doc3    游戏    坦克    飞机    数学    美丽</div><div class=\"line\">doc7    计算机  帅      枪      dota</div><div class=\"line\">doc6    美丽    购物    面膜    SPA     飘柔</div></pre></td></tr></table></figure>\n</li>\n<li><p>result<br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div></pre></td><td class=\"code\"><pre><div class=\"line\">doc2    枪      1</div><div class=\"line\">doc2    帅      1</div><div class=\"line\">doc2    电脑    1</div><div class=\"line\">doc2    坦克    1</div><div class=\"line\">doc2    飞机    1</div><div class=\"line\">doc3    游戏    1</div><div class=\"line\">doc3    坦克    1</div><div class=\"line\">doc3    飞机    1</div><div class=\"line\">doc3    数学    1</div><div class=\"line\">doc3    美丽    0</div><div class=\"line\">doc1    枪      1</div><div class=\"line\">doc1    游戏    1</div><div class=\"line\">doc1    计算机  1</div><div class=\"line\">doc1    dota    1</div><div class=\"line\">doc1    电脑    1</div><div class=\"line\">doc6    美丽    0</div><div class=\"line\">doc6    购物    0</div><div class=\"line\">doc6    面膜    0</div><div class=\"line\">doc6    SPA     0</div><div class=\"line\">doc6    飘柔    0</div><div class=\"line\">doc7    计算机  1</div><div class=\"line\">doc7    帅      1</div><div class=\"line\">doc7    枪      1</div><div class=\"line\">doc7    dota    1</div><div class=\"line\">doc4    娃娃    0</div><div class=\"line\">doc4    美丽    0</div><div class=\"line\">doc4    面膜    0</div><div class=\"line\">doc4    高跟鞋  0</div><div class=\"line\">doc4    裙子    0</div><div class=\"line\">doc5    购物    0</div><div class=\"line\">doc5    娃娃    0</div><div class=\"line\">doc5    裙子    0</div><div class=\"line\">doc5    SPA     0</div><div class=\"line\">doc5    指甲    0</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>写的样例默认有2个主题，一个是男生主题，一个是女生主题，lda的结果是可以把两个topic分开的。1-男生，0-女生。</p>"},{"title":"logit-n-probit","date":"2017-10-10T11:36:54.000Z","_content":"\n### logistic VS logit \n先上两幅logistic和logit的图\n\n<!-- more -->\n\n* logistic function\nsigmoid(x) = 1/(1+e^-x)\n{% asset_img \"logistic.png\" [logistic] %}\n\n* logit function\nlogit(x) = log(x/1-x)\n{% asset_img \"logit.png\" [logit] %}\n\n* logit和logistic的关系\nlogit和logistic互为反函数，如下：\n{% asset_img \"logit-logistic-relation.png\" [logit-logistic-relation] %}\n\n\n\n","source":"_posts/logit-n-probit.md","raw":"---\ntitle: logit-n-probit\ndate: 2017-10-10 19:36:54\ntags:\n---\n\n### logistic VS logit \n先上两幅logistic和logit的图\n\n<!-- more -->\n\n* logistic function\nsigmoid(x) = 1/(1+e^-x)\n{% asset_img \"logistic.png\" [logistic] %}\n\n* logit function\nlogit(x) = log(x/1-x)\n{% asset_img \"logit.png\" [logit] %}\n\n* logit和logistic的关系\nlogit和logistic互为反函数，如下：\n{% asset_img \"logit-logistic-relation.png\" [logit-logistic-relation] %}\n\n\n\n","slug":"logit-n-probit","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrzf000msi9gknyt6zk9","content":"<h3 id=\"logistic-VS-logit\"><a href=\"#logistic-VS-logit\" class=\"headerlink\" title=\"logistic VS logit\"></a>logistic VS logit</h3><p>先上两幅logistic和logit的图</p>\n<a id=\"more\"></a>\n<ul>\n<li><p>logistic function<br>sigmoid(x) = 1/(1+e^-x)</p>\n<img src=\"/2017/10/10/logit-n-probit/logistic.png\" alt=\"[logistic]\" title=\"[logistic]\">\n</li>\n<li><p>logit function<br>logit(x) = log(x/1-x)</p>\n<img src=\"/2017/10/10/logit-n-probit/logit.png\" alt=\"[logit]\" title=\"[logit]\">\n</li>\n<li><p>logit和logistic的关系<br>logit和logistic互为反函数，如下：</p>\n\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h3 id=\"logistic-VS-logit\"><a href=\"#logistic-VS-logit\" class=\"headerlink\" title=\"logistic VS logit\"></a>logistic VS logit</h3><p>先上两幅logistic和logit的图</p>","more":"<ul>\n<li><p>logistic function<br>sigmoid(x) = 1/(1+e^-x)</p>\n<img src=\"/2017/10/10/logit-n-probit/logistic.png\" alt=\"[logistic]\" title=\"[logistic]\">\n</li>\n<li><p>logit function<br>logit(x) = log(x/1-x)</p>\n<img src=\"/2017/10/10/logit-n-probit/logit.png\" alt=\"[logit]\" title=\"[logit]\">\n</li>\n<li><p>logit和logistic的关系<br>logit和logistic互为反函数，如下：</p>\n\n</li>\n</ul>"},{"title":"pr","date":"2017-10-15T07:40:41.000Z","_content":"\nPR有三个含义，差点儿搞晕了：\n* Public Relation: 公共关系，即公关\n* Peer Review: 同事评估，往往是代码的peer review, 代码的话常常code review(CR)\n* Pull Request: git上initiate discussion about your commits","source":"_posts/pr.md","raw":"---\ntitle: pr\ndate: 2017-10-15 15:40:41\ntags:\n---\n\nPR有三个含义，差点儿搞晕了：\n* Public Relation: 公共关系，即公关\n* Peer Review: 同事评估，往往是代码的peer review, 代码的话常常code review(CR)\n* Pull Request: git上initiate discussion about your commits","slug":"pr","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrzg000qsi9gilqvi1jc","content":"<p>PR有三个含义，差点儿搞晕了：</p>\n<ul>\n<li>Public Relation: 公共关系，即公关</li>\n<li>Peer Review: 同事评估，往往是代码的peer review, 代码的话常常code review(CR)</li>\n<li>Pull Request: git上initiate discussion about your commits</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>PR有三个含义，差点儿搞晕了：</p>\n<ul>\n<li>Public Relation: 公共关系，即公关</li>\n<li>Peer Review: 同事评估，往往是代码的peer review, 代码的话常常code review(CR)</li>\n<li>Pull Request: git上initiate discussion about your commits</li>\n</ul>\n"},{"title":"search","date":"2018-01-08T08:05:05.000Z","_content":"\n### 垂直领域搜索\nAPP搜索引擎，属于垂直领域的搜索引擎，相对于泛需求的搜索引擎会简单很多。\n泛搜索引擎 -> 意图识别（下APP，听歌，找wiki） -> 该意图垂直领域引擎 \n即在垂直领域，不需要挂载意图识别模块。\n\n<!-- more -->\n\n### 相关性(relevance)和重要性(importance)\n相关性：用户搜索“美”，京东和美团对比，美团的“相关性”更高，所以美团比京东排序高。\n重要性：用户搜索“美”，美团和美丽说对比，美团的“重要性”更高（美团用户量更大，美丽说小众），所以美团比美丽说的排序高。\n\n相关性和重要性，是搜索里需要tradeoff的两个指标，这里的tradeoff，往往是建立排序模型。\n即：相关性召回+重要性召回 -> 排序 \n\n### 相关性 \n#### 文本相似性\n* 标题精准匹配（Exact Match）\n用户搜索“美团”，大概率是对“APP名称的搜索”，因此按照名称精准匹配，最简单最有效。\n* 标题模糊匹配（Fuzzy Match）\n用户会打错字，用户会query打不全，因此模糊匹配非常重要。\n    * 拼音化\n用户搜索“meituan”，也能够出来“美团”的结果，是因为将app名称进行拼音化再匹配。\n    * query纠错/改写\n用户搜索“没团”，也能够出来“美团”的结果，是将query纠错后再匹配。\n    * 编辑距离\n用户搜索“美”，也能够出来“美团”的结果，是因为“美”和“美团”的编辑距离为1，比较小。\n【编辑距离ref】：https://zh.wikipedia.org/wiki/%E7%B7%A8%E8%BC%AF%E8%B7%9D%E9%9B%A2\n* 内容TF-IDF \n    * TF(Term Frequency)\nquery分词后，词语在内容（描述，评论）中出现的次数，可以使0-1值，可以是次数，可以是log（次数）等等任意变种。\n    * IDF(Inverse Document Frequency)\nquery分词后，词语在query中的重要性，可以是0-1值，可以是log(N/Nt)，可以是log(1+N/Nt)等等任意变种。\n    * 组合\nIDF将query分词后设置词的权重，TF将词去和文档匹配，TF-IDF就是加权的词和文档的匹配。\n【TF-IDF ref】https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n* 内容BM25\nBM25是一种TF-IDF-like retrieval functions，即原理相通。BM25假设有多个词，即sum(每个词的TF-IDF)。\n公式为：\nscore(D, Q) = sum(TF-IDF) D stands for doc, Q stands for query.\n\nTF: {% asset_img \"TF.png\" [TF] %}\n* f(qi, D) is qi's term frequency in the document D\n* |D| is the length of the document D in words\n* avgdl is the average document length in the text collection from which documents are drawn.\n* k1 and b are free parameters, usually chosen, in absence of an advanced optimization, as k1 in [1.2,2.0] and b = 0.75.\n\nIDF: {% asset_img \"IDF.png\" [IDF] %}\n* N is the total number of documents in the collection\n* n(qi) is the number of documents containing qi.\n\n【BM25 ref】https://en.wikipedia.org/wiki/Okapi_BM25\n\n#### 语义相似性\n* Topic Model\n    乔布斯的苹果和水果店的苹果，识别语义。建议采用”维基预料训练“，而非query来训练，因为LDA对短文本效果不好。\n* 类型相关\n    识别query的类型，比如搜索”聊天“，那就从”聊天软件“类目中召回APP。\n\n### 重要性\n#### 属性重要性 \n* 星级/评论挖掘\n* 曝光量/点击量/下载量/安装量/使用量/注册量/付费量/留存量（率）\n* 商业化价值\n\n#### 图重要性\n* Page Rank值\n\n#### 其他召回\n* x%用户下载召回\n* 新品召回\n* 运营召回\n* 个性化召回\n\n### 排序 \n* 点击熵\n    识别是精准需求（文本相关）还是泛需求（语义相关）的指标，可以用来确定召回比例，也可以用来作为排序特征（此时需要交叉两个召回来源）。\n* 条件概率 p(ctr|query, user, scene)\n给某人（对人的个性化），场景（对场景的把控，在机场推荐航班相关，晚饭时间推荐美食相关），query（用户对意图的主动描述）的最优化问题。\n\n### 评估（IR evaluation）\n* MAP: Mean Average Precision，不再赘述。\n* nDCG: Normalized Discounted cumulative gain，按照位置加权。\n\n【IR metrics ref】http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.htmll\n【MAP vs NDCG ref】https://www.youtube.com/watch?v=qm1In7NH8WE\n\n","source":"_posts/search.md","raw":"---\ntitle: search\ndate: 2018-01-08 16:05:05\ntags:\n---\n\n### 垂直领域搜索\nAPP搜索引擎，属于垂直领域的搜索引擎，相对于泛需求的搜索引擎会简单很多。\n泛搜索引擎 -> 意图识别（下APP，听歌，找wiki） -> 该意图垂直领域引擎 \n即在垂直领域，不需要挂载意图识别模块。\n\n<!-- more -->\n\n### 相关性(relevance)和重要性(importance)\n相关性：用户搜索“美”，京东和美团对比，美团的“相关性”更高，所以美团比京东排序高。\n重要性：用户搜索“美”，美团和美丽说对比，美团的“重要性”更高（美团用户量更大，美丽说小众），所以美团比美丽说的排序高。\n\n相关性和重要性，是搜索里需要tradeoff的两个指标，这里的tradeoff，往往是建立排序模型。\n即：相关性召回+重要性召回 -> 排序 \n\n### 相关性 \n#### 文本相似性\n* 标题精准匹配（Exact Match）\n用户搜索“美团”，大概率是对“APP名称的搜索”，因此按照名称精准匹配，最简单最有效。\n* 标题模糊匹配（Fuzzy Match）\n用户会打错字，用户会query打不全，因此模糊匹配非常重要。\n    * 拼音化\n用户搜索“meituan”，也能够出来“美团”的结果，是因为将app名称进行拼音化再匹配。\n    * query纠错/改写\n用户搜索“没团”，也能够出来“美团”的结果，是将query纠错后再匹配。\n    * 编辑距离\n用户搜索“美”，也能够出来“美团”的结果，是因为“美”和“美团”的编辑距离为1，比较小。\n【编辑距离ref】：https://zh.wikipedia.org/wiki/%E7%B7%A8%E8%BC%AF%E8%B7%9D%E9%9B%A2\n* 内容TF-IDF \n    * TF(Term Frequency)\nquery分词后，词语在内容（描述，评论）中出现的次数，可以使0-1值，可以是次数，可以是log（次数）等等任意变种。\n    * IDF(Inverse Document Frequency)\nquery分词后，词语在query中的重要性，可以是0-1值，可以是log(N/Nt)，可以是log(1+N/Nt)等等任意变种。\n    * 组合\nIDF将query分词后设置词的权重，TF将词去和文档匹配，TF-IDF就是加权的词和文档的匹配。\n【TF-IDF ref】https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n* 内容BM25\nBM25是一种TF-IDF-like retrieval functions，即原理相通。BM25假设有多个词，即sum(每个词的TF-IDF)。\n公式为：\nscore(D, Q) = sum(TF-IDF) D stands for doc, Q stands for query.\n\nTF: {% asset_img \"TF.png\" [TF] %}\n* f(qi, D) is qi's term frequency in the document D\n* |D| is the length of the document D in words\n* avgdl is the average document length in the text collection from which documents are drawn.\n* k1 and b are free parameters, usually chosen, in absence of an advanced optimization, as k1 in [1.2,2.0] and b = 0.75.\n\nIDF: {% asset_img \"IDF.png\" [IDF] %}\n* N is the total number of documents in the collection\n* n(qi) is the number of documents containing qi.\n\n【BM25 ref】https://en.wikipedia.org/wiki/Okapi_BM25\n\n#### 语义相似性\n* Topic Model\n    乔布斯的苹果和水果店的苹果，识别语义。建议采用”维基预料训练“，而非query来训练，因为LDA对短文本效果不好。\n* 类型相关\n    识别query的类型，比如搜索”聊天“，那就从”聊天软件“类目中召回APP。\n\n### 重要性\n#### 属性重要性 \n* 星级/评论挖掘\n* 曝光量/点击量/下载量/安装量/使用量/注册量/付费量/留存量（率）\n* 商业化价值\n\n#### 图重要性\n* Page Rank值\n\n#### 其他召回\n* x%用户下载召回\n* 新品召回\n* 运营召回\n* 个性化召回\n\n### 排序 \n* 点击熵\n    识别是精准需求（文本相关）还是泛需求（语义相关）的指标，可以用来确定召回比例，也可以用来作为排序特征（此时需要交叉两个召回来源）。\n* 条件概率 p(ctr|query, user, scene)\n给某人（对人的个性化），场景（对场景的把控，在机场推荐航班相关，晚饭时间推荐美食相关），query（用户对意图的主动描述）的最优化问题。\n\n### 评估（IR evaluation）\n* MAP: Mean Average Precision，不再赘述。\n* nDCG: Normalized Discounted cumulative gain，按照位置加权。\n\n【IR metrics ref】http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.htmll\n【MAP vs NDCG ref】https://www.youtube.com/watch?v=qm1In7NH8WE\n\n","slug":"search","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrzh000rsi9gft9wq43e","content":"<h3 id=\"垂直领域搜索\"><a href=\"#垂直领域搜索\" class=\"headerlink\" title=\"垂直领域搜索\"></a>垂直领域搜索</h3><p>APP搜索引擎，属于垂直领域的搜索引擎，相对于泛需求的搜索引擎会简单很多。<br>泛搜索引擎 -&gt; 意图识别（下APP，听歌，找wiki） -&gt; 该意图垂直领域引擎<br>即在垂直领域，不需要挂载意图识别模块。</p>\n<a id=\"more\"></a>\n<h3 id=\"相关性-relevance-和重要性-importance\"><a href=\"#相关性-relevance-和重要性-importance\" class=\"headerlink\" title=\"相关性(relevance)和重要性(importance)\"></a>相关性(relevance)和重要性(importance)</h3><p>相关性：用户搜索“美”，京东和美团对比，美团的“相关性”更高，所以美团比京东排序高。<br>重要性：用户搜索“美”，美团和美丽说对比，美团的“重要性”更高（美团用户量更大，美丽说小众），所以美团比美丽说的排序高。</p>\n<p>相关性和重要性，是搜索里需要tradeoff的两个指标，这里的tradeoff，往往是建立排序模型。<br>即：相关性召回+重要性召回 -&gt; 排序 </p>\n<h3 id=\"相关性\"><a href=\"#相关性\" class=\"headerlink\" title=\"相关性\"></a>相关性</h3><h4 id=\"文本相似性\"><a href=\"#文本相似性\" class=\"headerlink\" title=\"文本相似性\"></a>文本相似性</h4><ul>\n<li>标题精准匹配（Exact Match）<br>用户搜索“美团”，大概率是对“APP名称的搜索”，因此按照名称精准匹配，最简单最有效。</li>\n<li>标题模糊匹配（Fuzzy Match）<br>用户会打错字，用户会query打不全，因此模糊匹配非常重要。<ul>\n<li>拼音化<br>用户搜索“meituan”，也能够出来“美团”的结果，是因为将app名称进行拼音化再匹配。</li>\n<li>query纠错/改写<br>用户搜索“没团”，也能够出来“美团”的结果，是将query纠错后再匹配。</li>\n<li>编辑距离<br>用户搜索“美”，也能够出来“美团”的结果，是因为“美”和“美团”的编辑距离为1，比较小。<br>【编辑距离ref】：<a href=\"https://zh.wikipedia.org/wiki/%E7%B7%A8%E8%BC%AF%E8%B7%9D%E9%9B%A2\" target=\"_blank\" rel=\"external\">https://zh.wikipedia.org/wiki/%E7%B7%A8%E8%BC%AF%E8%B7%9D%E9%9B%A2</a></li>\n</ul>\n</li>\n<li>内容TF-IDF <ul>\n<li>TF(Term Frequency)<br>query分词后，词语在内容（描述，评论）中出现的次数，可以使0-1值，可以是次数，可以是log（次数）等等任意变种。</li>\n<li>IDF(Inverse Document Frequency)<br>query分词后，词语在query中的重要性，可以是0-1值，可以是log(N/Nt)，可以是log(1+N/Nt)等等任意变种。</li>\n<li>组合<br>IDF将query分词后设置词的权重，TF将词去和文档匹配，TF-IDF就是加权的词和文档的匹配。<br>【TF-IDF ref】<a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a></li>\n</ul>\n</li>\n<li>内容BM25<br>BM25是一种TF-IDF-like retrieval functions，即原理相通。BM25假设有多个词，即sum(每个词的TF-IDF)。<br>公式为：<br>score(D, Q) = sum(TF-IDF) D stands for doc, Q stands for query.</li>\n</ul>\n<p>TF: <img src=\"/2018/01/08/search/TF.png\" alt=\"[TF]\" title=\"[TF]\"></p>\n<ul>\n<li>f(qi, D) is qi’s term frequency in the document D</li>\n<li>|D| is the length of the document D in words</li>\n<li>avgdl is the average document length in the text collection from which documents are drawn.</li>\n<li>k1 and b are free parameters, usually chosen, in absence of an advanced optimization, as k1 in [1.2,2.0] and b = 0.75.</li>\n</ul>\n<p>IDF: <img src=\"/2018/01/08/search/IDF.png\" alt=\"[IDF]\" title=\"[IDF]\"></p>\n<ul>\n<li>N is the total number of documents in the collection</li>\n<li>n(qi) is the number of documents containing qi.</li>\n</ul>\n<p>【BM25 ref】<a href=\"https://en.wikipedia.org/wiki/Okapi_BM25\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Okapi_BM25</a></p>\n<h4 id=\"语义相似性\"><a href=\"#语义相似性\" class=\"headerlink\" title=\"语义相似性\"></a>语义相似性</h4><ul>\n<li>Topic Model<br>  乔布斯的苹果和水果店的苹果，识别语义。建议采用”维基预料训练“，而非query来训练，因为LDA对短文本效果不好。</li>\n<li>类型相关<br>  识别query的类型，比如搜索”聊天“，那就从”聊天软件“类目中召回APP。</li>\n</ul>\n<h3 id=\"重要性\"><a href=\"#重要性\" class=\"headerlink\" title=\"重要性\"></a>重要性</h3><h4 id=\"属性重要性\"><a href=\"#属性重要性\" class=\"headerlink\" title=\"属性重要性\"></a>属性重要性</h4><ul>\n<li>星级/评论挖掘</li>\n<li>曝光量/点击量/下载量/安装量/使用量/注册量/付费量/留存量（率）</li>\n<li>商业化价值</li>\n</ul>\n<h4 id=\"图重要性\"><a href=\"#图重要性\" class=\"headerlink\" title=\"图重要性\"></a>图重要性</h4><ul>\n<li>Page Rank值</li>\n</ul>\n<h4 id=\"其他召回\"><a href=\"#其他召回\" class=\"headerlink\" title=\"其他召回\"></a>其他召回</h4><ul>\n<li>x%用户下载召回</li>\n<li>新品召回</li>\n<li>运营召回</li>\n<li>个性化召回</li>\n</ul>\n<h3 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h3><ul>\n<li>点击熵<br>  识别是精准需求（文本相关）还是泛需求（语义相关）的指标，可以用来确定召回比例，也可以用来作为排序特征（此时需要交叉两个召回来源）。</li>\n<li>条件概率 p(ctr|query, user, scene)<br>给某人（对人的个性化），场景（对场景的把控，在机场推荐航班相关，晚饭时间推荐美食相关），query（用户对意图的主动描述）的最优化问题。</li>\n</ul>\n<h3 id=\"评估（IR-evaluation）\"><a href=\"#评估（IR-evaluation）\" class=\"headerlink\" title=\"评估（IR evaluation）\"></a>评估（IR evaluation）</h3><ul>\n<li>MAP: Mean Average Precision，不再赘述。</li>\n<li>nDCG: Normalized Discounted cumulative gain，按照位置加权。</li>\n</ul>\n<p>【IR metrics ref】<a href=\"http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.htmll\" target=\"_blank\" rel=\"external\">http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.htmll</a><br>【MAP vs NDCG ref】<a href=\"https://www.youtube.com/watch?v=qm1In7NH8WE\" target=\"_blank\" rel=\"external\">https://www.youtube.com/watch?v=qm1In7NH8WE</a></p>\n","site":{"data":{}},"excerpt":"<h3 id=\"垂直领域搜索\"><a href=\"#垂直领域搜索\" class=\"headerlink\" title=\"垂直领域搜索\"></a>垂直领域搜索</h3><p>APP搜索引擎，属于垂直领域的搜索引擎，相对于泛需求的搜索引擎会简单很多。<br>泛搜索引擎 -&gt; 意图识别（下APP，听歌，找wiki） -&gt; 该意图垂直领域引擎<br>即在垂直领域，不需要挂载意图识别模块。</p>","more":"<h3 id=\"相关性-relevance-和重要性-importance\"><a href=\"#相关性-relevance-和重要性-importance\" class=\"headerlink\" title=\"相关性(relevance)和重要性(importance)\"></a>相关性(relevance)和重要性(importance)</h3><p>相关性：用户搜索“美”，京东和美团对比，美团的“相关性”更高，所以美团比京东排序高。<br>重要性：用户搜索“美”，美团和美丽说对比，美团的“重要性”更高（美团用户量更大，美丽说小众），所以美团比美丽说的排序高。</p>\n<p>相关性和重要性，是搜索里需要tradeoff的两个指标，这里的tradeoff，往往是建立排序模型。<br>即：相关性召回+重要性召回 -&gt; 排序 </p>\n<h3 id=\"相关性\"><a href=\"#相关性\" class=\"headerlink\" title=\"相关性\"></a>相关性</h3><h4 id=\"文本相似性\"><a href=\"#文本相似性\" class=\"headerlink\" title=\"文本相似性\"></a>文本相似性</h4><ul>\n<li>标题精准匹配（Exact Match）<br>用户搜索“美团”，大概率是对“APP名称的搜索”，因此按照名称精准匹配，最简单最有效。</li>\n<li>标题模糊匹配（Fuzzy Match）<br>用户会打错字，用户会query打不全，因此模糊匹配非常重要。<ul>\n<li>拼音化<br>用户搜索“meituan”，也能够出来“美团”的结果，是因为将app名称进行拼音化再匹配。</li>\n<li>query纠错/改写<br>用户搜索“没团”，也能够出来“美团”的结果，是将query纠错后再匹配。</li>\n<li>编辑距离<br>用户搜索“美”，也能够出来“美团”的结果，是因为“美”和“美团”的编辑距离为1，比较小。<br>【编辑距离ref】：<a href=\"https://zh.wikipedia.org/wiki/%E7%B7%A8%E8%BC%AF%E8%B7%9D%E9%9B%A2\" target=\"_blank\" rel=\"external\">https://zh.wikipedia.org/wiki/%E7%B7%A8%E8%BC%AF%E8%B7%9D%E9%9B%A2</a></li>\n</ul>\n</li>\n<li>内容TF-IDF <ul>\n<li>TF(Term Frequency)<br>query分词后，词语在内容（描述，评论）中出现的次数，可以使0-1值，可以是次数，可以是log（次数）等等任意变种。</li>\n<li>IDF(Inverse Document Frequency)<br>query分词后，词语在query中的重要性，可以是0-1值，可以是log(N/Nt)，可以是log(1+N/Nt)等等任意变种。</li>\n<li>组合<br>IDF将query分词后设置词的权重，TF将词去和文档匹配，TF-IDF就是加权的词和文档的匹配。<br>【TF-IDF ref】<a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a></li>\n</ul>\n</li>\n<li>内容BM25<br>BM25是一种TF-IDF-like retrieval functions，即原理相通。BM25假设有多个词，即sum(每个词的TF-IDF)。<br>公式为：<br>score(D, Q) = sum(TF-IDF) D stands for doc, Q stands for query.</li>\n</ul>\n<p>TF: <img src=\"/2018/01/08/search/TF.png\" alt=\"[TF]\" title=\"[TF]\"></p>\n<ul>\n<li>f(qi, D) is qi’s term frequency in the document D</li>\n<li>|D| is the length of the document D in words</li>\n<li>avgdl is the average document length in the text collection from which documents are drawn.</li>\n<li>k1 and b are free parameters, usually chosen, in absence of an advanced optimization, as k1 in [1.2,2.0] and b = 0.75.</li>\n</ul>\n<p>IDF: <img src=\"/2018/01/08/search/IDF.png\" alt=\"[IDF]\" title=\"[IDF]\"></p>\n<ul>\n<li>N is the total number of documents in the collection</li>\n<li>n(qi) is the number of documents containing qi.</li>\n</ul>\n<p>【BM25 ref】<a href=\"https://en.wikipedia.org/wiki/Okapi_BM25\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/Okapi_BM25</a></p>\n<h4 id=\"语义相似性\"><a href=\"#语义相似性\" class=\"headerlink\" title=\"语义相似性\"></a>语义相似性</h4><ul>\n<li>Topic Model<br>  乔布斯的苹果和水果店的苹果，识别语义。建议采用”维基预料训练“，而非query来训练，因为LDA对短文本效果不好。</li>\n<li>类型相关<br>  识别query的类型，比如搜索”聊天“，那就从”聊天软件“类目中召回APP。</li>\n</ul>\n<h3 id=\"重要性\"><a href=\"#重要性\" class=\"headerlink\" title=\"重要性\"></a>重要性</h3><h4 id=\"属性重要性\"><a href=\"#属性重要性\" class=\"headerlink\" title=\"属性重要性\"></a>属性重要性</h4><ul>\n<li>星级/评论挖掘</li>\n<li>曝光量/点击量/下载量/安装量/使用量/注册量/付费量/留存量（率）</li>\n<li>商业化价值</li>\n</ul>\n<h4 id=\"图重要性\"><a href=\"#图重要性\" class=\"headerlink\" title=\"图重要性\"></a>图重要性</h4><ul>\n<li>Page Rank值</li>\n</ul>\n<h4 id=\"其他召回\"><a href=\"#其他召回\" class=\"headerlink\" title=\"其他召回\"></a>其他召回</h4><ul>\n<li>x%用户下载召回</li>\n<li>新品召回</li>\n<li>运营召回</li>\n<li>个性化召回</li>\n</ul>\n<h3 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h3><ul>\n<li>点击熵<br>  识别是精准需求（文本相关）还是泛需求（语义相关）的指标，可以用来确定召回比例，也可以用来作为排序特征（此时需要交叉两个召回来源）。</li>\n<li>条件概率 p(ctr|query, user, scene)<br>给某人（对人的个性化），场景（对场景的把控，在机场推荐航班相关，晚饭时间推荐美食相关），query（用户对意图的主动描述）的最优化问题。</li>\n</ul>\n<h3 id=\"评估（IR-evaluation）\"><a href=\"#评估（IR-evaluation）\" class=\"headerlink\" title=\"评估（IR evaluation）\"></a>评估（IR evaluation）</h3><ul>\n<li>MAP: Mean Average Precision，不再赘述。</li>\n<li>nDCG: Normalized Discounted cumulative gain，按照位置加权。</li>\n</ul>\n<p>【IR metrics ref】<a href=\"http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.htmll\" target=\"_blank\" rel=\"external\">http://lixinzhang.github.io/xin-xi-jian-suo-zhong-de-ping-jie-zhi-biao-maphe-ndcg.htmll</a><br>【MAP vs NDCG ref】<a href=\"https://www.youtube.com/watch?v=qm1In7NH8WE\" target=\"_blank\" rel=\"external\">https://www.youtube.com/watch?v=qm1In7NH8WE</a></p>"},{"title":"spark-streaming","date":"2017-11-17T06:38:36.000Z","_content":"\n### spark streaming k-means\n* decay(forgetfulness)\n* mini-batch k-means\n    * c_t+1 = [(c_t * n_t * a) + (x_t * m_t)] / [n_t + m_t]\n    * n_t+t = n_t * a + m_t\n\n\n### Broadcast Variables\n\n* ref\nhttps://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html","source":"_posts/spark-streaming.md","raw":"---\ntitle: spark-streaming\ndate: 2017-11-17 14:38:36\ntags:\n---\n\n### spark streaming k-means\n* decay(forgetfulness)\n* mini-batch k-means\n    * c_t+1 = [(c_t * n_t * a) + (x_t * m_t)] / [n_t + m_t]\n    * n_t+t = n_t * a + m_t\n\n\n### Broadcast Variables\n\n* ref\nhttps://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html","slug":"spark-streaming","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrzi000ssi9gmqkydi15","content":"<h3 id=\"spark-streaming-k-means\"><a href=\"#spark-streaming-k-means\" class=\"headerlink\" title=\"spark streaming k-means\"></a>spark streaming k-means</h3><ul>\n<li>decay(forgetfulness)</li>\n<li>mini-batch k-means<ul>\n<li>c_t+1 = [(c_t <em> n_t </em> a) + (x_t * m_t)] / [n_t + m_t]</li>\n<li>n_t+t = n_t * a + m_t</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Broadcast-Variables\"><a href=\"#Broadcast-Variables\" class=\"headerlink\" title=\"Broadcast Variables\"></a>Broadcast Variables</h3><ul>\n<li>ref<br><a href=\"https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\" target=\"_blank\" rel=\"external\">https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"spark-streaming-k-means\"><a href=\"#spark-streaming-k-means\" class=\"headerlink\" title=\"spark streaming k-means\"></a>spark streaming k-means</h3><ul>\n<li>decay(forgetfulness)</li>\n<li>mini-batch k-means<ul>\n<li>c_t+1 = [(c_t <em> n_t </em> a) + (x_t * m_t)] / [n_t + m_t]</li>\n<li>n_t+t = n_t * a + m_t</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Broadcast-Variables\"><a href=\"#Broadcast-Variables\" class=\"headerlink\" title=\"Broadcast Variables\"></a>Broadcast Variables</h3><ul>\n<li>ref<br><a href=\"https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html\" target=\"_blank\" rel=\"external\">https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html</a></li>\n</ul>\n"},{"title":"做地铁和囚徒困境","date":"2017-10-26T02:06:09.000Z","_content":"早上做地铁，很挤，发现进地铁门，是个囚徒困境。\n* 环境\n    * 进地铁门，可以从左边上，可以从右边上\n    * 出地铁门，可以从左边下，右边下（标志是中间下，但门不大，不可能同时两边上中间下 ，尴尬）\n* 困境\n    * 两边同时上：因车上人要下来，所以谁都上不去\n    * 一边上一遍下：上去了的开心，下去了的开心，上不去的一边尴尬\n    * 两边先都不上：车上人迅速下完，开开心心上车\n* 结论\n    * 社会在惩罚遵守规则者\n    * 建议地铁门做大点儿","source":"_posts/subway.md","raw":"---\ntitle: 做地铁和囚徒困境\ndate: 2017-10-26 10:06:09\ntags:\n---\n早上做地铁，很挤，发现进地铁门，是个囚徒困境。\n* 环境\n    * 进地铁门，可以从左边上，可以从右边上\n    * 出地铁门，可以从左边下，右边下（标志是中间下，但门不大，不可能同时两边上中间下 ，尴尬）\n* 困境\n    * 两边同时上：因车上人要下来，所以谁都上不去\n    * 一边上一遍下：上去了的开心，下去了的开心，上不去的一边尴尬\n    * 两边先都不上：车上人迅速下完，开开心心上车\n* 结论\n    * 社会在惩罚遵守规则者\n    * 建议地铁门做大点儿","slug":"subway","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrzk000tsi9gkrys3xfx","content":"<p>早上做地铁，很挤，发现进地铁门，是个囚徒困境。</p>\n<ul>\n<li>环境<ul>\n<li>进地铁门，可以从左边上，可以从右边上</li>\n<li>出地铁门，可以从左边下，右边下（标志是中间下，但门不大，不可能同时两边上中间下 ，尴尬）</li>\n</ul>\n</li>\n<li>困境<ul>\n<li>两边同时上：因车上人要下来，所以谁都上不去</li>\n<li>一边上一遍下：上去了的开心，下去了的开心，上不去的一边尴尬</li>\n<li>两边先都不上：车上人迅速下完，开开心心上车</li>\n</ul>\n</li>\n<li>结论<ul>\n<li>社会在惩罚遵守规则者</li>\n<li>建议地铁门做大点儿</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>早上做地铁，很挤，发现进地铁门，是个囚徒困境。</p>\n<ul>\n<li>环境<ul>\n<li>进地铁门，可以从左边上，可以从右边上</li>\n<li>出地铁门，可以从左边下，右边下（标志是中间下，但门不大，不可能同时两边上中间下 ，尴尬）</li>\n</ul>\n</li>\n<li>困境<ul>\n<li>两边同时上：因车上人要下来，所以谁都上不去</li>\n<li>一边上一遍下：上去了的开心，下去了的开心，上不去的一边尴尬</li>\n<li>两边先都不上：车上人迅速下完，开开心心上车</li>\n</ul>\n</li>\n<li>结论<ul>\n<li>社会在惩罚遵守规则者</li>\n<li>建议地铁门做大点儿</li>\n</ul>\n</li>\n</ul>\n"},{"title":"tf_wnd","date":"2018-01-05T03:46:39.000Z","_content":"\n#### wide_n_deep code\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport logging\nimport json\nimport math\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\n# 读取文件\nreader = tf.TextLineReader(skip_header_lines = 0)\n\n# 文件列表\ntrain_input_files = [\"/root/xpguo/wnd/1.txt\", \"/root/xpguo/wnd/2.txt\"]\n\ninput_file_list = []\nfor input_file in train_input_files:\n    if len(input_file) > 0:\n        input_file_list.append(tf.train.match_filenames_once(input_file))\n\nfilename_queue = tf.train.string_input_producer(\n                tf.concat(input_file_list, axis = 0),\n                num_epochs = 10,     # strings are repeated num_epochs\n                shuffle = True,     # strings are randomly shuffled within each epoch\n                capacity = 512)\n\nbatch_size = 3\n\n(_, records) = reader.read_up_to(filename_queue, num_records = batch_size)\nsamples = tf.decode_csv(records, record_defaults = column_defaults, field_delim = ',')\nlabel = tf.cast(samples[self.column_dict[\"label\"]], dtype = tf.int32)\nfeature_dict = {}\nfor (key, value) in self.column_dict.items():\n    if key == \"label\" or value < 0 or value >= len(samples):\n        continue\n    if key in [\"user_features\", \"ads_features\"]:\n        feature_dict[key] = tf.string_split(samples[value], delimiter = ';')\n    if key in [\"user_weights\", \"ads_weights\"]:\n        feature_dict[key] = self.string_to_number(\n                tf.string_split(samples[value], delimiter = ';'),\n                dtype = tf.float32)\nreturn feature_dict, label\n```\n\n```\n\n```","source":"_posts/tf-wnd.md","raw":"---\ntitle: tf_wnd\ndate: 2018-01-05 11:46:39\ntags:\n---\n\n#### wide_n_deep code\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport logging\nimport json\nimport math\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\n# 读取文件\nreader = tf.TextLineReader(skip_header_lines = 0)\n\n# 文件列表\ntrain_input_files = [\"/root/xpguo/wnd/1.txt\", \"/root/xpguo/wnd/2.txt\"]\n\ninput_file_list = []\nfor input_file in train_input_files:\n    if len(input_file) > 0:\n        input_file_list.append(tf.train.match_filenames_once(input_file))\n\nfilename_queue = tf.train.string_input_producer(\n                tf.concat(input_file_list, axis = 0),\n                num_epochs = 10,     # strings are repeated num_epochs\n                shuffle = True,     # strings are randomly shuffled within each epoch\n                capacity = 512)\n\nbatch_size = 3\n\n(_, records) = reader.read_up_to(filename_queue, num_records = batch_size)\nsamples = tf.decode_csv(records, record_defaults = column_defaults, field_delim = ',')\nlabel = tf.cast(samples[self.column_dict[\"label\"]], dtype = tf.int32)\nfeature_dict = {}\nfor (key, value) in self.column_dict.items():\n    if key == \"label\" or value < 0 or value >= len(samples):\n        continue\n    if key in [\"user_features\", \"ads_features\"]:\n        feature_dict[key] = tf.string_split(samples[value], delimiter = ';')\n    if key in [\"user_weights\", \"ads_weights\"]:\n        feature_dict[key] = self.string_to_number(\n                tf.string_split(samples[value], delimiter = ';'),\n                dtype = tf.float32)\nreturn feature_dict, label\n```\n\n```\n\n```","slug":"tf-wnd","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrzm000usi9gt6jtsh8k","content":"<h4 id=\"wide-n-deep-code\"><a href=\"#wide-n-deep-code\" class=\"headerlink\" title=\"wide_n_deep code\"></a>wide_n_deep code</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div></pre></td><td class=\"code\"><pre><div class=\"line\">from __future__ import absolute_import</div><div class=\"line\">from __future__ import division</div><div class=\"line\">from __future__ import print_function</div><div class=\"line\"></div><div class=\"line\">import sys</div><div class=\"line\">import os</div><div class=\"line\">import logging</div><div class=\"line\">import json</div><div class=\"line\">import math</div><div class=\"line\"></div><div class=\"line\">from six.moves import urllib</div><div class=\"line\">import tensorflow as tf</div><div class=\"line\"></div><div class=\"line\"># 读取文件</div><div class=\"line\">reader = tf.TextLineReader(skip_header_lines = 0)</div><div class=\"line\"></div><div class=\"line\"># 文件列表</div><div class=\"line\">train_input_files = [&quot;/root/xpguo/wnd/1.txt&quot;, &quot;/root/xpguo/wnd/2.txt&quot;]</div><div class=\"line\"></div><div class=\"line\">input_file_list = []</div><div class=\"line\">for input_file in train_input_files:</div><div class=\"line\">    if len(input_file) &gt; 0:</div><div class=\"line\">        input_file_list.append(tf.train.match_filenames_once(input_file))</div><div class=\"line\"></div><div class=\"line\">filename_queue = tf.train.string_input_producer(</div><div class=\"line\">                tf.concat(input_file_list, axis = 0),</div><div class=\"line\">                num_epochs = 10,     # strings are repeated num_epochs</div><div class=\"line\">                shuffle = True,     # strings are randomly shuffled within each epoch</div><div class=\"line\">                capacity = 512)</div><div class=\"line\"></div><div class=\"line\">batch_size = 3</div><div class=\"line\"></div><div class=\"line\">(_, records) = reader.read_up_to(filename_queue, num_records = batch_size)</div><div class=\"line\">samples = tf.decode_csv(records, record_defaults = column_defaults, field_delim = &apos;,&apos;)</div><div class=\"line\">label = tf.cast(samples[self.column_dict[&quot;label&quot;]], dtype = tf.int32)</div><div class=\"line\">feature_dict = &#123;&#125;</div><div class=\"line\">for (key, value) in self.column_dict.items():</div><div class=\"line\">    if key == &quot;label&quot; or value &lt; 0 or value &gt;= len(samples):</div><div class=\"line\">        continue</div><div class=\"line\">    if key in [&quot;user_features&quot;, &quot;ads_features&quot;]:</div><div class=\"line\">        feature_dict[key] = tf.string_split(samples[value], delimiter = &apos;;&apos;)</div><div class=\"line\">    if key in [&quot;user_weights&quot;, &quot;ads_weights&quot;]:</div><div class=\"line\">        feature_dict[key] = self.string_to_number(</div><div class=\"line\">                tf.string_split(samples[value], delimiter = &apos;;&apos;),</div><div class=\"line\">                dtype = tf.float32)</div><div class=\"line\">return feature_dict, label</div></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"></div></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h4 id=\"wide-n-deep-code\"><a href=\"#wide-n-deep-code\" class=\"headerlink\" title=\"wide_n_deep code\"></a>wide_n_deep code</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div></pre></td><td class=\"code\"><pre><div class=\"line\">from __future__ import absolute_import</div><div class=\"line\">from __future__ import division</div><div class=\"line\">from __future__ import print_function</div><div class=\"line\"></div><div class=\"line\">import sys</div><div class=\"line\">import os</div><div class=\"line\">import logging</div><div class=\"line\">import json</div><div class=\"line\">import math</div><div class=\"line\"></div><div class=\"line\">from six.moves import urllib</div><div class=\"line\">import tensorflow as tf</div><div class=\"line\"></div><div class=\"line\"># 读取文件</div><div class=\"line\">reader = tf.TextLineReader(skip_header_lines = 0)</div><div class=\"line\"></div><div class=\"line\"># 文件列表</div><div class=\"line\">train_input_files = [&quot;/root/xpguo/wnd/1.txt&quot;, &quot;/root/xpguo/wnd/2.txt&quot;]</div><div class=\"line\"></div><div class=\"line\">input_file_list = []</div><div class=\"line\">for input_file in train_input_files:</div><div class=\"line\">    if len(input_file) &gt; 0:</div><div class=\"line\">        input_file_list.append(tf.train.match_filenames_once(input_file))</div><div class=\"line\"></div><div class=\"line\">filename_queue = tf.train.string_input_producer(</div><div class=\"line\">                tf.concat(input_file_list, axis = 0),</div><div class=\"line\">                num_epochs = 10,     # strings are repeated num_epochs</div><div class=\"line\">                shuffle = True,     # strings are randomly shuffled within each epoch</div><div class=\"line\">                capacity = 512)</div><div class=\"line\"></div><div class=\"line\">batch_size = 3</div><div class=\"line\"></div><div class=\"line\">(_, records) = reader.read_up_to(filename_queue, num_records = batch_size)</div><div class=\"line\">samples = tf.decode_csv(records, record_defaults = column_defaults, field_delim = &apos;,&apos;)</div><div class=\"line\">label = tf.cast(samples[self.column_dict[&quot;label&quot;]], dtype = tf.int32)</div><div class=\"line\">feature_dict = &#123;&#125;</div><div class=\"line\">for (key, value) in self.column_dict.items():</div><div class=\"line\">    if key == &quot;label&quot; or value &lt; 0 or value &gt;= len(samples):</div><div class=\"line\">        continue</div><div class=\"line\">    if key in [&quot;user_features&quot;, &quot;ads_features&quot;]:</div><div class=\"line\">        feature_dict[key] = tf.string_split(samples[value], delimiter = &apos;;&apos;)</div><div class=\"line\">    if key in [&quot;user_weights&quot;, &quot;ads_weights&quot;]:</div><div class=\"line\">        feature_dict[key] = self.string_to_number(</div><div class=\"line\">                tf.string_split(samples[value], delimiter = &apos;;&apos;),</div><div class=\"line\">                dtype = tf.float32)</div><div class=\"line\">return feature_dict, label</div></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"></div></pre></td></tr></table></figure>"},{"title":"xgboost-n-spark","date":"2017-10-28T06:30:04.000Z","_content":"\n### xgboost\n* model: GBDT/GBRT/GBM\n* language: Python, R, Java, Scala, C++\n* integrity: single machine, Hadoop, Spark, Flink, DataFlow\n\n### xgboost和gbdt的区别\n>The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms.\nWhich is the reason why many people use xgboost. \nFor model, it might be more suitable to be called as regularized gradient boosting.\n\n### xgboost","source":"_posts/xgboost-n-spark.md","raw":"---\ntitle: xgboost-n-spark\ndate: 2017-10-28 14:30:04\ntags:\n---\n\n### xgboost\n* model: GBDT/GBRT/GBM\n* language: Python, R, Java, Scala, C++\n* integrity: single machine, Hadoop, Spark, Flink, DataFlow\n\n### xgboost和gbdt的区别\n>The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms.\nWhich is the reason why many people use xgboost. \nFor model, it might be more suitable to be called as regularized gradient boosting.\n\n### xgboost","slug":"xgboost-n-spark","published":1,"updated":"2018-03-15T14:44:51.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjex1jrzn000wsi9gh2ef64u2","content":"<h3 id=\"xgboost\"><a href=\"#xgboost\" class=\"headerlink\" title=\"xgboost\"></a>xgboost</h3><ul>\n<li>model: GBDT/GBRT/GBM</li>\n<li>language: Python, R, Java, Scala, C++</li>\n<li>integrity: single machine, Hadoop, Spark, Flink, DataFlow</li>\n</ul>\n<h3 id=\"xgboost和gbdt的区别\"><a href=\"#xgboost和gbdt的区别\" class=\"headerlink\" title=\"xgboost和gbdt的区别\"></a>xgboost和gbdt的区别</h3><blockquote>\n<p>The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms.<br>Which is the reason why many people use xgboost.<br>For model, it might be more suitable to be called as regularized gradient boosting.</p>\n</blockquote>\n<h3 id=\"xgboost-1\"><a href=\"#xgboost-1\" class=\"headerlink\" title=\"xgboost\"></a>xgboost</h3>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"xgboost\"><a href=\"#xgboost\" class=\"headerlink\" title=\"xgboost\"></a>xgboost</h3><ul>\n<li>model: GBDT/GBRT/GBM</li>\n<li>language: Python, R, Java, Scala, C++</li>\n<li>integrity: single machine, Hadoop, Spark, Flink, DataFlow</li>\n</ul>\n<h3 id=\"xgboost和gbdt的区别\"><a href=\"#xgboost和gbdt的区别\" class=\"headerlink\" title=\"xgboost和gbdt的区别\"></a>xgboost和gbdt的区别</h3><blockquote>\n<p>The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms.<br>Which is the reason why many people use xgboost.<br>For model, it might be more suitable to be called as regularized gradient boosting.</p>\n</blockquote>\n<h3 id=\"xgboost-1\"><a href=\"#xgboost-1\" class=\"headerlink\" title=\"xgboost\"></a>xgboost</h3>"}],"PostAsset":[{"_id":"source/_posts/auc-n-logloss/3.png","slug":"3.png","post":"cjex1jryh0003si9gibtjyf42","modified":1,"renderable":0},{"_id":"source/_posts/cnn/lenet.png","slug":"lenet.png","post":"cjex1jryw0007si9gqihcx2ap","modified":1,"renderable":0},{"_id":"source/_posts/auc-n-logloss/1.gif","slug":"1.gif","post":"cjex1jryh0003si9gibtjyf42","modified":1,"renderable":0},{"_id":"source/_posts/cnn/alexNet.png","slug":"alexNet.png","post":"cjex1jryw0007si9gqihcx2ap","modified":1,"renderable":0},{"_id":"source/_posts/lda/302.png","slug":"302.png","post":"cjex1jrze000lsi9grm9b9qvt","modified":1,"renderable":0},{"_id":"source/_posts/feature-engineer/զ%A6ڥ%E4%FE%F6+%B5%AB%C1.png","post":"cjex1jrzc000isi9g8e3m8jgr","slug":"զ%A6ڥ%E4%FE%F6+%B5%AB%C1.png","modified":1,"renderable":1},{"_id":"source/_posts/ee-n-dqn/1.png","post":"cjex1jrza000gsi9g7irg4z99","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_posts/ee-n-dqn/2.png","post":"cjex1jrza000gsi9g7irg4z99","slug":"2.png","modified":1,"renderable":1},{"_id":"source/_posts/search/IDF.png","post":"cjex1jrzh000rsi9gft9wq43e","slug":"IDF.png","modified":1,"renderable":1},{"_id":"source/_posts/search/TF.png","post":"cjex1jrzh000rsi9gft9wq43e","slug":"TF.png","modified":1,"renderable":1},{"_id":"source/_posts/logit-n-probit/logistic.png","post":"cjex1jrzf000msi9gknyt6zk9","slug":"logistic.png","modified":1,"renderable":1},{"_id":"source/_posts/logit-n-probit/logit-logistic-relation.jpg","post":"cjex1jrzf000msi9gknyt6zk9","slug":"logit-logistic-relation.jpg","modified":1,"renderable":1},{"_id":"source/_posts/logit-n-probit/logit.png","post":"cjex1jrzf000msi9gknyt6zk9","slug":"logit.png","modified":1,"renderable":1},{"_id":"source/_posts/cnn/cnn002.png","post":"cjex1jryw0007si9gqihcx2ap","slug":"cnn002.png","modified":1,"renderable":1},{"_id":"source/_posts/cnn/cnn003.png","slug":"cnn003.png","post":"cjex1jryw0007si9gqihcx2ap","modified":1,"renderable":0},{"_id":"source/_posts/cnn/logistic.png","post":"cjex1jryw0007si9gqihcx2ap","slug":"logistic.png","modified":1,"renderable":1},{"_id":"source/_posts/auc-n-logloss/2.png","post":"cjex1jryh0003si9gibtjyf42","slug":"2.png","modified":1,"renderable":1},{"_id":"source/_posts/auc-n-logloss/4.png","post":"cjex1jryh0003si9gibtjyf42","slug":"4.png","modified":1,"renderable":1},{"_id":"source/_posts/auc-n-logloss/6.png","post":"cjex1jryh0003si9gibtjyf42","slug":"6.png","modified":1,"renderable":1},{"_id":"source/_posts/auc-n-logloss/7.png","post":"cjex1jryh0003si9gibtjyf42","slug":"7.png","modified":1,"renderable":1},{"_id":"source/_posts/auc-n-logloss/8.png","post":"cjex1jryh0003si9gibtjyf42","slug":"8.png","modified":1,"renderable":1},{"_id":"source/_posts/auc-n-logloss/9.png","post":"cjex1jryh0003si9gibtjyf42","slug":"9.png","modified":1,"renderable":1},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A51.1.png","slug":"%D5%F8%A51.1.png","post":"cjex1jryz0008si9gz0yzyxuy","modified":1,"renderable":0},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A52.2.png","post":"cjex1jryz0008si9gz0yzyxuy","slug":"%D5%F8%A52.2.png","modified":1,"renderable":1},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A53.3.jpg","slug":"%D5%F8%A53.3.jpg","post":"cjex1jryz0008si9gz0yzyxuy","modified":1,"renderable":0},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A53.3.png","slug":"%D5%F8%A53.3.png","post":"cjex1jryz0008si9gz0yzyxuy","modified":1,"renderable":0},{"_id":"source/_posts/ctr-recalibration/%D5%F8%A54.4.png","post":"cjex1jryz0008si9gz0yzyxuy","slug":"%D5%F8%A54.4.png","modified":1,"renderable":1},{"_id":"source/_posts/ctr-recalibration/图1.1.png","slug":"图1.1.png","post":"cjex1jryz0008si9gz0yzyxuy","modified":1,"renderable":0},{"_id":"source/_posts/ctr-recalibration/图2.2.png","post":"cjex1jryz0008si9gz0yzyxuy","slug":"图2.2.png","modified":1,"renderable":1},{"_id":"source/_posts/ctr-recalibration/图3.3.jpg","slug":"图3.3.jpg","post":"cjex1jryz0008si9gz0yzyxuy","modified":1,"renderable":0},{"_id":"source/_posts/ctr-recalibration/图3.3.png","slug":"图3.3.png","post":"cjex1jryz0008si9gz0yzyxuy","modified":1,"renderable":0},{"_id":"source/_posts/ctr-recalibration/图4.4.png","post":"cjex1jryz0008si9gz0yzyxuy","slug":"图4.4.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E71.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E71.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7100.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E7100.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7101.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E7101.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7102.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E7102.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7103.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E7103.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7104.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E7104.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7105.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E7105.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7106.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E7106.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E72.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E72.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E7201.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E7201.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E73.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E73.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E74.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E74.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E75.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E75.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E76.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E76.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E77.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E77.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E78.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E78.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/%D5%F8%A5%FE%EB%E79.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"%D5%F8%A5%FE%EB%E79.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/301.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"301.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/401.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"401.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/402.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"402.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/501.png","post":"cjex1jrze000lsi9grm9b9qvt","slug":"501.png","modified":1,"renderable":1},{"_id":"source/_posts/lda/601.jpg","post":"cjex1jrze000lsi9grm9b9qvt","slug":"601.jpg","modified":1,"renderable":1}],"PostCategory":[{"post_id":"cjex1jryh0003si9gibtjyf42","category_id":"cjex1jryu0006si9g62ypblhd","_id":"cjex1jrz6000dsi9g6omdbf08"},{"post_id":"cjex1jrzc000isi9g8e3m8jgr","category_id":"cjex1jryu0006si9g62ypblhd","_id":"cjex1jrzf000nsi9guq92r6el"},{"post_id":"cjex1jrze000lsi9grm9b9qvt","category_id":"cjex1jrzf000osi9guqfrfi3p","_id":"cjex1jrzn000vsi9gf3evvk4a"}],"PostTag":[{"post_id":"cjex1jryz0008si9gz0yzyxuy","tag_id":"cjex1jrz4000asi9g4wy7cysl","_id":"cjex1jrza000fsi9gfrrdqx55"},{"post_id":"cjex1jrzb000hsi9gc6fmyf5j","tag_id":"cjex1jrzd000jsi9gszti7s2z","_id":"cjex1jrzf000psi9gwu8uu2eb"}],"Tag":[{"name":"ML","_id":"cjex1jrz4000asi9g4wy7cysl"},{"name":"DNN","_id":"cjex1jrzd000jsi9gszti7s2z"}]}}