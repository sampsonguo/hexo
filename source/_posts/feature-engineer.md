---
title: 聊聊特征工程
date: 2017-10-02 17:52:35
tags:
categories: 机器学习
---

### 特征工程

在机器学习项目中，往往受到关注的是高大上的机器学习模型，特征工程很少有人问津，可能唯一提到的便是浮夸的一句“我们的模型使用了百万级别的特征”。然而特征工程对于线上效果的贡献，往往远远大于模型，所以一个健全的特征工程方法论非常的重要。

<!-- more -->

### 最有效的特征是什么

在pCTR项目中，决定是否点击的最重要的因素，是Item本身和User本身，即ItemID和UserID特征。
* ItemID: 推荐“王者荣耀”和“全民超神”，大家都会选择“王者荣耀”，因为你的朋友都在这款游戏里，所以个人偏好远远小于物品属性的影响。
* UserID: 用户需求明确（就是来找一款MOBA手游），点击率自然高；用户就是来逛逛，刷刷页面，那点击率自然低。
* 其他的特征: 时间地点场景年龄性别星级类型等对模型的影响是次要的。

### ID特征太多怎么办
如果ID数量太多不便处理，可以简单用统计CTR特征来代替，纯粹ID特征等价于纯粹CTR特征，从理论推导和代码实践上皆可证明。

#### 实验经验
自己曾经做了一次实验，单CTR特征模型AUC=0.7+，其他所有特征（单单排除CTR特征）模型AUC=0.6+，所有特征一起建模AUC=0.8+。

#### CTR特征的坏处
但是用CTR特征的坏处是，交叉的时候相对于ID特征，会丢失信息。

### 最好的非个性化模型
对于某个UserX来说，ItemID特征（CTR特征）起到主导作用，其他特征只是辅助，那么最好的非个性化模型即是CTR排序模型（或只有ItemID的特征的模型）。

### Item个数和提升天花板
当Item数量越少，Item之间差别越大的时候，个性化的能够提升的空间越小（比如某业务只有40+个特征，个性化模型只能比CTR热门提升6%左右）；当Item数量非常庞大的时候（如淘宝），或者用户偏好非常分散的时候（如书籍，各个年龄性别行业都不同），推荐才有大的发挥空间。

### 连续特征 VS 离散特征
在工程实践中，有2种类型的特征：连续特征和离散特征。而“百万级别特征”里往往大部分是离散特征，以App推荐为例，有User/Item ID，城市，地区，标签特征，分类特征，厂商等等，经过one-hot之后，数量急剧爆炸；而连续特征有很多是人造统计特征，比如：下载量，访问量，ltv，arpu，实时ctr等等，成本高，数量少。

### 特征工程

#### 人工特征工程
##### 特征提取
特征的提取，很大程度上是人的工作（除去一些端到端的NN方案），初期依照业务知识，自行YY出一些特征出来。以APP推荐为例，CTR特征保证高转化，下载量特征保证热门，星级特征保证质量，用户安装使用/APP类别特征保证个性化。
从划分来看，特征可以有以下来源：
1. 基础属性：不随时间变化的属性。如User的性别，年龄，职业，住址等；Item的自身属性（如APP的星级，公司，包大小等）
2. 统计属性：简单统计可以得到的特征。如User的下载量，点击量，CTR值等；Item的曝光，点击，下载，ARPU，LTV，留存等。
3. 标签转移属性：标签转移是建设画像的一种重要思路。APP画像转移到用户画像上的有：点击的类型分布，下载的类型分布等；用户画像转移到APP画像上的有：男女使用分布，性别安装分布，地域点击率分布等。
4. 场景属性：事情发生的时间，地点，场景等，如：APP的某个页面ID，猜你喜欢的第X位等。
5. 设备属性：手机的好坏。ROM，RAM大小等非常影响用户的游戏下载属性。
6. 迁移属性：画像的特点就是知识迁移方便。广告业务的特征用到APP业务上，WiFi的特征用到流量业务上，非常的常见。
7. （人工）交叉特征：比如User的三级分类画像和APP的三级分类画像，每一个相对应的特征，交叉一遍，得到的人工交叉特征。
8. 实时特征：讲上述的特征，尤其是统计特征，实时化。获取当前热点信息。

##### 特征选择（特征重要性）
特征选择有非常多的方法，一个常见的错误是将LR的权重作为特征选择的依据。因为LR中每个Feature的量纲是不同的（比如年龄1-100，温度是-10-40，下载量是几十万），所以特征值大权重小，特征值小权重大。所以LR的权重只有参考意义，不能盲目信任。
个人列举一些常用的选择的方法：
1. 单特征AUC（最常用）
2. 单特征gini index（信息增益，信息增益率）
3. 相关系数，卡方检验
4. L1模型自动选择
5. RF/GBDT打印Feature Importance
6. wrapper：1-n逐个增加特征，有用就加，无用就抛弃（同事用过，个人经验不足）

##### 特征归一化
即Z-score，minmax，log变换等，在这里不再赘述。
需要了解的是：归一化本身并不增加模型精读，只是将特征统一量纲，加速训练。

##### 特征分段
1. 等宽：1-10,11-20,21-30等距离分。
2. 等频：先rank，top1-100,top101-200,top201-300等频率分。
3. 人工：0-17未成年，18-25青年，25-35工作，35-45中年，45以上...
4. Label决定：如先分桶，通过gini index求最佳分隔点；如使用如下CTR图

{% asset_img "年龄画段.png" [年龄画段] %}

##### 特征组合
1. one-hot特征交叉：01交叉得0, 11交叉得1
2. one-real特征交叉：0-real交叉得0, 1-real交叉得real
3. 强强联合：两个强特征进行交叉

#### 自动化特征工程
上述人工特征工程实在是费心费力，所以建议不使用人工特征工程，全部使用”最原始“特征交给模型来做。首先将特征分成”连续特征“和”离散特征“两种，然后将特征扔进GBDT，GBDT自动进行：
1. 特征选择：不好的特征，根本进不去树里面。
2. 特征分段：树的split的分支，即是分段方案。
3. 特征组合：叶子节点路径，即使特征组合。
强烈推荐。

### 0 or missing?
最后讨论一个小问题，libsvm中被稀疏掉的特征，表示0还是表示missing？
答案是0，libsvm中默认没有missing。
但是xgboost中对libsvm的处理，是按照missing来处理的，将0和missing分开的方法是：
1. 连续特征：增加隐控制变量表达是否missing，另一个变量表示值。
2. 离散特征：将missing枚举为一个离散值。
